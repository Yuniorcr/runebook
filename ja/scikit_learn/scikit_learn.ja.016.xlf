<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ja" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="73890f2d8730349939758bb99469c8dd6b7e6151" translate="yes" xml:space="preserve">
          <source>The maximum number of patches per image to extract. If max_patches is a float in (0, 1), it is taken to mean a proportion of the total number of patches.</source>
          <target state="translated">抽出する画像あたりのパッチの最大数。max_patches が (0,1)の float の場合は、 パッチの総数に対する割合を意味します。</target>
        </trans-unit>
        <trans-unit id="9914613b7b3d16fc7caab060022c123f66024339" translate="yes" xml:space="preserve">
          <source>The maximum number of patches to extract. If max_patches is a float between 0 and 1, it is taken to be a proportion of the total number of patches.</source>
          <target state="translated">抽出するパッチの最大数。max_patches が 0 から 1 の間の浮動小数点数の場合は、パッチの総数に対する割合として扱われます。</target>
        </trans-unit>
        <trans-unit id="353b6da890fd9a7a3db82e5fc166d25d61607aa3" translate="yes" xml:space="preserve">
          <source>The maximum number of points on the path used to compute the residuals in the cross-validation</source>
          <target state="translated">クロスバリデーションで残差を計算するために使用されるパス上の最大ポイント数</target>
        </trans-unit>
        <trans-unit id="c53181fb6c1656f1967e529a2fe72fd9bc29ff60" translate="yes" xml:space="preserve">
          <source>The mean and the empirical covariance of the full dataset, which break down as soon as there are outliers in the data set</source>
          <target state="translated">完全データセットの平均と経験的共分散は,データセットに外れ値があるとすぐに分解する.</target>
        </trans-unit>
        <trans-unit id="f962fad68fff332002b42ee3dcc1e06f41fcfe6c" translate="yes" xml:space="preserve">
          <source>The mean and the empirical covariance of the observations that are known to be good ones. This can be considered as a &amp;ldquo;perfect&amp;rdquo; MCD estimation, so one can trust our implementation by comparing to this case.</source>
          <target state="translated">良いものとして知られている観測値の平均と経験的共分散。これは「完全な」MCD推定と見なすことができるため、このケースと比較することで、実装を信頼できます。</target>
        </trans-unit>
        <trans-unit id="f98043dc9f229ba1c70dcb8021abddb5d793d8af" translate="yes" xml:space="preserve">
          <source>The mean of each mixture component.</source>
          <target state="translated">各混合成分の平均値。</target>
        </trans-unit>
        <trans-unit id="05329e8678ffdabb505c75140f9a49322a5129f1" translate="yes" xml:space="preserve">
          <source>The mean of the multi-dimensional normal distribution. If None then use the origin (0, 0, &amp;hellip;).</source>
          <target state="translated">多次元正規分布の平均。Noneの場合、原点（0、0、&amp;hellip;）を使用します。</target>
        </trans-unit>
        <trans-unit id="3598a929d1a64528ce62560f1aa02f9fb9981b30" translate="yes" xml:space="preserve">
          <source>The mean predicted probability in each bin.</source>
          <target state="translated">各ビンの平均予測確率。</target>
        </trans-unit>
        <trans-unit id="8aaf5602eb2242bddc9912d47746326b31b0a1d5" translate="yes" xml:space="preserve">
          <source>The mean score and the 95% confidence interval of the score estimate are hence given by:</source>
          <target state="translated">したがって、平均スコアとスコア推定値の95%信頼区間は次式で与えられます。</target>
        </trans-unit>
        <trans-unit id="94d27e1df24bf9a05c82ae625a1197b415bc3fdc" translate="yes" xml:space="preserve">
          <source>The mean value for each feature in the training set. Equal to &lt;code&gt;None&lt;/code&gt; when &lt;code&gt;with_mean=False&lt;/code&gt;.</source>
          <target state="translated">トレーニングセットの各特徴の平均値。 &lt;code&gt;with_mean=False&lt;/code&gt; の場合は &lt;code&gt;None&lt;/code&gt; と同じです。</target>
        </trans-unit>
        <trans-unit id="b5e7dcb7d882c8689297cb339a998a6e74140e5f" translate="yes" xml:space="preserve">
          <source>The mean, standard error, and &amp;ldquo;worst&amp;rdquo; or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.</source>
          <target state="translated">これらの特徴の平均、標準誤差、および「最悪」または最大（3つの最大値の平均）が各画像について計算され、30の特徴が得られました。たとえば、フィールド3は平均半径、フィールド13は半径SE、フィールド23は最悪半径です。</target>
        </trans-unit>
        <trans-unit id="9952f9f2a02ad0002415a0bd8b6c9bae196d7ee8" translate="yes" xml:space="preserve">
          <source>The measure of normality of an observation given a tree is the depth of the leaf containing this observation, which is equivalent to the number of splittings required to isolate this point. In case of several observations n_left in the leaf, the average path length of a n_left samples isolation tree is added.</source>
          <target state="translated">木が与えられたオブザベーションの正規性の尺度は,このオブザベーションを含むリーフの深さであり,この点を分離するのに必要な分割の数に相当する.リーフ内に複数のオブザベーションn_leftがある場合、n_leftサンプルの分離木の平均パス長が追加されます。</target>
        </trans-unit>
        <trans-unit id="3ff66cd9c701474c3d9f795cee9d82f5e1659bc8" translate="yes" xml:space="preserve">
          <source>The median absolute deviation to non corrupt new data is used to judge the quality of the prediction.</source>
          <target state="translated">予測の質を判断するために、非破損の新しいデータへの絶対偏差の中央値を使用しています。</target>
        </trans-unit>
        <trans-unit id="970571bcac487854f4caea3e516e12da8ac34c22" translate="yes" xml:space="preserve">
          <source>The median value for each feature in the training set.</source>
          <target state="translated">学習セットの各特徴の中央値。</target>
        </trans-unit>
        <trans-unit id="c8bf90a15bbbe280812d1ade2a9f9077adb9d41d" translate="yes" xml:space="preserve">
          <source>The memmapping mode used when loading from cache numpy arrays. See numpy.load for the meaning of the arguments.</source>
          <target state="translated">キャッシュnumpy配列からロードする際に使用するmemmappingモード。引数の意味は numpy.load を参照してください。</target>
        </trans-unit>
        <trans-unit id="4971ea4c8c64c5bc8a4bdd1e4563c9705f8166bf" translate="yes" xml:space="preserve">
          <source>The memmapping mode used when loading from cache numpy arrays. See numpy.load for the meaning of the arguments. By default that of the memory object is used.</source>
          <target state="translated">キャッシュnumpy配列からロードする際に使用するmemmappingモード。引数の意味は numpy.load を参照してください。デフォルトではメモリオブジェクトのものが使われます。</target>
        </trans-unit>
        <trans-unit id="85874f620128fa58a2e359ad1c9c828fcfd537bb" translate="yes" xml:space="preserve">
          <source>The memory footprint of randomized &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; is also proportional to \(2 \cdot n_{\max} \cdot n_{\mathrm{components}}\) instead of \(n_{\max} \cdot n_{\min}\) for the exact method.</source>
          <target state="translated">ランダム化された&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt;のメモリフットプリントは、\（n _ {\ max} \ cdot n _ {\ min} \）ではなく、\（2 \ cdot n _ {\ max} \ cdot n _ {\ mathrm {components}} \）にも比例します。正確な方法。</target>
        </trans-unit>
        <trans-unit id="4fa2b59a5a9d6863d86f91b2a847ac2c39bc204e" translate="yes" xml:space="preserve">
          <source>The method assumes the inputs come from a binary classifier.</source>
          <target state="translated">この方法は,入力がバイナリ分類器からのものであることを前提としています.</target>
        </trans-unit>
        <trans-unit id="ff8ec8205e374ddf520735804ecbfa39550f37bd" translate="yes" xml:space="preserve">
          <source>The method fits the model &lt;code&gt;n_init&lt;/code&gt; times and sets the parameters with which the model has the largest likelihood or lower bound. Within each trial, the method iterates between E-step and M-step for &lt;code&gt;max_iter&lt;/code&gt; times until the change of likelihood or lower bound is less than &lt;code&gt;tol&lt;/code&gt;, otherwise, a &lt;code&gt;ConvergenceWarning&lt;/code&gt; is raised. If &lt;code&gt;warm_start&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then &lt;code&gt;n_init&lt;/code&gt; is ignored and a single initialization is performed upon the first call. Upon consecutive calls, training starts where it left off.</source>
          <target state="translated">このメソッドは、モデルを &lt;code&gt;n_init&lt;/code&gt; 回適合させ、モデルが最尤または下限をもつパラメーターを設定します。各試行内で、メソッドは尤度または下限の変化が &lt;code&gt;tol&lt;/code&gt; 未満になるまで &lt;code&gt;max_iter&lt;/code&gt; とM-stepの間をmax_iter回繰り返します。そうでない場合は、 &lt;code&gt;ConvergenceWarning&lt;/code&gt; が発生します。場合 &lt;code&gt;warm_start&lt;/code&gt; がある &lt;code&gt;True&lt;/code&gt; 、そして &lt;code&gt;n_init&lt;/code&gt; 無視され、単一の初期化は、最初の呼び出し時に行われます。連続した呼び出しで、トレーニングは中断したところから始まります。</target>
        </trans-unit>
        <trans-unit id="5aeb4fb178bdd7f679ad7ab31606d57c02d18da8" translate="yes" xml:space="preserve">
          <source>The method fits the model n_init times and sets the parameters with which the model has the largest likelihood or lower bound. Within each trial, the method iterates between E-step and M-step for &lt;code&gt;max_iter&lt;/code&gt; times until the change of likelihood or lower bound is less than &lt;code&gt;tol&lt;/code&gt;, otherwise, a &lt;code&gt;ConvergenceWarning&lt;/code&gt; is raised. After fitting, it predicts the most probable label for the input data points.</source>
          <target state="translated">このメソッドは、モデルをn_init回適合させ、モデルが最尤または下限をもつパラメーターを設定します。各試行内で、メソッドは尤度または下限の変化が &lt;code&gt;tol&lt;/code&gt; 未満になるまで &lt;code&gt;max_iter&lt;/code&gt; とM-stepの間をmax_iter回繰り返します。そうでない場合は、 &lt;code&gt;ConvergenceWarning&lt;/code&gt; が発生します。フィッティング後、入力データポイントの最も可能性の高いラベルを予測します。</target>
        </trans-unit>
        <trans-unit id="02e13e01ea0f52a6e082ed53d9636e409ba7f22e" translate="yes" xml:space="preserve">
          <source>The method gained popularity for initializing deep neural networks with the weights of independent RBMs. This method is known as unsupervised pre-training.</source>
          <target state="translated">この方法は、独立したRBMの重みでディープニューラルネットワークを初期化するために人気を博した。この方法は教師なし事前学習と呼ばれている。</target>
        </trans-unit>
        <trans-unit id="867b88e44ce337abe62bbd2070ac4235fc6ec53b" translate="yes" xml:space="preserve">
          <source>The method of Support Vector Classification can be extended to solve regression problems. This method is called Support Vector Regression.</source>
          <target state="translated">サポートベクター分類の手法を拡張して、回帰問題を解くことができます。この方法は、サポートベクトル回帰と呼ばれています。</target>
        </trans-unit>
        <trans-unit id="0b9b2478ee76aa8f8a62d64db00bfa346a8108cd" translate="yes" xml:space="preserve">
          <source>The method to use for calibration. Can be &amp;lsquo;sigmoid&amp;rsquo; which corresponds to Platt&amp;rsquo;s method or &amp;lsquo;isotonic&amp;rsquo; which is a non-parametric approach. It is not advised to use isotonic calibration with too few calibration samples &lt;code&gt;(&amp;lt;&amp;lt;1000)&lt;/code&gt; since it tends to overfit. Use sigmoids (Platt&amp;rsquo;s calibration) in this case.</source>
          <target state="translated">キャリブレーションに使用する方法。プラットの方法に対応する「シグモイド」またはノンパラメトリックアプローチである「等張性」のいずれかになります。キャリブレーションサンプルが少なすぎる &lt;code&gt;(&amp;lt;&amp;lt;1000)&lt;/code&gt; 等張キャリブレーションを使用することはお勧めしません。この場合、シグモイド（プラットの校正）を使用します。</target>
        </trans-unit>
        <trans-unit id="21e55011234a7eafaab581faf9cf56cabba5a5c2" translate="yes" xml:space="preserve">
          <source>The method used to initialize the weights, the means and the covariances. Must be one of:</source>
          <target state="translated">重み,平均,共分散の初期化に用いられるメソッド.のいずれかでなければならない.</target>
        </trans-unit>
        <trans-unit id="963cb60032e3efabe5bef9b8ad76de7b8282f830" translate="yes" xml:space="preserve">
          <source>The method used to initialize the weights, the means and the precisions. Must be one of:</source>
          <target state="translated">重み、平均、精度を初期化するために使用されるメソッド.のいずれかでなければならない.</target>
        </trans-unit>
        <trans-unit id="8015d2d76c1dfbbe3de68f3d3f8aa78fbf89dd67" translate="yes" xml:space="preserve">
          <source>The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form &lt;code&gt;&amp;lt;component&amp;gt;__&amp;lt;parameter&amp;gt;&lt;/code&gt; so that it&amp;rsquo;s possible to update each component of a nested object.</source>
          <target state="translated">このメソッドは、ネストされたオブジェクト（パイプラインなど）だけでなく、単純な推定量でも機能します。後者には &lt;code&gt;&amp;lt;component&amp;gt;__&amp;lt;parameter&amp;gt;&lt;/code&gt; という形式のパラメーターがあるため、ネストされたオブジェクトの各コンポーネントを更新できます。</target>
        </trans-unit>
        <trans-unit id="5d1673cba254e117532409bf5f2a151ed75e6f39" translate="yes" xml:space="preserve">
          <source>The method works on simple kernels as well as on nested kernels. The latter have parameters of the form &lt;code&gt;&amp;lt;component&amp;gt;__&amp;lt;parameter&amp;gt;&lt;/code&gt; so that it&amp;rsquo;s possible to update each component of a nested object.</source>
          <target state="translated">この方法は、ネストされたカーネルだけでなく、単純なカーネルでも機能します。後者には &lt;code&gt;&amp;lt;component&amp;gt;__&amp;lt;parameter&amp;gt;&lt;/code&gt; という形式のパラメーターがあるため、ネストされたオブジェクトの各コンポーネントを更新できます。</target>
        </trans-unit>
        <trans-unit id="653c073b67dfbefaf963d2a9e7b682aaf13d10c5" translate="yes" xml:space="preserve">
          <source>The methods based on F-test estimate the degree of linear dependency between two random variables. On the other hand, mutual information methods can capture any kind of statistical dependency, but being nonparametric, they require more samples for accurate estimation.</source>
          <target state="translated">F-検定に基づく手法は、2つのランダム変数間の線形依存性の度合いを推定する。一方、相互情報法は、あらゆる種類の統計的依存関係を捉えることができますが、ノンパラメトリックであるため、正確な推定のためにはより多くのサンプルを必要とします。</target>
        </trans-unit>
        <trans-unit id="42e3e79f7ef752ca5a9bd963af37b6abd9f4c61f" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by &lt;a href=&quot;sklearn.metrics.pairwise_distances#sklearn.metrics.pairwise_distances&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise_distances&lt;/code&gt;&lt;/a&gt; for its metric parameter. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a distance matrix and must be square. X may be a sparse matrix, in which case only &amp;ldquo;nonzero&amp;rdquo; elements may be considered neighbors for DBSCAN.</source>
          <target state="translated">フィーチャ配列のインスタンス間の距離を計算するときに使用するメトリック。指標が文字列または呼び出し可能である場合、それはそのmetricパラメーターに対して&lt;a href=&quot;sklearn.metrics.pairwise_distances#sklearn.metrics.pairwise_distances&quot;&gt; &lt;code&gt;sklearn.metrics.pairwise_distances&lt;/code&gt; &lt;/a&gt;によって許可されるオプションの1つである必要があります。メトリックが「事前計算」されている場合、Xは距離行列と見なされ、正方でなければなりません。 Xはスパース行列である場合があります。その場合、「非ゼロ」要素のみがDBSCANの隣接要素と見なされます。</target>
        </trans-unit>
        <trans-unit id="379ba7f47f97569c7bd4b20c4c77667f05344897" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by metrics.pairwise.pairwise_distances for its metric parameter. The centroids for the samples corresponding to each class is the point from which the sum of the distances (according to the metric) of all samples that belong to that particular class are minimized. If the &amp;ldquo;manhattan&amp;rdquo; metric is provided, this centroid is the median and for all other metrics, the centroid is now set to be the mean.</source>
          <target state="translated">フィーチャ配列のインスタンス間の距離を計算するときに使用するメトリック。メトリックが文字列または呼び出し可能である場合、それは、そのメトリックパラメーターのmetrics.pairwise.pairwise_distancesによって許可されたオプションの1つである必要があります。各クラスに対応するサンプルの重心は、その特定のクラスに属するすべてのサンプルの距離の合計（メトリックに基づく）が最小化されるポイントです。 「マンハッタン」メトリックが提供されている場合、この重心は中央値であり、他のすべてのメトリックでは、重心は平均に設定されています。</target>
        </trans-unit>
        <trans-unit id="c83cb22e1c81dc697b4c0637e95b8aeb91bdccce" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by &lt;code&gt;metrics.pairwise.pairwise_distances&lt;/code&gt;. If X is the distance array itself, use &lt;code&gt;metric=&quot;precomputed&quot;&lt;/code&gt;.</source>
          <target state="translated">フィーチャ配列のインスタンス間の距離を計算するときに使用するメトリック。metricが文字列の場合は、 &lt;code&gt;metrics.pairwise.pairwise_distances&lt;/code&gt; で許可されているオプションの1つである必要があります。Xが距離配列自体の場合は、 &lt;code&gt;metric=&quot;precomputed&quot;&lt;/code&gt; を使用します。</target>
        </trans-unit>
        <trans-unit id="76b4b37055a409fe2e4e93cf3ad5227beac3187f" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by &lt;code&gt;sklearn.metrics.pairwise.pairwise_distances&lt;/code&gt;. If X is the distance array itself, use &amp;ldquo;precomputed&amp;rdquo; as the metric.</source>
          <target state="translated">フィーチャ配列のインスタンス間の距離を計算するときに使用するメトリック。メトリックが文字列の場合、それは &lt;code&gt;sklearn.metrics.pairwise.pairwise_distances&lt;/code&gt; で許可されているオプションの1つである必要があります。Xが距離配列自体の場合、メトリックとして「事前計算済み」を使用します。</target>
        </trans-unit>
        <trans-unit id="f0f209bf70493187675786c136d5f63b20f1b34d" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter, or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a distance matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them.</source>
          <target state="translated">フィーチャ配列のインスタンス間の距離を計算するときに使用するメトリック。メトリックが文字列の場合、それはscipy.spatial.distance.pdistによってそのメトリックパラメータに対して許可されているオプションの1つ、またはpairwise.PAIRWISE_DISTANCE_FUNCTIONSにリストされているメトリックでなければなりません。メトリックが「事前計算」されている場合、Xは距離行列であると見なされます。または、メトリックが呼び出し可能な関数の場合、インスタンス（行）の各ペアで呼び出され、結果の値が記録されます。呼び出し可能オブジェクトは、Xから2つの配列を入力として取り、それらの間の距離を示す値を返す必要があります。</target>
        </trans-unit>
        <trans-unit id="5d6bfb4b6dd59ce295c63dba458c016e3505d6e2" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter, or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a distance matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them. The default is &amp;ldquo;euclidean&amp;rdquo; which is interpreted as squared euclidean distance.</source>
          <target state="translated">フィーチャ配列のインスタンス間の距離を計算するときに使用するメトリック。メトリックが文字列の場合、それはscipy.spatial.distance.pdistによってそのメトリックパラメータに対して許可されているオプションの1つ、またはpairwise.PAIRWISE_DISTANCE_FUNCTIONSにリストされているメトリックでなければなりません。メトリックが「事前計算」されている場合、Xは距離行列であると見なされます。または、メトリックが呼び出し可能な関数の場合、インスタンス（行）の各ペアで呼び出され、結果の値が記録されます。呼び出し可能オブジェクトは、Xから2つの配列を入力として取り、それらの間の距離を示す値を返す必要があります。デフォルトは「ユークリッド」で、ユークリッド距離の2乗として解釈されます。</target>
        </trans-unit>
        <trans-unit id="1d21e688cc862f7d70c6767af76fb5452c5fdcd0" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options specified in PAIRED_DISTANCES, including &amp;ldquo;euclidean&amp;rdquo;, &amp;ldquo;manhattan&amp;rdquo;, or &amp;ldquo;cosine&amp;rdquo;. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them.</source>
          <target state="translated">フィーチャ配列のインスタンス間の距離を計算するときに使用するメトリック。指標が文字列の場合、「ユークリッド」、「マンハッタン」、「コサイン」など、PAIRED_DISTANCESで指定されたオプションの1つである必要があります。または、メトリックが呼び出し可能な関数の場合、インスタンス（行）の各ペアで呼び出され、結果の値が記録されます。呼び出し可能オブジェクトは、Xから2つの配列を入力として取り、それらの間の距離を示す値を返す必要があります。</target>
        </trans-unit>
        <trans-unit id="f36b3ebd6c1e0314a19882dfd7c792465ced8cb2" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating kernel between instances in a feature array. If metric is a string, it must be one of the metrics in pairwise.PAIRWISE_KERNEL_FUNCTIONS. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a kernel matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them.</source>
          <target state="translated">機能配列のインスタンス間のカーネルを計算するときに使用するメトリック。指標が文字列の場合は、pairwise.PAIRWISE_KERNEL_FUNCTIONSの指標の1つである必要があります。メトリックが「事前計算」されている場合、Xはカーネル行列であると見なされます。または、メトリックが呼び出し可能な関数の場合、インスタンス（行）の各ペアで呼び出され、結果の値が記録されます。呼び出し可能オブジェクトは、Xから2つの配列を入力として取り、それらの間の距離を示す値を返す必要があります。</target>
        </trans-unit>
        <trans-unit id="f4d18e9d9a4e3ba27fc7d43cfc9aa960df294a66" translate="yes" xml:space="preserve">
          <source>The minimal number of components to guarantee with good probability an eps-embedding with n_samples.</source>
          <target state="translated">n_samplesを持つeps-embeddingを高い確率で保証するための最小構成要素数。</target>
        </trans-unit>
        <trans-unit id="dcfd1fcfc3ab5a126126fe7777d3f592dbce3714" translate="yes" xml:space="preserve">
          <source>The minimum consensus score, 0, occurs when all pairs of biclusters are totally dissimilar. The maximum score, 1, occurs when both sets are identical.</source>
          <target state="translated">コンセンサススコアの最小値0は、すべてのバイクラスターのペアが完全に異質な場合に発生します。最大スコアである1は、両方のセットが同一の場合に発生します。</target>
        </trans-unit>
        <trans-unit id="546685a327b5ecf09c20bafa81b23b9f31e36223" translate="yes" xml:space="preserve">
          <source>The minimum number of components to guarantee the eps-embedding is given by:</source>
          <target state="translated">eps-embeddingを保証するための最小構成要素数は次のように与えられます。</target>
        </trans-unit>
        <trans-unit id="d171bb6d353676c30b749e2ca85c8811f4027e78" translate="yes" xml:space="preserve">
          <source>The minimum number of components to guarantees the eps-embedding is given by:</source>
          <target state="translated">eps-embeddingを保証するためのコンポーネントの最小数は次のように与えられる。</target>
        </trans-unit>
        <trans-unit id="78f0aa92767e958a159a7201fe662ff62e3924a6" translate="yes" xml:space="preserve">
          <source>The minimum number of features to be selected. This number of features will always be scored, even if the difference between the original feature count and &lt;code&gt;min_features_to_select&lt;/code&gt; isn&amp;rsquo;t divisible by &lt;code&gt;step&lt;/code&gt;.</source>
          <target state="translated">選択する機能の最小数。元のフィーチャ数と &lt;code&gt;min_features_to_select&lt;/code&gt; の差が &lt;code&gt;step&lt;/code&gt; 割り切れない場合でも、この数のフィーチャは常にスコアリングされます。</target>
        </trans-unit>
        <trans-unit id="1db45f422759f4529cb388eaa249fdf3a381a4d3" translate="yes" xml:space="preserve">
          <source>The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least &lt;code&gt;min_samples_leaf&lt;/code&gt; training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.</source>
          <target state="translated">リーフノードに存在するために必要なサンプルの最小数。深さのある分割ポイントは、左右のブランチのそれぞれに少なくとも &lt;code&gt;min_samples_leaf&lt;/code&gt; トレーニングサンプルが残っている場合にのみ考慮されます。これは、特に回帰において、モデルを平滑化する効果がある場合があります。</target>
        </trans-unit>
        <trans-unit id="754bd23e1994f02a9f11fc7f2013baebc017ca4d" translate="yes" xml:space="preserve">
          <source>The minimum number of samples required to split an internal node:</source>
          <target state="translated">内部ノードを分割するのに必要な最小サンプル数。</target>
        </trans-unit>
        <trans-unit id="34dec0ced4163b0b0c5f6b2dfda2c2ae915251bf" translate="yes" xml:space="preserve">
          <source>The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.</source>
          <target state="translated">リーフノードに存在するために必要な(すべての入力サンプルの)重みの合計の最小重み分数。sample_weightが指定されていない場合は,サンプルの重みは等しくなります.</target>
        </trans-unit>
        <trans-unit id="adbfb8d83e84eef2c959ef548acb01276646831a" translate="yes" xml:space="preserve">
          <source>The missing indicator for input data. The data type of &lt;code&gt;Xt&lt;/code&gt; will be boolean.</source>
          <target state="translated">入力データの欠落している標識。 &lt;code&gt;Xt&lt;/code&gt; のデータ型はブール値になります。</target>
        </trans-unit>
        <trans-unit id="2b5249d3ed9eb260ab7aedd15c61af2c7df4b9f1" translate="yes" xml:space="preserve">
          <source>The mixing matrix to be used to initialize the algorithm.</source>
          <target state="translated">アルゴリズムの初期化に使用される混合行列。</target>
        </trans-unit>
        <trans-unit id="bc7bce3b2af118e0efad437caf7d72239aa18a90" translate="yes" xml:space="preserve">
          <source>The mixing matrix.</source>
          <target state="translated">混合マトリックス。</target>
        </trans-unit>
        <trans-unit id="d9852ae9396dc8e73ffef0a95fb9e0d330c7fbbc" translate="yes" xml:space="preserve">
          <source>The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix.</source>
          <target state="translated">このモデルは、すべてのクラスが同じ共分散行列を共有していると仮定して、各クラスにガウス密度を適合させます。</target>
        </trans-unit>
        <trans-unit id="c1fbc40d4a13f1c2e80dae47c2199d2a0ef37a24" translate="yes" xml:space="preserve">
          <source>The model fits a Gaussian density to each class.</source>
          <target state="translated">モデルはガウス密度を各クラスにフィットさせます。</target>
        </trans-unit>
        <trans-unit id="744fe4c01d7aac1fa2b93515c5b761f9f450f5fc" translate="yes" xml:space="preserve">
          <source>The model makes assumptions regarding the distribution of inputs. At the moment, scikit-learn only provides &lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt;&lt;code&gt;BernoulliRBM&lt;/code&gt;&lt;/a&gt;, which assumes the inputs are either binary values or values between 0 and 1, each encoding the probability that the specific feature would be turned on.</source>
          <target state="translated">モデルは、入力の分布に関する仮定を行います。現時点では、scikit-learnは&lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt; &lt;code&gt;BernoulliRBM&lt;/code&gt; &lt;/a&gt;のみを提供します。これは、入力がバイナリ値または0と1の間の値のいずれかであると想定し、それぞれが特定の機能がオンになる確率をエンコードします。</target>
        </trans-unit>
        <trans-unit id="3e7d91224c848a3199f89b8ed290703400860de0" translate="yes" xml:space="preserve">
          <source>The model need to have probability information computed at training time: fit with attribute &lt;code&gt;probability&lt;/code&gt; set to True.</source>
          <target state="translated">モデルは、トレーニング時に計算された確率情報を持つ必要があります。属性 &lt;code&gt;probability&lt;/code&gt; をTrueに設定してフィットします。</target>
        </trans-unit>
        <trans-unit id="532fcfc5fc4303622a7e9b0379fb5dd6d278b658" translate="yes" xml:space="preserve">
          <source>The model parameters can be accessed through the members &lt;code&gt;coef_&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt;:</source>
          <target state="translated">モデルパラメーターには、メンバー &lt;code&gt;coef_&lt;/code&gt; および &lt;code&gt;intercept_&lt;/code&gt; を介してアクセスできます。</target>
        </trans-unit>
        <trans-unit id="87fcd2dcd4a9683677aa4a82e264db34c1778c7c" translate="yes" xml:space="preserve">
          <source>The model produced by support vector classification (as described above) depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by Support Vector Regression depends only on a subset of the training data, because the cost function for building the model ignores any training data close to the model prediction.</source>
          <target state="translated">サポート・ベクトル分類(上述のように)によって生成されたモデルは,モデルを構築するためのコスト関数が,マージンを超えたところにある訓練点を気にしないので,訓練データのサブセットだけに依存する.同様に,サポート・ベクトル回帰によって生成されるモデルは,モデルを構築するためのコスト関数が,モデルの予測に近い訓練データを無視するので,訓練データのサブセットにのみ依存する.</target>
        </trans-unit>
        <trans-unit id="ffaf4f38449ad493c6f07021545ef1cc0f916269" translate="yes" xml:space="preserve">
          <source>The models are ordered from strongest regularized to least regularized. The 4 coefficients of the models are collected and plotted as a &amp;ldquo;regularization path&amp;rdquo;: on the left-hand side of the figure (strong regularizers), all the coefficients are exactly 0. When regularization gets progressively looser, coefficients can get non-zero values one after the other.</source>
          <target state="translated">モデルは、最も強い正則化から最も低い正則化へと並べられます。モデルの4つの係数が収集され、「正規化パス」としてプロットされます。図の左側（強力な正則化器）では、すべての係数が正確に0になります。正則化が徐々に緩くなると、係数はゼロ以外になる可能性があります次々と値。</target>
        </trans-unit>
        <trans-unit id="41864e959b3c3945768dfd483a7ad2160aff5a56" translate="yes" xml:space="preserve">
          <source>The module &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; includes the popular boosting algorithm AdaBoost, introduced in 1995 by Freund and Schapire &lt;a href=&quot;#fs1995&quot; id=&quot;id9&quot;&gt;[FS1995]&lt;/a&gt;.</source>
          <target state="translated">モジュール&lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; に&lt;/a&gt;は、Freund and Schapire &lt;a href=&quot;#fs1995&quot; id=&quot;id9&quot;&gt;[FS1995]&lt;/a&gt;によって1995年に導入された一般的なブースティングアルゴリズムAdaBoostが含まれています。</target>
        </trans-unit>
        <trans-unit id="06014352d795d21d24d63a43012b2cdda688123a" translate="yes" xml:space="preserve">
          <source>The module &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; provides methods for both classification and regression via gradient boosted regression trees.</source>
          <target state="translated">モジュール&lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt;は、勾配ブースト回帰ツリーによる分類と回帰の両方のメソッドを提供します。</target>
        </trans-unit>
        <trans-unit id="c3b04b84598eb19b700a6f5a28a6ebcc8d4034a0" translate="yes" xml:space="preserve">
          <source>The module &lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt;&lt;code&gt;sklearn.metrics&lt;/code&gt;&lt;/a&gt; also exposes a set of simple functions measuring a prediction error given ground truth and prediction:</source>
          <target state="translated">モジュール&lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt; &lt;code&gt;sklearn.metrics&lt;/code&gt; &lt;/a&gt;は、グラウンドトゥルースと予測が与えられた場合の予測エラーを測定する一連の単純な関数も公開します。</target>
        </trans-unit>
        <trans-unit id="a4ba63f9b8e0d9fa0a182e0bb4dc5760121e3e0e" translate="yes" xml:space="preserve">
          <source>The module &lt;code&gt;partial_dependence&lt;/code&gt; provides a convenience function &lt;a href=&quot;generated/sklearn.ensemble.partial_dependence.plot_partial_dependence#sklearn.ensemble.partial_dependence.plot_partial_dependence&quot;&gt;&lt;code&gt;plot_partial_dependence&lt;/code&gt;&lt;/a&gt; to create one-way and two-way partial dependence plots. In the below example we show how to create a grid of partial dependence plots: two one-way PDPs for the features &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt; and a two-way PDP between the two features:</source>
          <target state="translated">モジュール &lt;code&gt;partial_dependence&lt;/code&gt; は、一方向および双方向の部分依存プロットを作成するための便利な関数&lt;a href=&quot;generated/sklearn.ensemble.partial_dependence.plot_partial_dependence#sklearn.ensemble.partial_dependence.plot_partial_dependence&quot;&gt; &lt;code&gt;plot_partial_dependence&lt;/code&gt; &lt;/a&gt;を提供します。以下の例では、部分依存プロットのグリッドを作成する方法を示します &lt;code&gt;0&lt;/code&gt; フィーチャ0と &lt;code&gt;1&lt;/code&gt; の2つの一方向PDPと、2つのフィーチャ間の双方向PDP：</target>
        </trans-unit>
        <trans-unit id="5b2518b0fdafd75a6658a4d2a018bccb79345ce3" translate="yes" xml:space="preserve">
          <source>The module contains the public attributes &lt;code&gt;coefs_&lt;/code&gt; and &lt;code&gt;intercepts_&lt;/code&gt;. &lt;code&gt;coefs_&lt;/code&gt; is a list of weight matrices, where weight matrix at index \(i\) represents the weights between layer \(i\) and layer \(i+1\). &lt;code&gt;intercepts_&lt;/code&gt; is a list of bias vectors, where the vector at index \(i\) represents the bias values added to layer \(i+1\).</source>
          <target state="translated">このモジュールには、パブリック属性 &lt;code&gt;coefs_&lt;/code&gt; および &lt;code&gt;intercepts_&lt;/code&gt; が含まれています。 &lt;code&gt;coefs_&lt;/code&gt; は重み行列のリストです。インデックス\（i \）の重み行列は、レイヤー\（i \）とレイヤー\（i + 1 \）の間の重みを表します。 &lt;code&gt;intercepts_&lt;/code&gt; はバイアスベクトルのリストで、インデックス\（i \）のベクトルはレイヤー\（i + 1 \）に追加されたバイアス値を表します。</target>
        </trans-unit>
        <trans-unit id="b31fa7fd3ac8f2a64f0dd29549e8dd9abb96ee19" translate="yes" xml:space="preserve">
          <source>The module: &lt;code&gt;random_projection&lt;/code&gt; provides several tools for data reduction by random projections. See the relevant section of the documentation: &lt;a href=&quot;random_projection#random-projection&quot;&gt;Random Projection&lt;/a&gt;.</source>
          <target state="translated">モジュール： &lt;code&gt;random_projection&lt;/code&gt; は、ランダム投影によるデータ削減のためのいくつかのツールを提供します。ドキュメントの関連セクションを参照してください：&lt;a href=&quot;random_projection#random-projection&quot;&gt;ランダムプロジェクション&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="81f532ac90d157aeffd690ee0c3b7aa6b7bbd149" translate="yes" xml:space="preserve">
          <source>The monitor is called after each iteration with the current iteration, a reference to the estimator and the local variables of &lt;code&gt;_fit_stages&lt;/code&gt; as keyword arguments &lt;code&gt;callable(i, self,
locals())&lt;/code&gt;. If the callable returns &lt;code&gt;True&lt;/code&gt; the fitting procedure is stopped. The monitor can be used for various things such as computing held-out estimates, early stopping, model introspect, and snapshoting.</source>
          <target state="translated">モニターは、各反復の後に、現在の反復、推定器への参照、および &lt;code&gt;_fit_stages&lt;/code&gt; のローカル変数をキーワード引数 &lt;code&gt;callable(i, self, locals())&lt;/code&gt; として呼び出します。呼び出し可能オブジェクトが &lt;code&gt;True&lt;/code&gt; を返す場合、フィッティング手順は停止されます。モニターは、延期された見積もりの​​計算、早期停止、モデルのイントロスペクト、スナップショットなど、さまざまな目的に使用できます。</target>
        </trans-unit>
        <trans-unit id="52aa15c9df5da45110f63c8c26b8cfa477557d62" translate="yes" xml:space="preserve">
          <source>The most common parameter amenable to this strategy is the parameter encoding the strength of the regularizer. In this case we say that we compute the &lt;strong&gt;regularization path&lt;/strong&gt; of the estimator.</source>
          <target state="translated">この戦略を適用できる最も一般的なパラメーターは、レギュラライザーの強度をエンコードするパラメーターです。この場合、推定量の&lt;strong&gt;正則化パス&lt;/strong&gt;を計算するとします。</target>
        </trans-unit>
        <trans-unit id="3156312e704bdb91fdff12aa2e110d4f21e21302" translate="yes" xml:space="preserve">
          <source>The most intuitive way to do so is to use a bags of words representation:</source>
          <target state="translated">最も直感的な方法は、言葉の表現の袋を使うことです。</target>
        </trans-unit>
        <trans-unit id="399fde41b63b2ea676c5145583a3950879f6c9fa" translate="yes" xml:space="preserve">
          <source>The motivation to use this scaling include robustness to very small standard deviations of features and preserving zero entries in sparse data.</source>
          <target state="translated">このスケーリングを使用する動機は、特徴量の非常に小さな標準偏差に対するロバスト性と、疎なデータのゼロエントリを保存することです。</target>
        </trans-unit>
        <trans-unit id="3ec6281766fab2b8f9e9f9f6e92da7d4704e6316" translate="yes" xml:space="preserve">
          <source>The multi-task lasso allows to fit multiple regression problems jointly enforcing the selected features to be the same across tasks. This example simulates sequential measurements, each task is a time instant, and the relevant features vary in amplitude over time while being the same. The multi-task lasso imposes that features that are selected at one time point are select for all time point. This makes feature selection by the Lasso more stable.</source>
          <target state="translated">マルチタスク・レーザは、選択された特徴がタスク間で同じであることを強制することで、複数の回帰問題を共同で適合させることができます。この例は、連続した測定をシミュレートし、各タスクは時間の瞬間であり、関連する特徴は同じでありながら時間の経過とともに振幅が変化します。マルチタスクラッソは、ある時点で選択された特徴がすべての時点で選択されることを強制します。これにより、ラッソによる特徴の選択がより安定したものとなる。</target>
        </trans-unit>
        <trans-unit id="7b55b776834f44186e095c0df9ee2b704ce3630c" translate="yes" xml:space="preserve">
          <source>The multiclass definition here seems the most reasonable extension of the metric used in binary classification, though there is no certain consensus in the literature:</source>
          <target state="translated">ここでのマルチクラスの定義は、文献には一定のコンセンサスはありませんが、二値分類で使用されるメトリックの最も合理的な拡張であると思われます。</target>
        </trans-unit>
        <trans-unit id="5fdb408d7bc477f0762da630aa0f3aea39db3f13" translate="yes" xml:space="preserve">
          <source>The multiclass support is handled according to a one-vs-one scheme.</source>
          <target state="translated">マルチクラス対応は、1対1のスキームに従って処理されます。</target>
        </trans-unit>
        <trans-unit id="66359e593d021aa5ae2d568f4acc056ec0bf4a32" translate="yes" xml:space="preserve">
          <source>The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.</source>
          <target state="translated">多項式ナイーブベイズ分類器は、離散的な特徴(例えば、テキスト分類のための単語数)を持つ分類に適しています。多項分布は、通常、整数の特徴数を必要とします。しかし、実際には、tf-idfのような小数のカウントも有効です。</target>
        </trans-unit>
        <trans-unit id="9fcb660998dc7edb2d34f5dc5854df445bad9a92" translate="yes" xml:space="preserve">
          <source>The multiple metrics can be specified either as a list, tuple or set of predefined scorer names:</source>
          <target state="translated">複数のメトリクスは、リスト、タプル、または定義済みスコアラー名のセットのいずれかで指定することができます。</target>
        </trans-unit>
        <trans-unit id="a09e250651300d47ed9cd74b128d0a12a5aa8e3e" translate="yes" xml:space="preserve">
          <source>The name of the sample image loaded</source>
          <target state="translated">読み込んだサンプル画像の名前</target>
        </trans-unit>
        <trans-unit id="b853190860f83bcb94f9f4edb130e6f04adf4ae5" translate="yes" xml:space="preserve">
          <source>The names &lt;code&gt;vect&lt;/code&gt;, &lt;code&gt;tfidf&lt;/code&gt; and &lt;code&gt;clf&lt;/code&gt; (classifier) are arbitrary. We will use them to perform grid search for suitable hyperparameters below. We can now train the model with a single command:</source>
          <target state="translated">&lt;code&gt;vect&lt;/code&gt; 、 &lt;code&gt;tfidf&lt;/code&gt; 、および &lt;code&gt;clf&lt;/code&gt; （分類子）という名前は任意です。これらを使用して、以下の適切なハイパーパラメータのグリッド検索を実行します。これで、1つのコマンドでモデルをトレーニングできます。</target>
        </trans-unit>
        <trans-unit id="9146e93a1405c3d2cac65e0084d1b1c7a951b3b3" translate="yes" xml:space="preserve">
          <source>The names of the dataset columns</source>
          <target state="translated">データセットの列の名前</target>
        </trans-unit>
        <trans-unit id="a0fb0e8bf0410fa55523d36fa2a7a49ac4415117" translate="yes" xml:space="preserve">
          <source>The new dtype will be np.float32 or np.float64, depending on the original type. The function can create a copy or modify the argument depending on the argument copy.</source>
          <target state="translated">新しいdtypeは、元の型に応じてnp.float32またはnp.float64になります。この関数は、引数のコピーに応じて、コピーを作成したり、引数を修正したりすることができます。</target>
        </trans-unit>
        <trans-unit id="6d7b951e7afa9271dd7834467cb67e175d7e626f" translate="yes" xml:space="preserve">
          <source>The new entry \(d(u,v)\) is computed as follows,</source>
          <target state="translated">The new entry \(d(u,v)\)は、以下のように計算されます。</target>
        </trans-unit>
        <trans-unit id="c771668f5d4b4d7aa145f31455a67e2ba21af3ce" translate="yes" xml:space="preserve">
          <source>The next figure compares the results obtained for the different type of the weight concentration prior (parameter &lt;code&gt;weight_concentration_prior_type&lt;/code&gt;) for different values of &lt;code&gt;weight_concentration_prior&lt;/code&gt;. Here, we can see the value of the &lt;code&gt;weight_concentration_prior&lt;/code&gt; parameter has a strong impact on the effective number of active components obtained. We can also notice that large values for the concentration weight prior lead to more uniform weights when the type of prior is &amp;lsquo;dirichlet_distribution&amp;rsquo; while this is not necessarily the case for the &amp;lsquo;dirichlet_process&amp;rsquo; type (used by default).</source>
          <target state="translated">次の図は、前重量濃度の異なるタイプについて得られた結果（パラメータ比較 &lt;code&gt;weight_concentration_prior_type&lt;/code&gt; の異なる値に対して） &lt;code&gt;weight_concentration_prior&lt;/code&gt; を。ここでは、 &lt;code&gt;weight_concentration_prior&lt;/code&gt; パラメータの値が、取得されるアクティブなコンポーネントの有効数に強い影響を与えることがわかります。また、事前のタイプが「dirichlet_distribution」の場合、事前の濃度重みの値が大きいと重みがより均一になることにも注意できますが、これは必ずしも「dirichlet_process」タイプの場合とは限りません（デフォルトで使用されます）。</target>
        </trans-unit>
        <trans-unit id="b6d38fd34e131451ba8b974153991556ff8b9398" translate="yes" xml:space="preserve">
          <source>The next figure compares the time for fitting and prediction of &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;SVR&lt;/code&gt; for different sizes of the training set. Fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is faster than &lt;code&gt;SVR&lt;/code&gt; for medium-sized training sets (less than 1000 samples); however, for larger training sets &lt;code&gt;SVR&lt;/code&gt; scales better. With regard to prediction time, &lt;code&gt;SVR&lt;/code&gt; is faster than &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; for all sizes of the training set because of the learned sparse solution. Note that the degree of sparsity and thus the prediction time depends on the parameters \(\epsilon\) and \(C\) of the &lt;code&gt;SVR&lt;/code&gt;; \(\epsilon = 0\) would correspond to a dense model.</source>
          <target state="translated">次の図は、トレーニングセットのさまざまなサイズに対する&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt;と &lt;code&gt;SVR&lt;/code&gt; のフィッティングと予測の時間を比較しています。&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; の&lt;/a&gt;フィッティングは、中規模のトレーニングセット（1000サンプル未満）の &lt;code&gt;SVR&lt;/code&gt; よりも高速です。ただし、トレーニングセットが大きいほど、 &lt;code&gt;SVR&lt;/code&gt; のスケーリングが向上します。&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt;時間の点では、学習されたスパースソリューションのため、トレーニングセットのすべてのサイズで &lt;code&gt;SVR&lt;/code&gt; はKernelRidgeよりも高速です。スパース性の程度、したがって予測時間は、 &lt;code&gt;SVR&lt;/code&gt; のパラメーター\（\ epsilon \）および\（C \）に依存することに注意してください。 \（\ epsilon = 0 \）は密なモデルに対応します。</target>
        </trans-unit>
        <trans-unit id="7fb8a1fd588ff5df76899e15821355218c8c70df" translate="yes" xml:space="preserve">
          <source>The next figure compares the time for fitting and prediction of KRR and SVR for different sizes of the training set. Fitting KRR is faster than SVR for medium- sized training sets (less than 1000 samples); however, for larger training sets SVR scales better. With regard to prediction time, SVR is faster than KRR for all sizes of the training set because of the learned sparse solution. Note that the degree of sparsity and thus the prediction time depends on the parameters epsilon and C of the SVR.</source>
          <target state="translated">次の図は、訓練セットのサイズが異なる場合のKRRとSVRのフィッティングと予測の時間を比較したものです。中規模の訓練セット(1000サンプル以下)では、KRRのフィッティングはSVRよりも速いですが、大規模な訓練セットではSVRの方がスケールが大きくなります。予測時間に関しては、学習された疎な解のため、学習された訓練セットのすべてのサイズにおいて、SVRの方がKRRよりも高速です。疎さの程度と予測時間は、SVRのパラメータεとCに依存することに注意してください。</target>
        </trans-unit>
        <trans-unit id="b8b8c72913234ec998c925f50d9fa1a29ef7082f" translate="yes" xml:space="preserve">
          <source>The next image illustrates how sigmoid calibration changes predicted probabilities for a 3-class classification problem. Illustrated is the standard 2-simplex, where the three corners correspond to the three classes. Arrows point from the probability vectors predicted by an uncalibrated classifier to the probability vectors predicted by the same classifier after sigmoid calibration on a hold-out validation set. Colors indicate the true class of an instance (red: class 1, green: class 2, blue: class 3).</source>
          <target state="translated">次の画像は、シグモイド校正が3クラスの分類問題の予測確率をどのように変化させるかを示しています。図示されているのは、3つの角が3つのクラスに対応する標準の2-simplexです。矢印は、校正されていない分類器によって予測された確率ベクトルから、ホールドアウト検証セットでのシグモイド校正後の同じ分類器によって予測された確率ベクトルを指しています。色は、インスタンスの真のクラスを示します(赤:クラス1、緑:クラス2、青:クラス3)。</target>
        </trans-unit>
        <trans-unit id="d16a780143f0785e9248e298dec17c9fe8de9d1e" translate="yes" xml:space="preserve">
          <source>The nodes are random variables whose states depend on the state of the other nodes they are connected to. The model is therefore parameterized by the weights of the connections, as well as one intercept (bias) term for each visible and hidden unit, omitted from the image for simplicity.</source>
          <target state="translated">ノードは、その状態が接続されている他のノードの状態に依存するランダム変数である。したがって,モデルは,接続の重みと,可視単位と非表示単位のそれぞれについての切片(バイアス)項によってパラメータ化されています(図では簡略化のために省略しています).</target>
        </trans-unit>
        <trans-unit id="55f688a90e745dd5020f6b5c567bbe8f6396fa6e" translate="yes" xml:space="preserve">
          <source>The noise level in the targets can be specified by passing it via the parameter &lt;code&gt;alpha&lt;/code&gt;, either globally as a scalar or per datapoint. Note that a moderate noise level can also be helpful for dealing with numeric issues during fitting as it is effectively implemented as Tikhonov regularization, i.e., by adding it to the diagonal of the kernel matrix. An alternative to specifying the noise level explicitly is to include a WhiteKernel component into the kernel, which can estimate the global noise level from the data (see example below).</source>
          <target state="translated">ターゲットのノイズレベルは、パラメーター &lt;code&gt;alpha&lt;/code&gt; を介してグローバルに、またはデータポイントごとに渡すことで指定できます。適度なノイズレベルは、ティコノフの正則化として効果的に実装されるため、フィッティング中に数値の問題を処理する場合にも役立ちます。つまり、カーネルマトリックスの対角に追加します。ノイズレベルを明示的に指定する代わりに、カーネルにWhiteKernelコンポーネントを含めることもできます。これにより、データからグローバルノイズレベルを推定できます（以下の例を参照）。</target>
        </trans-unit>
        <trans-unit id="99b58f648d173c7793dc0e913318a8835572f0c2" translate="yes" xml:space="preserve">
          <source>The non-fixed, log-transformed hyperparameters of the kernel</source>
          <target state="translated">カーネルの非固定対数変換されたハイパーパラメータ</target>
        </trans-unit>
        <trans-unit id="8ea3cf8563db967f688cb46c0883a9d020905a15" translate="yes" xml:space="preserve">
          <source>The nonmetric algorithm adds a monotonic regression step before computing the stress.</source>
          <target state="translated">非メトリックアルゴリズムは、応力を計算する前に単調回帰ステップを追加します。</target>
        </trans-unit>
        <trans-unit id="8cecdfcc92ccd83125ecaa6c4beb7447e43f92cb" translate="yes" xml:space="preserve">
          <source>The norm to use to normalize each non zero sample (or each non-zero feature if axis is 0).</source>
          <target state="translated">各非ゼロサンプル(軸が0の場合は各非ゼロ特徴量)を正規化するために使用するノルム。</target>
        </trans-unit>
        <trans-unit id="39e8a7c3ff72c23082d4ee16a84d74279c8584ba" translate="yes" xml:space="preserve">
          <source>The norm to use to normalize each non zero sample.</source>
          <target state="translated">各非ゼロサンプルを正規化するために使用するノルム。</target>
        </trans-unit>
        <trans-unit id="b4510db24d586496f5cd27e9cd8024ffe16a368e" translate="yes" xml:space="preserve">
          <source>The normalized mutual information is defined as</source>
          <target state="translated">正規化された相互情報は</target>
        </trans-unit>
        <trans-unit id="ad22e0beb8a3a9f5574d757b3678d39d78057ba3" translate="yes" xml:space="preserve">
          <source>The normalizer instance can then be used on sample vectors as any transformer:</source>
          <target state="translated">ノーマライザのインスタンスは、他の変換器と同様にサンプルベクトル上で使用することができます。</target>
        </trans-unit>
        <trans-unit id="82d79be22b3b3fa23f79d393f8a5b628a27ddb08" translate="yes" xml:space="preserve">
          <source>The number k of neighbors considered, (alias parameter n_neighbors) is typically chosen 1) greater than the minimum number of objects a cluster has to contain, so that other objects can be local outliers relative to this cluster, and 2) smaller than the maximum number of close by objects that can potentially be local outliers. In practice, such informations are generally not available, and taking n_neighbors=20 appears to work well in general. When the proportion of outliers is high (i.e. greater than 10 %, as in the example below), n_neighbors should be greater (n_neighbors=35 in the example below).</source>
          <target state="translated">考慮される隣人の数k(別名パラメータn_neighbors)は、一般的に、1)他のオブジェクトがこのクラスタに対して局所的な外れ値となり得るように、クラスタが含まなければならないオブジェクトの最小数よりも大きく、2)局所的な外れ値となり得る近くのオブジェクトの最大数よりも小さい値が選ばれます。実際には、このような情報は一般的には得られず、n_neighbors=20とすることが一般的にはうまくいくようです。外れ値の割合が高い場合(以下の例のように10%よりも大きい場合)、n_neighborsはより大きくすべきです(以下の例ではn_neighbors=35)。</target>
        </trans-unit>
        <trans-unit id="4a35a9d90f2abd9af864433913570f3dbe7f2765" translate="yes" xml:space="preserve">
          <source>The number of CPUs to use to do the OVA (One Versus All, for multi-class problems) computation. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">OVA（マルチクラス問題の場合は1つとすべて）の計算に使用するCPUの数。 &lt;code&gt;None&lt;/code&gt; は、&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="36ae328d3607f79ec631b3fe327da7ed8cf8098b" translate="yes" xml:space="preserve">
          <source>The number of CPUs to use to do the computation. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">計算に使用するCPUの数。 &lt;code&gt;None&lt;/code&gt; は、&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="720879c36f3df22b5ec2b112039efcd8b5ba8b1b" translate="yes" xml:space="preserve">
          <source>The number of EM iterations to perform.</source>
          <target state="translated">実行するEMの反復回数。</target>
        </trans-unit>
        <trans-unit id="40648db4f1524a67ed66976c14cc75bac6eed386" translate="yes" xml:space="preserve">
          <source>The number of atomic tasks to dispatch at once to each worker. When individual evaluations are very fast, dispatching calls to workers can be slower than sequential computation because of the overhead. Batching fast computations together can mitigate this. The &lt;code&gt;'auto'&lt;/code&gt; strategy keeps track of the time it takes for a batch to complete, and dynamically adjusts the batch size to keep the time on the order of half a second, using a heuristic. The initial batch size is 1. &lt;code&gt;batch_size=&quot;auto&quot;&lt;/code&gt; with &lt;code&gt;backend=&quot;threading&quot;&lt;/code&gt; will dispatch batches of a single task at a time as the threading backend has very little overhead and using larger batch size has not proved to bring any gain in that case.</source>
          <target state="translated">各ワーカーに一度にディスパッチするアトミックタスクの数。個々の評価が非常に高速である場合、オーバーヘッドのため、ワーカーへの呼び出しのディスパッチは順次計算よりも遅くなる可能性があります。高速計算をバッチ処理すると、これを軽減できます。 &lt;code&gt;'auto'&lt;/code&gt; の戦略は、それが完全にバッチにかかる時間を追跡し、動的ヒューリスティックを使用して、半秒のオーダーの時間を保つためにバッチサイズを調整します。最初のバッチサイズは1です。 &lt;code&gt;batch_size=&quot;auto&quot;&lt;/code&gt; と &lt;code&gt;backend=&quot;threading&quot;&lt;/code&gt; スレッドのバックエンドは非常にわずかなオーバーヘッドがあるとして、一度に単一のタスクのバッチを派遣し、より大きなバッチサイズを使用すると、その中の任意の利得をもたらすことが証明されていません場合。</target>
        </trans-unit>
        <trans-unit id="3ae39cb6a942b138204dbb141781bfb4c75e3760" translate="yes" xml:space="preserve">
          <source>The number of base estimators in the ensemble.</source>
          <target state="translated">アンサンブルに含まれる基底推定器の数。</target>
        </trans-unit>
        <trans-unit id="9fe1c6517ea4c36af295e91a2e2410361ff8432c" translate="yes" xml:space="preserve">
          <source>The number of batches (of tasks) to be pre-dispatched. Default is &amp;lsquo;2*n_jobs&amp;rsquo;. When batch_size=&amp;rdquo;auto&amp;rdquo; this is reasonable default and the workers should never starve.</source>
          <target state="translated">事前ディスパッチされる（タスクの）バッチの数。デフォルトは「2 * n_jobs」です。batch_size =&amp;rdquo; auto&amp;rdquo;の場合、これは妥当なデフォルトであり、ワーカーが決して飢えてはいけません。</target>
        </trans-unit>
        <trans-unit id="f9d0a981566d58378881b2cabe57053da9d34fb6" translate="yes" xml:space="preserve">
          <source>The number of biclusters to find.</source>
          <target state="translated">ビックラスターの数字の語呂合わせを見つけます。</target>
        </trans-unit>
        <trans-unit id="b9c52ec2125ed0e2339a6eae69fb246b4dc5348e" translate="yes" xml:space="preserve">
          <source>The number of biclusters.</source>
          <target state="translated">バイクラスターの数です。</target>
        </trans-unit>
        <trans-unit id="3424019fa75a2f96f62b30f8336ea4cebfa5c4fe" translate="yes" xml:space="preserve">
          <source>The number of bins to produce. The intervals for the bins are determined by the minimum and maximum of the input data. Raises ValueError if &lt;code&gt;n_bins &amp;lt; 2&lt;/code&gt;.</source>
          <target state="translated">生成するビンの数。ビンの間隔は、入力データの最小値と最大値によって決まります。 &lt;code&gt;n_bins &amp;lt; 2&lt;/code&gt; 場合、ValueErrorを発生させます。</target>
        </trans-unit>
        <trans-unit id="016567b86fa6f6a4b4c043763b4e841bd445fc32" translate="yes" xml:space="preserve">
          <source>The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.</source>
          <target state="translated">ブーストするステージの数を指定します。勾配ブーストはオーバーフィットにかなり強いので、通常は数が多い方がパフォーマンスが向上します。</target>
        </trans-unit>
        <trans-unit id="d31e8a7065eae23aeb71500302a31ebb485560ec" translate="yes" xml:space="preserve">
          <source>The number of classes</source>
          <target state="translated">クラスの数</target>
        </trans-unit>
        <trans-unit id="77a0204799f583fccb414a4215d0dc841510a2f7" translate="yes" xml:space="preserve">
          <source>The number of classes (for single output problems), or a list containing the number of classes for each output (for multi-output problems).</source>
          <target state="translated">クラスの数(単一出力問題の場合)、または各出力(複数出力問題の場合)のクラスの数を含むリスト。</target>
        </trans-unit>
        <trans-unit id="221daa93ea83049b71eff667b2da7d0ce3fa89ae" translate="yes" xml:space="preserve">
          <source>The number of classes (or labels) of the classification problem.</source>
          <target state="translated">分類問題のクラス(またはラベル)の数。</target>
        </trans-unit>
        <trans-unit id="7b75bf8cc583f50195e96e51d244ce57f23360a0" translate="yes" xml:space="preserve">
          <source>The number of classes (single output problem), or a list containing the number of classes for each output (multi-output problem).</source>
          <target state="translated">クラスの数(単一出力問題)、または各出力(複数出力問題)のクラス数を含むリスト。</target>
        </trans-unit>
        <trans-unit id="8b5fc0d622c5abb76da13c6a8f50c84a211eca46" translate="yes" xml:space="preserve">
          <source>The number of classes in the training data</source>
          <target state="translated">学習データのクラス数</target>
        </trans-unit>
        <trans-unit id="a096e01744ef01b9d139303dfd4a94a83569986b" translate="yes" xml:space="preserve">
          <source>The number of classes of the classification problem.</source>
          <target state="translated">分類問題のクラス数。</target>
        </trans-unit>
        <trans-unit id="21583bce2023d80fa6dedc801ccace76e8a2445f" translate="yes" xml:space="preserve">
          <source>The number of classes to return.</source>
          <target state="translated">返すクラスの数。</target>
        </trans-unit>
        <trans-unit id="7defdc95bb1ddc0580ac0be76fc251278b72fb84" translate="yes" xml:space="preserve">
          <source>The number of classes.</source>
          <target state="translated">クラスの数です。</target>
        </trans-unit>
        <trans-unit id="062fe5ee9cf896a5c74f9ba057f89b8b6de5fa8a" translate="yes" xml:space="preserve">
          <source>The number of clusters per class.</source>
          <target state="translated">クラスごとのクラスター数。</target>
        </trans-unit>
        <trans-unit id="21f23a23e06d5c295fade98ab37ab55d16e621ca" translate="yes" xml:space="preserve">
          <source>The number of clusters to find.</source>
          <target state="translated">見つけるべきクラスターの数。</target>
        </trans-unit>
        <trans-unit id="b4184f0399110f5f8e690a26513dd6f78511c0a5" translate="yes" xml:space="preserve">
          <source>The number of clusters to form as well as the number of centroids to generate.</source>
          <target state="translated">形成するクラスターの数だけでなく、生成するセントロイドの数を指定します。</target>
        </trans-unit>
        <trans-unit id="35d84f4a157603ef94bd9baafa0d524311104205" translate="yes" xml:space="preserve">
          <source>The number of columns in the grid plot (default: 3).</source>
          <target state="translated">グリッドプロットの列数(デフォルトは3)。</target>
        </trans-unit>
        <trans-unit id="25441c707266953707d0c752071de32d0051d235" translate="yes" xml:space="preserve">
          <source>The number of connected components in the graph.</source>
          <target state="translated">グラフ内の接続された構成要素の数。</target>
        </trans-unit>
        <trans-unit id="5d17146f816382e55c9752b437d1e0a90ca8e256" translate="yes" xml:space="preserve">
          <source>The number of cross-validation splits (folds/iterations).</source>
          <target state="translated">クロスバリデーションの分割数(ひだ/反復)。</target>
        </trans-unit>
        <trans-unit id="aa32f92ae0454e6e9ee52207fe93d7eb6600c995" translate="yes" xml:space="preserve">
          <source>The number of degrees of freedom of each components in the model.</source>
          <target state="translated">モデルの各構成要素の自由度数。</target>
        </trans-unit>
        <trans-unit id="b2ef77453518fcb650d3ae2f90fcfda15611a36f" translate="yes" xml:space="preserve">
          <source>The number of duplicated features, drawn randomly from the informative and the redundant features.</source>
          <target state="translated">情報特徴量と冗長特徴量からランダムに抽出された重複特徴量の数。</target>
        </trans-unit>
        <trans-unit id="f3756e321fd8c5762ae8ac31516f2ecb21110717" translate="yes" xml:space="preserve">
          <source>The number of equally spaced points on the &lt;code&gt;grid&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;grid&lt;/code&gt; 上の等間隔のポイントの数。</target>
        </trans-unit>
        <trans-unit id="b29bfd2ea8e3e1682dd4a79d1706f446dfb38a05" translate="yes" xml:space="preserve">
          <source>The number of equally spaced points on the axes.</source>
          <target state="translated">軸上に等間隔に配置された点の数。</target>
        </trans-unit>
        <trans-unit id="a2df871c8f4e92cb59ed15324ccda38fad42ea75" translate="yes" xml:space="preserve">
          <source>The number of estimators as selected by early stopping (if &lt;code&gt;n_iter_no_change&lt;/code&gt; is specified). Otherwise it is set to &lt;code&gt;n_estimators&lt;/code&gt;.</source>
          <target state="translated">早期停止によって選択された推定子の数（ &lt;code&gt;n_iter_no_change&lt;/code&gt; が指定されている場合）。それ以外の場合は &lt;code&gt;n_estimators&lt;/code&gt; に設定されます。</target>
        </trans-unit>
        <trans-unit id="1c41aa32f1f0f6cf009b98065236eda5fafde67a" translate="yes" xml:space="preserve">
          <source>The number of features (columns) in the output matrices. Small numbers of features are likely to cause hash collisions, but large numbers will cause larger coefficient dimensions in linear learners.</source>
          <target state="translated">出力行列の特徴量(列)の数。特徴の数が少ないとハッシュ衝突が起こりやすくなりますが、数が多いと線形学習者の係数次元が大きくなります。</target>
        </trans-unit>
        <trans-unit id="586a175a91db906a71d554d2394ac768c54a011d" translate="yes" xml:space="preserve">
          <source>The number of features for each sample.</source>
          <target state="translated">各サンプルの特徴量の数。</target>
        </trans-unit>
        <trans-unit id="7aa2f8ce84af9869f7a766d49383cc26f37d08c4" translate="yes" xml:space="preserve">
          <source>The number of features has to be &amp;gt;= 5.</source>
          <target state="translated">機能の数は5以上でなければなりません。</target>
        </trans-unit>
        <trans-unit id="129d21ac97bd672e4633231039dccb80b794a16c" translate="yes" xml:space="preserve">
          <source>The number of features to consider when looking for the best split:</source>
          <target state="translated">最適なスプリットを探す際に考慮すべき機能の数です。</target>
        </trans-unit>
        <trans-unit id="7bfa1c66757d06ac8d59fc4bf744ebce5057ba13" translate="yes" xml:space="preserve">
          <source>The number of features to draw from X to train each base estimator.</source>
          <target state="translated">各基底推定器を訓練するためにXから描画する特徴量の数。</target>
        </trans-unit>
        <trans-unit id="d51bb9401f1843316f34a353f5592cb098582ce3" translate="yes" xml:space="preserve">
          <source>The number of features to select. If &lt;code&gt;None&lt;/code&gt;, half of the features are selected.</source>
          <target state="translated">選択する機能の数。 &lt;code&gt;None&lt;/code&gt; の場合、機能の半分が選択されます。</target>
        </trans-unit>
        <trans-unit id="9a2e8b8c9f83e59315eadeb30286733009f73579" translate="yes" xml:space="preserve">
          <source>The number of features to use. If None, it will be inferred from the maximum column index occurring in any of the files.</source>
          <target state="translated">使用する機能の数。Noneの場合は、いずれかのファイルに存在する最大カラムインデックスから推測されます。</target>
        </trans-unit>
        <trans-unit id="98598b839c3ae52a8bef761a8c292e4971f87fa1" translate="yes" xml:space="preserve">
          <source>The number of features to use. If None, it will be inferred. This argument is useful to load several files that are subsets of a bigger sliced dataset: each subset might not have examples of every feature, hence the inferred shape might vary from one slice to another. n_features is only required if &lt;code&gt;offset&lt;/code&gt; or &lt;code&gt;length&lt;/code&gt; are passed a non-default value.</source>
          <target state="translated">使用する機能の数。Noneの場合、推測されます。この引数は、より大きなスライスデータセットのサブセットである複数のファイルをロードするのに役立ちます。各サブセットにはすべての機能の例がない場合があるため、推定される形状はスライスごとに異なる場合があります。n_featuresは、 &lt;code&gt;offset&lt;/code&gt; または &lt;code&gt;length&lt;/code&gt; デフォルト以外の値が渡された場合にのみ必要です。</target>
        </trans-unit>
        <trans-unit id="deca4fdfd1ca37b7e04bbcf3985c1b98fb8a3a95" translate="yes" xml:space="preserve">
          <source>The number of features when &lt;code&gt;fit&lt;/code&gt; is performed.</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; が実行されたときの特徴の数。</target>
        </trans-unit>
        <trans-unit id="000ec70bd77b0bf4abd3e711fc085eb14c1166a9" translate="yes" xml:space="preserve">
          <source>The number of features.</source>
          <target state="translated">特徴の数です。</target>
        </trans-unit>
        <trans-unit id="9910eba7c229f9255e2c9649c38205ffc855802c" translate="yes" xml:space="preserve">
          <source>The number of features. Should be at least 5.</source>
          <target state="translated">機能の数。5以上である必要があります。</target>
        </trans-unit>
        <trans-unit id="6b858bf37b4f474c4599d32f8587786621c82cda" translate="yes" xml:space="preserve">
          <source>The number of informative features, i.e., the number of features used to build the linear model used to generate the output.</source>
          <target state="translated">情報的特徴量の数、すなわち、出力を生成するために使用される線形モデルを構築するために使用される特徴量の数。</target>
        </trans-unit>
        <trans-unit id="1a77292033054c608772a804dab5a2e1fdf27c5c" translate="yes" xml:space="preserve">
          <source>The number of informative features. Each class is composed of a number of gaussian clusters each located around the vertices of a hypercube in a subspace of dimension &lt;code&gt;n_informative&lt;/code&gt;. For each cluster, informative features are drawn independently from N(0, 1) and then randomly linearly combined within each cluster in order to add covariance. The clusters are then placed on the vertices of the hypercube.</source>
          <target state="translated">有益な機能の数。各クラスは、次元 &lt;code&gt;n_informative&lt;/code&gt; の部分空間内の超立方体の頂点の周りにそれぞれ配置されたいくつかのガウスクラスターで構成されています。各クラスターについて、有益な特徴がN（0、1）から独立して描画され、共分散を追加するために各クラスター内でランダムに線形結合されます。次に、クラスターをハイパーキューブの頂点に配置します。</target>
        </trans-unit>
        <trans-unit id="8860e037fbe039fe78d5e3c48f8e1848feb12312" translate="yes" xml:space="preserve">
          <source>The number of initializations to perform. The best results are kept.</source>
          <target state="translated">実行する初期化の数です。最良の結果を保持します。</target>
        </trans-unit>
        <trans-unit id="51a26d62cae82295bdfa00f11021a221252f4d93" translate="yes" xml:space="preserve">
          <source>The number of initializations to perform. The result with the highest lower bound value on the likelihood is kept.</source>
          <target state="translated">実行する初期化の数。尤度の下限値が最も高い結果が保持されます。</target>
        </trans-unit>
        <trans-unit id="38ff309a985e30ab2e29b9afc192f79c8b9a0c6a" translate="yes" xml:space="preserve">
          <source>The number of integer to sample.</source>
          <target state="translated">サンプルする整数の数。</target>
        </trans-unit>
        <trans-unit id="a52a9529854ceea9a883fc2df829925e52c70f47" translate="yes" xml:space="preserve">
          <source>The number of iteration on data batches that has been performed before this call to partial_fit. This is optional: if no number is passed, the memory of the object is used.</source>
          <target state="translated">このpartial_fitの呼び出しの前に実行されたデータバッチの反復回数。これはオプションです:数値が渡されなかった場合は、オブジェクトのメモリが使用されます。</target>
        </trans-unit>
        <trans-unit id="a3bc7b28196b3922c89415e845096f6cf78b8faf" translate="yes" xml:space="preserve">
          <source>The number of iterations corresponding to the best stress. Returned only if &lt;code&gt;return_n_iter&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">最高のストレスに対応する反復回数。 &lt;code&gt;return_n_iter&lt;/code&gt; が &lt;code&gt;True&lt;/code&gt; に設定されている場合にのみ返されます。</target>
        </trans-unit>
        <trans-unit id="09e334ba47a4a0e3959de08638739eb9eaaab635" translate="yes" xml:space="preserve">
          <source>The number of iterations taken by lars_path to find the grid of alphas for each target.</source>
          <target state="translated">各ターゲットのアルファのグリッドを見つけるために lars_path によって取られた反復回数.</target>
        </trans-unit>
        <trans-unit id="b1098a1922ae37c291de7345b0094c64c3a02834" translate="yes" xml:space="preserve">
          <source>The number of iterations taken by the coordinate descent optimizer to reach the specified tolerance for each alpha.</source>
          <target state="translated">座標降下オプティマイザが、各アルファについて指定された許容範囲に到達するまでに要した反復回数。</target>
        </trans-unit>
        <trans-unit id="cb53eef7959113120386cfa7066166d0b269478f" translate="yes" xml:space="preserve">
          <source>The number of iterations taken by the coordinate descent optimizer to reach the specified tolerance for each alpha. (Is returned when &lt;code&gt;return_n_iter&lt;/code&gt; is set to True).</source>
          <target state="translated">座標降下オプティマイザが各アルファの指定された許容値に到達するために行う反復の数。（ &lt;code&gt;return_n_iter&lt;/code&gt; がTrueに設定されている場合に返されます）。</target>
        </trans-unit>
        <trans-unit id="33de6e55bb60c2bc5ac457a31b105deefc3f996b" translate="yes" xml:space="preserve">
          <source>The number of iterations the solver has ran.</source>
          <target state="translated">ソルバーが実行した反復回数。</target>
        </trans-unit>
        <trans-unit id="db80deec4964c14c90bbb2ba0d38dab8d92c99f4" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for &lt;code&gt;fit&lt;/code&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; のために並行して実行するジョブの数。 &lt;code&gt;None&lt;/code&gt; は、&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="828e81cc3e32985aa18f25c0aa8cb224a18c0ff7" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for both &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; と &lt;code&gt;predict&lt;/code&gt; 両方で並行して実行するジョブの数。 &lt;code&gt;None&lt;/code&gt; は、&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="c43ba6eb14ab669b0479493b6caf6c135c124b5c" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for both &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;. &lt;code&gt;None`&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; と &lt;code&gt;predict&lt;/code&gt; 両方で並行して実行するジョブの数。 &lt;code&gt;None`&lt;/code&gt; は、&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="1c706846846b90bc0cd14952c05d5f24ee8d014f" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">計算に使用するジョブの数。 &lt;code&gt;None&lt;/code&gt; は、&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="6833984bce80d83c32632eedfcb38fdab54d1b8b" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. If multiple initializations are used (&lt;code&gt;n_init&lt;/code&gt;), each run of the algorithm is computed in parallel.</source>
          <target state="translated">計算に使用するジョブの数。複数の初期化が使用されている場合（ &lt;code&gt;n_init&lt;/code&gt; ）、アルゴリズムの各実行は並列で計算されます。</target>
        </trans-unit>
        <trans-unit id="6691b8acedaea4810fafdb230140a5bee73c0f13" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. It does each target variable in y in parallel. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">計算に使用するジョブの数。yの各ターゲット変数を並行して実行します。 &lt;code&gt;None&lt;/code&gt; は、&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="e3d27815195706836eb73a8c598c8129b01e46ca" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. This will only provide speedup for n_targets &amp;gt; 1 and sufficient large problems. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">計算に使用するジョブの数。これは、n_targets&amp;gt; 1および十分に大きな問題に対してのみ高速化を提供します。 &lt;code&gt;None&lt;/code&gt; は、&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="aea0fe9213de8cdb23ff0f2fe182f38ade8bf1a8" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. This works by breaking down the pairwise matrix into n_jobs even slices and computing them in parallel.</source>
          <target state="translated">計算に使用するジョブの数。これは、ペアワイズ行列をn_jobsの偶数スライスに分解し、並列に計算することで動作します。</target>
        </trans-unit>
        <trans-unit id="355b7ccfb662137f73492d9f4ce45fbb16afbbbc" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel.</source>
          <target state="translated">計算に使用するジョブの数を指定します。これは、n_initの各実行を並列に計算することで動作します。</target>
        </trans-unit>
        <trans-unit id="a5a46de70d97dcbce1cd57e62c5d0b48ff30934b" translate="yes" xml:space="preserve">
          <source>The number of jobs to use in the E-step. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">Eステップで使用するジョブの数。 &lt;code&gt;None&lt;/code&gt; は、&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="27ef8dc8b90e4adb5cf60c4b5861107cfc3fcd66" translate="yes" xml:space="preserve">
          <source>The number of leaves in the tree</source>
          <target state="translated">木の葉の数</target>
        </trans-unit>
        <trans-unit id="aa280d8fa86c4668dcae50e829dc6fe4b82c5c09" translate="yes" xml:space="preserve">
          <source>The number of longitudes (x) and latitudes (y) in the grid</source>
          <target state="translated">グリッド内の長さ(x)と緯度(y)の数</target>
        </trans-unit>
        <trans-unit id="c1079f8f5554d74d12065ae30d0d0d5ad6da869f" translate="yes" xml:space="preserve">
          <source>The number of mixture components.</source>
          <target state="translated">混合成分の数です。</target>
        </trans-unit>
        <trans-unit id="1b5111fc1060a675cec3a1f3743c5b7235e4d74d" translate="yes" xml:space="preserve">
          <source>The number of mixture components. Depending on the data and the value of the &lt;code&gt;weight_concentration_prior&lt;/code&gt; the model can decide to not use all the components by setting some component &lt;code&gt;weights_&lt;/code&gt; to values very close to zero. The number of effective components is therefore smaller than n_components.</source>
          <target state="translated">混合コンポーネントの数。データの値に応じて、 &lt;code&gt;weight_concentration_prior&lt;/code&gt; モデルは、いくつかのコンポーネント設定することで、すべてのコンポーネントを使用しないことを決定することができ &lt;code&gt;weights_&lt;/code&gt; を非常にゼロに近い値に。したがって、有効なコンポーネントの数はn_componentsよりも少なくなります。</target>
        </trans-unit>
        <trans-unit id="add0998b97eff8ce13b181824a749694c3eae657" translate="yes" xml:space="preserve">
          <source>The number of nearest neighbors to return</source>
          <target state="translated">返すべき最も近い隣人の数</target>
        </trans-unit>
        <trans-unit id="15fc684195ef8537dde92583b88b14e2ce9227a5" translate="yes" xml:space="preserve">
          <source>The number of neighbors considered (parameter n_neighbors) is typically set 1) greater than the minimum number of samples a cluster has to contain, so that other samples can be local outliers relative to this cluster, and 2) smaller than the maximum number of close by samples that can potentially be local outliers. In practice, such informations are generally not available, and taking n_neighbors=20 appears to work well in general.</source>
          <target state="translated">考慮される隣人の数(パラメータ n_neighbors)は、一般的に、1)他のサンプルがこのクラスターに対して局所的な外れ値となり得るように、クラスターが含まなければならないサンプルの最小数よりも大きく、2)局所的な外れ値となり得る近くのサンプルの最大数よりも小さく設定されています。実際には、このような情報は一般的には得られず、n_neighbors=20とするのが一般的にはうまくいくようです。</target>
        </trans-unit>
        <trans-unit id="acccc5b01db683b034e704a8a9a779e161564526" translate="yes" xml:space="preserve">
          <source>The number of neighbors considered, (parameter n_neighbors) is typically set 1) greater than the minimum number of samples a cluster has to contain, so that other samples can be local outliers relative to this cluster, and 2) smaller than the maximum number of close by samples that can potentially be local outliers. In practice, such informations are generally not available, and taking n_neighbors=20 appears to work well in general.</source>
          <target state="translated">考慮される隣人の数(パラメータ n_neighbors)は、一般的に、1)クラスターに含まれるサンプルの最小数よりも大きく設定され、他のサンプルがこのクラスターに対して局所的な外れ値になる可能性があるため、2)局所的な外れ値になる可能性がある近くのサンプルの最大数よりも小さく設定されます。実際には、このような情報は一般的には得られず、n_neighbors=20とするのが一般的にはうまくいくようです。</target>
        </trans-unit>
        <trans-unit id="877763fdbf37d108bc07acba2c3c7a43a6b1f5d0" translate="yes" xml:space="preserve">
          <source>The number of occurrences of each label in &lt;code&gt;y_true&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;y_true&lt;/code&gt; 内の各ラベルの出現回数。</target>
        </trans-unit>
        <trans-unit id="d77e127c483a951faddd4898060a91624b32c7e2" translate="yes" xml:space="preserve">
          <source>The number of outlying points matters, but also how much they are outliers.</source>
          <target state="translated">外れ点の数も重要ですが、どれだけ外れているかも重要です。</target>
        </trans-unit>
        <trans-unit id="28c74611a0fbfe9d7c9bc15166aba9285108f840" translate="yes" xml:space="preserve">
          <source>The number of outputs when &lt;code&gt;fit&lt;/code&gt; is performed.</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; が実行されたときの出力の数。</target>
        </trans-unit>
        <trans-unit id="bb79176b795c17c39b28af054f09df09e008175b" translate="yes" xml:space="preserve">
          <source>The number of outputs.</source>
          <target state="translated">出力数です。</target>
        </trans-unit>
        <trans-unit id="b09418506b739dbda708239c423400be69d20b7d" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search.</source>
          <target state="translated">近隣探索のために実行する並列ジョブの数。</target>
        </trans-unit>
        <trans-unit id="f8c9b6b47de0b026f28768b5a0afa5c012cbf5fc" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">ネイバー検索で実行する並列ジョブの数。 &lt;code&gt;None&lt;/code&gt; は、&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="7704c4671581e69d735cac67f4c8350e13fe7ef2" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details. Affects only &lt;a href=&quot;#sklearn.neighbors.LocalOutlierFactor.kneighbors&quot;&gt;&lt;code&gt;kneighbors&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt;&lt;code&gt;kneighbors_graph&lt;/code&gt;&lt;/a&gt; methods.</source>
          <target state="translated">ネイバー検索で実行する並列ジョブの数。 &lt;code&gt;None&lt;/code&gt; は、&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。&lt;a href=&quot;#sklearn.neighbors.LocalOutlierFactor.kneighbors&quot;&gt; &lt;code&gt;kneighbors&lt;/code&gt; &lt;/a&gt;および&lt;a href=&quot;sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt; &lt;code&gt;kneighbors_graph&lt;/code&gt; &lt;/a&gt;メソッドのみに影響します。</target>
        </trans-unit>
        <trans-unit id="361059c7cf2c1088b100ce86f03251d9b9b3606a" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details. Doesn&amp;rsquo;t affect &lt;a href=&quot;#sklearn.neighbors.KNeighborsClassifier.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="translated">ネイバー検索で実行する並列ジョブの数。 &lt;code&gt;None&lt;/code&gt; は、&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。&lt;a href=&quot;#sklearn.neighbors.KNeighborsClassifier.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt;方法には影響しません。</target>
        </trans-unit>
        <trans-unit id="0d88911bb44ab8ec6ef4a44ece33bbbd3a1ce8ce" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details. Doesn&amp;rsquo;t affect &lt;a href=&quot;#sklearn.neighbors.KNeighborsRegressor.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="translated">ネイバー検索で実行する並列ジョブの数。 &lt;code&gt;None&lt;/code&gt; は、&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。&lt;a href=&quot;#sklearn.neighbors.KNeighborsRegressor.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt;方法には影響しません。</target>
        </trans-unit>
        <trans-unit id="92152098131975c56771bce4e909262b46d5eb7d" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">実行する並列ジョブの数。 &lt;code&gt;None&lt;/code&gt; は、&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="99143dd67a79337f4ad9e3dbea2cd1e515a938df" translate="yes" xml:space="preserve">
          <source>The number of passes over the training data (aka epochs). Defaults to None. Deprecated, will be removed in 0.21.</source>
          <target state="translated">学習データを通過する回数(エポック)。デフォルトは None です。非推奨。0.21で削除される予定です。</target>
        </trans-unit>
        <trans-unit id="ec5cdfb884714451da411c4ff8cb1db87b4e7636" translate="yes" xml:space="preserve">
          <source>The number of redundant features. These features are generated as random linear combinations of the informative features.</source>
          <target state="translated">冗長特徴量の数である。これらの特徴は、情報的特徴のランダムな線形の組み合わせとして生成される。</target>
        </trans-unit>
        <trans-unit id="47b1c5caf88dbd07fabbf6968de5281c23c7d24f" translate="yes" xml:space="preserve">
          <source>The number of regression targets, i.e., the dimension of the y output vector associated with a sample. By default, the output is a scalar.</source>
          <target state="translated">回帰ターゲットの数、すなわち、サンプルに関連するy出力ベクトルの次元。デフォルトでは,出力はスカラです.</target>
        </trans-unit>
        <trans-unit id="338ec305a58ac58159822d262713a3e274e16037" translate="yes" xml:space="preserve">
          <source>The number of restarts of the optimizer for finding the kernel&amp;rsquo;s parameters which maximize the log-marginal likelihood. The first run of the optimizer is performed from the kernel&amp;rsquo;s initial parameters, the remaining ones (if any) from thetas sampled log-uniform randomly from the space of allowed theta-values. If greater than 0, all bounds must be finite. Note that n_restarts_optimizer == 0 implies that one run is performed.</source>
          <target state="translated">対数限界尤度を最大化するカーネルのパラメーターを見つけるためのオプティマイザーの再起動回数。オプティマイザの最初の実行は、カーネルの初期パラメータから実行され、残りのパラメータは（もしあれば）許可されたtheta値のスペースからランダムに抽出されたthetasから対数均一にサンプリングされます。0より大きい場合、すべての境界は有限でなければなりません。n_restarts_optimizer == 0は、1回の実行が実行されることを意味することに注意してください。</target>
        </trans-unit>
        <trans-unit id="a4b3a5a4d2704e96fc904a7b55b3f1c5b2c8d507" translate="yes" xml:space="preserve">
          <source>The number of restarts of the optimizer for finding the kernel&amp;rsquo;s parameters which maximize the log-marginal likelihood. The first run of the optimizer is performed from the kernel&amp;rsquo;s initial parameters, the remaining ones (if any) from thetas sampled log-uniform randomly from the space of allowed theta-values. If greater than 0, all bounds must be finite. Note that n_restarts_optimizer=0 implies that one run is performed.</source>
          <target state="translated">対数限界尤度を最大化するカーネルのパラメーターを見つけるためのオプティマイザーの再起動回数。オプティマイザの最初の実行は、カーネルの初期パラメータから実行され、残りのパラメータ（ある場合）は、許可されたtheta値のスペースからランダムに抽出されたthetasから対数均一にサンプリングされます。0より大きい場合、すべての境界は有限でなければなりません。n_restarts_optimizer = 0は、1回の実行が実行されることを意味することに注意してください。</target>
        </trans-unit>
        <trans-unit id="8f64e7c256c29c0afec3f69dafbe77018e516c64" translate="yes" xml:space="preserve">
          <source>The number of row and column clusters in the checkerboard structure.</source>
          <target state="translated">チェッカーボード構造の行と列のクラスタ数。</target>
        </trans-unit>
        <trans-unit id="1d47b9ac186fe69411b80e5cb5c0bc35de2b1552" translate="yes" xml:space="preserve">
          <source>The number of row and column clusters.</source>
          <target state="translated">行と列のクラスタ数。</target>
        </trans-unit>
        <trans-unit id="a93f4e954bb39f85a0cd6723eb5913eb20116e8e" translate="yes" xml:space="preserve">
          <source>The number of sample points on the S curve.</source>
          <target state="translated">S曲線上のサンプル点の数。</target>
        </trans-unit>
        <trans-unit id="e771006aa290a36177ef8a2fd15f85ed45a9f7ce" translate="yes" xml:space="preserve">
          <source>The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself.</source>
          <target state="translated">中心点とみなされる点の近傍のサンプル数(または総重量)。これには点自体も含まれる。</target>
        </trans-unit>
        <trans-unit id="d8023265c3023688c589a292c9dfac5ffedb5a44" translate="yes" xml:space="preserve">
          <source>The number of samples drawn from the Gaussian process</source>
          <target state="translated">ガウス過程から引き出されるサンプル数</target>
        </trans-unit>
        <trans-unit id="1589955effed900fd5766b276491de7c2d2b9bf4" translate="yes" xml:space="preserve">
          <source>The number of samples processed by the estimator for each feature. If there are not missing samples, the &lt;code&gt;n_samples_seen&lt;/code&gt; will be an integer, otherwise it will be an array. Will be reset on new calls to fit, but increments across &lt;code&gt;partial_fit&lt;/code&gt; calls.</source>
          <target state="translated">各機能の推定器によって処理されたサンプルの数。欠落しているサンプルがない場合、 &lt;code&gt;n_samples_seen&lt;/code&gt; は整数になり、それ以外の場合は配列になります。新しい呼び出しでリセットされてフィットしますが、 &lt;code&gt;partial_fit&lt;/code&gt; 呼び出しで増分されます。</target>
        </trans-unit>
        <trans-unit id="5282c1e1c469ae21abb6fc766981198bf00b68c4" translate="yes" xml:space="preserve">
          <source>The number of samples processed by the estimator. Will be reset on new calls to fit, but increments across &lt;code&gt;partial_fit&lt;/code&gt; calls.</source>
          <target state="translated">推定器によって処理されたサンプルの数。新しい呼び出しでリセットされてフィットしますが、 &lt;code&gt;partial_fit&lt;/code&gt; 呼び出しで増分されます。</target>
        </trans-unit>
        <trans-unit id="1793fd9997ff1f0552981d710a775a55fe6d48a0" translate="yes" xml:space="preserve">
          <source>The number of samples to draw from X to train each base estimator.</source>
          <target state="translated">各基底推定器を訓練するために X から描画するサンプル数。</target>
        </trans-unit>
        <trans-unit id="bb3a932b7568c921848c0d1cfe6540980845aa8f" translate="yes" xml:space="preserve">
          <source>The number of samples to take in each batch.</source>
          <target state="translated">各バッチで採取するサンプル数。</target>
        </trans-unit>
        <trans-unit id="45c4c8ec08a2e1d782bf77a47114ec6ebd4dca5e" translate="yes" xml:space="preserve">
          <source>The number of samples to use for each batch. Only used when calling &lt;code&gt;fit&lt;/code&gt;. If &lt;code&gt;batch_size&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt;, then &lt;code&gt;batch_size&lt;/code&gt; is inferred from the data and set to &lt;code&gt;5 * n_features&lt;/code&gt;, to provide a balance between approximation accuracy and memory consumption.</source>
          <target state="translated">各バッチに使用するサンプルの数。 &lt;code&gt;fit&lt;/code&gt; を呼び出すときにのみ使用されます。場合 &lt;code&gt;batch_size&lt;/code&gt; しない &lt;code&gt;None&lt;/code&gt; 、次いで &lt;code&gt;batch_size&lt;/code&gt; にデータセットから推定される &lt;code&gt;5 * n_features&lt;/code&gt; 近似精度とメモリ消費のバランスを提供します。</target>
        </trans-unit>
        <trans-unit id="cc9506bec678834c1e3b7d03354e1ff3a3f08ddc" translate="yes" xml:space="preserve">
          <source>The number of samples to use. If not given, all samples are used.</source>
          <target state="translated">使用するサンプル数。指定しない場合は、すべてのサンプルが使用されます。</target>
        </trans-unit>
        <trans-unit id="06d71b7513bf6cfbd6babc125146f20e54cecd08" translate="yes" xml:space="preserve">
          <source>The number of samples.</source>
          <target state="translated">サンプル数です。</target>
        </trans-unit>
        <trans-unit id="8ed3bb3b4a6145be5f5af9755587163fa8bebbaa" translate="yes" xml:space="preserve">
          <source>The number of seconds contained in delta</source>
          <target state="translated">デルタに含まれる秒数</target>
        </trans-unit>
        <trans-unit id="fd2b02981da97aea3b937c6f113a2aa5413af4a0" translate="yes" xml:space="preserve">
          <source>The number of selected features with cross-validation.</source>
          <target state="translated">クロスバリデーションで選択された特徴量の数。</target>
        </trans-unit>
        <trans-unit id="f83194da71427b41aa36e9d801ef17814b93c795" translate="yes" xml:space="preserve">
          <source>The number of selected features.</source>
          <target state="translated">選択された機能の数。</target>
        </trans-unit>
        <trans-unit id="ecf3b2d99c2ce29bc7f1b5f51d42517d75e68e85" translate="yes" xml:space="preserve">
          <source>The number of stages of the final model is available at the attribute &lt;code&gt;n_estimators_&lt;/code&gt;.</source>
          <target state="translated">最終モデルのステージ数は、属性 &lt;code&gt;n_estimators_&lt;/code&gt; で取得できます。</target>
        </trans-unit>
        <trans-unit id="40db2965d8f58e28ab3da3567d512db3201d5fa4" translate="yes" xml:space="preserve">
          <source>The number of times the grid is refined. Not used if explicit values of alphas are passed.</source>
          <target state="translated">グリッドがリファインされる回数を指定します。アルファの明示的な値が渡された場合は使用されません。</target>
        </trans-unit>
        <trans-unit id="9b5e4a652e0296cb387fe06bf82a965799e670b1" translate="yes" xml:space="preserve">
          <source>The number of trees in the forest.</source>
          <target state="translated">森の中にある木の数。</target>
        </trans-unit>
        <trans-unit id="b609a37dc2d7220a5b1e0ac4ad20a19f7617a90e" translate="yes" xml:space="preserve">
          <source>The number of weak learners (i.e. regression trees) is controlled by the parameter &lt;code&gt;n_estimators&lt;/code&gt;; &lt;a href=&quot;#gradient-boosting-tree-size&quot;&gt;The size of each tree&lt;/a&gt; can be controlled either by setting the tree depth via &lt;code&gt;max_depth&lt;/code&gt; or by setting the number of leaf nodes via &lt;code&gt;max_leaf_nodes&lt;/code&gt;. The &lt;code&gt;learning_rate&lt;/code&gt; is a hyper-parameter in the range (0.0, 1.0] that controls overfitting via &lt;a href=&quot;#gradient-boosting-shrinkage&quot;&gt;shrinkage&lt;/a&gt; .</source>
          <target state="translated">弱学習器（つまり、回帰ツリー）の数は、パラメーター &lt;code&gt;n_estimators&lt;/code&gt; によって制御されます。&lt;a href=&quot;#gradient-boosting-tree-size&quot;&gt;各ツリーのサイズは、&lt;/a&gt;を介してツリーの深さを設定することによってのいずれかで制御することができる &lt;code&gt;max_depth&lt;/code&gt; 又は介しリーフノードの数を設定することにより &lt;code&gt;max_leaf_nodes&lt;/code&gt; 。 &lt;code&gt;learning_rate&lt;/code&gt; は、コントロールを介してオーバーフィットする範囲（0.0、1.0]でハイパーパラメータで&lt;a href=&quot;#gradient-boosting-shrinkage&quot;&gt;収縮&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="68e68bbcb31faf3cd13be5ec90f20eecf860f6d7" translate="yes" xml:space="preserve">
          <source>The number of weak learners is controlled by the parameter &lt;code&gt;n_estimators&lt;/code&gt;. The &lt;code&gt;learning_rate&lt;/code&gt; parameter controls the contribution of the weak learners in the final combination. By default, weak learners are decision stumps. Different weak learners can be specified through the &lt;code&gt;base_estimator&lt;/code&gt; parameter. The main parameters to tune to obtain good results are &lt;code&gt;n_estimators&lt;/code&gt; and the complexity of the base estimators (e.g., its depth &lt;code&gt;max_depth&lt;/code&gt; or minimum required number of samples to consider a split &lt;code&gt;min_samples_split&lt;/code&gt;).</source>
          <target state="translated">弱学習器の数は、パラメーター &lt;code&gt;n_estimators&lt;/code&gt; によって制御されます。 &lt;code&gt;learning_rate&lt;/code&gt; パラメータは、最終的な組み合わせにおける弱識別器の寄与を制御します。デフォルトでは、弱学習器は意思決定の切り株です。 &lt;code&gt;base_estimator&lt;/code&gt; パラメータを使用して、さまざまな弱学習器を指定できます。良い結果を得るために調整する主なパラメーターは、 &lt;code&gt;n_estimators&lt;/code&gt; と基本推定量の複雑さです（たとえば、その深さ &lt;code&gt;max_depth&lt;/code&gt; または分割 &lt;code&gt;min_samples_split&lt;/code&gt; を考慮するために必要なサンプルの最小数）。</target>
        </trans-unit>
        <trans-unit id="a6d1422bf72f5a61faa255a9a1207169e4a394c6" translate="yes" xml:space="preserve">
          <source>The object solves the same problem as the LassoCV object. However, unlike the LassoCV, it find the relevant alphas values by itself. In general, because of this property, it will be more stable. However, it is more fragile to heavily multicollinear datasets.</source>
          <target state="translated">このオブジェクトは,LassoCV オブジェクトと同じ問題を解決します.ただし,LassoCV とは異なり,自分で関連するアルファ値を見つけます.一般的に,この性質のため,より安定しています.しかし,重度のマルチコリニアなデータセットに対しては,より脆弱です.</target>
        </trans-unit>
        <trans-unit id="909e016dde1f323cbfe3daf2401dd2bee2829e0c" translate="yes" xml:space="preserve">
          <source>The object to use to fit the data.</source>
          <target state="translated">データをはめ込むのに使うオブジェクト。</target>
        </trans-unit>
        <trans-unit id="d5c8a5f63b0e142ed66ba0f0aa5318c460719fdf" translate="yes" xml:space="preserve">
          <source>The object&amp;rsquo;s &lt;code&gt;best_score_&lt;/code&gt; and &lt;code&gt;best_params_&lt;/code&gt; attributes store the best mean score and the parameters setting corresponding to that score:</source>
          <target state="translated">オブジェクトの &lt;code&gt;best_score_&lt;/code&gt; および &lt;code&gt;best_params_&lt;/code&gt; 属性には、最良の平均スコアと、そのスコアに対応するパラメーター設定が格納されます。</target>
        </trans-unit>
        <trans-unit id="512dd720938772db73af76fb4221b6596459608c" translate="yes" xml:space="preserve">
          <source>The objective function is minimized with an alternating minimization of W and H.</source>
          <target state="translated">目的関数は、WとHを交互に最小化することで最小化されます。</target>
        </trans-unit>
        <trans-unit id="a139874cf1d9e2c13462d6567d8563d658f0907b" translate="yes" xml:space="preserve">
          <source>The objective function is:</source>
          <target state="translated">目的関数は</target>
        </trans-unit>
        <trans-unit id="bf56422fecab64934517c1fdfbcaed66ab27df08" translate="yes" xml:space="preserve">
          <source>The objective function to minimize is in this case</source>
          <target state="translated">最小化する目的関数は、この場合</target>
        </trans-unit>
        <trans-unit id="e546a5e9110cb4077ff5ab03d80b93cb6429070c" translate="yes" xml:space="preserve">
          <source>The observations are assumed to be caused by a linear transformation of lower dimensional latent factors and added Gaussian noise. Without loss of generality the factors are distributed according to a Gaussian with zero mean and unit covariance. The noise is also zero mean and has an arbitrary diagonal covariance matrix.</source>
          <target state="translated">オブザベーションが,低次元の潜在因子の線形変換と追加されたガウス・ノイズによって引き起こされると仮定される.一般性を損なうことなく、因子は、ゼロ平均と単位共分散を持つガウスに従って分布する。ノイズもまたゼロ平均で,任意の対角共分散行列を持つ.</target>
        </trans-unit>
        <trans-unit id="f8b5be5464139b25f15f97e4d9d483501d55af35" translate="yes" xml:space="preserve">
          <source>The observations to cluster. It must be noted that the data will be converted to C ordering, which will cause a memory copy if the given data is not C-contiguous.</source>
          <target state="translated">クラスタリングするオブザベーション。データはC順に変換され、与えられたデータがC連続でない場合はメモリコピーが発生することに注意しなければなりません。</target>
        </trans-unit>
        <trans-unit id="5af240b8e218ff237309779b56f83d5e52d1af7b" translate="yes" xml:space="preserve">
          <source>The observations, the Mahalanobis distances of the which we compute. Observations are assumed to be drawn from the same distribution than the data used in fit.</source>
          <target state="translated">オブザベーション、我々が計算するマハラノビス距離。観測は、フィットで使用されるデータと同じ分布から描かれていると仮定しています。</target>
        </trans-unit>
        <trans-unit id="eb71736c8c9acd5b1d75a4e7144f466f3b859a53" translate="yes" xml:space="preserve">
          <source>The obtained score is always strictly greater than 0 and the best value is 1.</source>
          <target state="translated">得られたスコアは常に厳密には0よりも大きく、最良値は1である。</target>
        </trans-unit>
        <trans-unit id="b7dea734e0307eec19c3b5341e6f81839af6082a" translate="yes" xml:space="preserve">
          <source>The one-vs-the-rest meta-classifier also implements a &lt;code&gt;predict_proba&lt;/code&gt; method, so long as such a method is implemented by the base classifier. This method returns probabilities of class membership in both the single label and multilabel case. Note that in the multilabel case, probabilities are the marginal probability that a given sample falls in the given class. As such, in the multilabel case the sum of these probabilities over all possible labels for a given sample &lt;em&gt;will not&lt;/em&gt; sum to unity, as they do in the single label case.</source>
          <target state="translated">one-vs-the-restメタ分類子は、ベース分類子によって実装される限り、 &lt;code&gt;predict_proba&lt;/code&gt; メソッドも実装します。このメソッドは、単一ラベルとマルチラベルの両方のケースでクラスメンバーシップの確率を返します。マルチラベルの場合、確率は、特定のサンプルが特定のクラスに分類される限界確率であることに注意してください。そのため、マルチラベルの場合には、与えられたサンプルについて、すべての可能なラベルの上にこれらの確率の合計は&lt;em&gt;ないだろう&lt;/em&gt;、彼らは単一ラベルの場合にはそうであるように、団結を合計します。</target>
        </trans-unit>
        <trans-unit id="f36ee9570dbac199195d25256879fa51a751bbaa" translate="yes" xml:space="preserve">
          <source>The opposite LOF of the training samples. The higher, the more normal. Inliers tend to have a LOF score close to 1 (&lt;code&gt;negative_outlier_factor_&lt;/code&gt; close to -1), while outliers tend to have a larger LOF score.</source>
          <target state="translated">トレーニングサンプルの反対のLOF。高いほど、正常です。外れ値のLOFスコアは大きくなる傾向がありますが、インライアのLOFスコアは1に近い傾向があります（ &lt;code&gt;negative_outlier_factor_&lt;/code&gt; は-1に近い）。</target>
        </trans-unit>
        <trans-unit id="df3e454f1090a47509e70dbea510891595829b28" translate="yes" xml:space="preserve">
          <source>The opposite of the Local Outlier Factor of each input samples. The lower, the more abnormal.</source>
          <target state="translated">各入力サンプルの局所外れ要因の逆。低いほど異常。</target>
        </trans-unit>
        <trans-unit id="4312ae849a138f7db034f6fd7f5a1ea0b817b171" translate="yes" xml:space="preserve">
          <source>The optimal algorithm for a given dataset is a complicated choice, and depends on a number of factors:</source>
          <target state="translated">与えられたデータセットに対する最適なアルゴリズムは複雑な選択であり、多くの要因に依存します。</target>
        </trans-unit>
        <trans-unit id="e994897b226c2495f8cea3f1e46f2c7029c61954" translate="yes" xml:space="preserve">
          <source>The optimal lambda parameter for minimizing skewness is estimated on each feature independently using maximum likelihood.</source>
          <target state="translated">歪度を最小化するための最適なラムダパラメータは、最尤法を用いて各特徴に独立して推定されます。</target>
        </trans-unit>
        <trans-unit id="033713ef73311880bab79fd88513749d67a56824" translate="yes" xml:space="preserve">
          <source>The optimization objective for Lasso is:</source>
          <target state="translated">Lassoの最適化目的は</target>
        </trans-unit>
        <trans-unit id="64214421bf615c105d2891f743437d06253a6ade" translate="yes" xml:space="preserve">
          <source>The optimization objective for MultiTaskElasticNet is:</source>
          <target state="translated">MultiTaskElasticNetの最適化目標は以下の通りです。</target>
        </trans-unit>
        <trans-unit id="f43132c3f7097ed8020d37840c051e421e41e300" translate="yes" xml:space="preserve">
          <source>The optimization objective for MultiTaskLasso is:</source>
          <target state="translated">MultiTaskLassoの最適化の目的は</target>
        </trans-unit>
        <trans-unit id="3bee4ea6ed3cad366ecf4d67dfc05469de43ad46" translate="yes" xml:space="preserve">
          <source>The optimization objective for the case method=&amp;rsquo;lasso&amp;rsquo; is:</source>
          <target state="translated">case method = 'lasso'の最適化の目的は次のとおりです。</target>
        </trans-unit>
        <trans-unit id="8d89fe15a9f2092d4f8a7ef569d775bf0279d26d" translate="yes" xml:space="preserve">
          <source>The optional extra argument will be appended to the deprecation message and the docstring. Note: to use this with the default value for extra, put in an empty of parentheses:</source>
          <target state="translated">オプションの extra 引数は、非推奨のメッセージと docstring に追加されます。注意:extra のデフォルト値で使用するには、括弧を空にしてください。</target>
        </trans-unit>
        <trans-unit id="0c86ba504c90ec1412c0ccc3d7f0b2f074294dc1" translate="yes" xml:space="preserve">
          <source>The optional parameter &lt;code&gt;whiten=True&lt;/code&gt; makes it possible to project the data onto the singular space while scaling each component to unit variance. This is often useful if the models down-stream make strong assumptions on the isotropy of the signal: this is for example the case for Support Vector Machines with the RBF kernel and the K-Means clustering algorithm.</source>
          <target state="translated">オプションのパラメーター &lt;code&gt;whiten=True&lt;/code&gt; を使用すると、各コンポーネントを単位分散にスケーリングしながら、データを特異空間に投影できます。これは、下流のモデルが信号の等方性を強く想定している場合に役立ちます。これは、たとえば、RBFカーネルとK-Meansクラスタリングアルゴリズムを備えたサポートベクターマシンの場合です。</target>
        </trans-unit>
        <trans-unit id="846ba284e005e0c78a1fd443a812172705bbc8f3" translate="yes" xml:space="preserve">
          <source>The order of labels in the classifier chain.</source>
          <target state="translated">分類器チェーンのラベルの順序。</target>
        </trans-unit>
        <trans-unit id="7b082acdda498538bd71b7e32b0b230c2144c284" translate="yes" xml:space="preserve">
          <source>The order of the chain can be explicitly set by providing a list of integers. For example, for a chain of length 5.:</source>
          <target state="translated">鎖の順序は、整数のリストを指定することで明示的に設定することができます。例えば、長さ5の鎖の場合...</target>
        </trans-unit>
        <trans-unit id="f9d11a930ecaf4c38c05b12592342da86a8c77ff" translate="yes" xml:space="preserve">
          <source>The order of the columns in the transformed feature matrix follows the order of how the columns are specified in the &lt;code&gt;transformers&lt;/code&gt; list. Columns of the original feature matrix that are not specified are dropped from the resulting transformed feature matrix, unless specified in the &lt;code&gt;passthrough&lt;/code&gt; keyword. Those columns specified with &lt;code&gt;passthrough&lt;/code&gt; are added at the right to the output of the transformers.</source>
          <target state="translated">変換された特徴マトリックスの列の順序は、 &lt;code&gt;transformers&lt;/code&gt; リストで列が指定された順序に従います。指定されていない元の特徴マトリックスの列は、 &lt;code&gt;passthrough&lt;/code&gt; キーワードで指定されていない限り、変換後の変換された特徴マトリックスから削除されます。 &lt;code&gt;passthrough&lt;/code&gt; 指定された列は、トランスフォーマーの出力の右側に追加されます。</target>
        </trans-unit>
        <trans-unit id="0b5c0a29228cf2f99af9ecdff2f5b2a0dc08ed2d" translate="yes" xml:space="preserve">
          <source>The original data</source>
          <target state="translated">オリジナルデータ</target>
        </trans-unit>
        <trans-unit id="77422b337aacebf704cab947c2765e7ef78426db" translate="yes" xml:space="preserve">
          <source>The original dataset consisted of 92 x 112, while the version available here consists of 64x64 images.</source>
          <target state="translated">元のデータセットは92×112枚で構成されていましたが、ここで利用できるバージョンは64×64枚の画像で構成されています。</target>
        </trans-unit>
        <trans-unit id="11407c8a67a7b9eefbd2b65794ca1ecbfa48a97d" translate="yes" xml:space="preserve">
          <source>The original formulation of the hashing trick by Weinberger et al. used two separate hash functions \(h\) and \(\xi\) to determine the column index and sign of a feature, respectively. The present implementation works under the assumption that the sign bit of MurmurHash3 is independent of its other bits.</source>
          <target state="translated">Weinbergerらによるハッシュ・トリックのオリジナルの定式化では、特徴の列のインデックスと符号を決定するために、2つの別々のハッシュ関数を使用していました。現在の実装では、MurmurHash3の符号ビットが他のビットから独立しているという仮定の下で動作している。</target>
        </trans-unit>
        <trans-unit id="8eec938f1b6ddec4315f3fac08b8dc8e8e67d5d5" translate="yes" xml:space="preserve">
          <source>The original images are 250 x 250 pixels, but the default slice and resize arguments reduce them to 62 x 47.</source>
          <target state="translated">元の画像は250×250ピクセルですが、デフォルトのスライスとリサイズの引数で62×47に縮小されます。</target>
        </trans-unit>
        <trans-unit id="a555f40ab758aba55bcba7bb9ac36af67b065d6a" translate="yes" xml:space="preserve">
          <source>The other kernels</source>
          <target state="translated">他のカーネル</target>
        </trans-unit>
        <trans-unit id="5978e5bbc3d0aa56b4c3321d1d9ccbd8b149a1a9" translate="yes" xml:space="preserve">
          <source>The outer product of the row and column label vectors shows a representation of the checkerboard structure.</source>
          <target state="translated">行と列のラベルベクトルの外積は、市松模様の構造の表現を示しています。</target>
        </trans-unit>
        <trans-unit id="4e7d9f946f101134a65fd7d38a82dce953516bd7" translate="yes" xml:space="preserve">
          <source>The output &lt;code&gt;y&lt;/code&gt; is created according to the formula:</source>
          <target state="translated">出力 &lt;code&gt;y&lt;/code&gt; は次の式に従って作成されます。</target>
        </trans-unit>
        <trans-unit id="7348f35a4e7d1a3450461b231ee28ef95517ba57" translate="yes" xml:space="preserve">
          <source>The output is generated by applying a (potentially biased) random linear regression model with &lt;code&gt;n_informative&lt;/code&gt; nonzero regressors to the previously generated input and some gaussian centered noise with some adjustable scale.</source>
          <target state="translated">出力は、 &lt;code&gt;n_informative&lt;/code&gt; 非ゼロリグレッサを持つ（潜在的にバイアスされた）ランダム線形回帰モデルを以前に生成された入力といくつかの調整可能なスケールを持ついくつかのガウス中心ノイズに適用することによって生成されます。</target>
        </trans-unit>
        <trans-unit id="0c62eccd54e33fb989ef31175162303c11637d7c" translate="yes" xml:space="preserve">
          <source>The output of a singular value decomposition is only unique up to a permutation of the signs of the singular vectors. If &lt;code&gt;flip_sign&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, the sign ambiguity is resolved by making the largest loadings for each component in the left singular vectors positive.</source>
          <target state="translated">特異値分解の出力は、特異ベクトルの符号の順列までのみ一意です。 &lt;code&gt;flip_sign&lt;/code&gt; が &lt;code&gt;True&lt;/code&gt; に設定されている場合、左の特異ベクトルの各コンポーネントの最大の負荷を正にすることにより、符号のあいまいさが解決されます。</target>
        </trans-unit>
        <trans-unit id="ab5b2ab18fdef999b51de35c5e9c33e7d3ac7cc7" translate="yes" xml:space="preserve">
          <source>The output of the 3 models are combined in a 2D graph where nodes represents the stocks and edges the:</source>
          <target state="translated">3つのモデルの出力は、ノードが株式とエッジを表す2Dグラフに結合されます。</target>
        </trans-unit>
        <trans-unit id="c9b52f3c910ded5afcb915db265ed0583f446660" translate="yes" xml:space="preserve">
          <source>The output of transform is sometimes referred to as the 1-of-K coding scheme.</source>
          <target state="translated">変換の出力は、1-of-K符号化方式と呼ばれることがあります。</target>
        </trans-unit>
        <trans-unit id="f172299244e465e38a0859a0349f26df08f75701" translate="yes" xml:space="preserve">
          <source>The output of transform is sometimes referred to by some authors as the 1-of-K coding scheme.</source>
          <target state="translated">変換の出力は、いくつかの著者によって1-of-K符号化方式と呼ばれることがあります。</target>
        </trans-unit>
        <trans-unit id="58983c4b722f7fa5c156e992e47b9ca634d33156" translate="yes" xml:space="preserve">
          <source>The output values.</source>
          <target state="translated">出力値です。</target>
        </trans-unit>
        <trans-unit id="f348d2a86dcf3bc20a82250a4d5068aa50c67aad" translate="yes" xml:space="preserve">
          <source>The overall complexity of Isomap is \(O[D \log(k) N \log(N)] + O[N^2(k + \log(N))] + O[d N^2]\).</source>
          <target state="translated">Isomapの全体的な複雑さは、\(O[D \log(k)N \log(N)]+O[N^2(k+\log(N))]+O[d N^2]です。)</target>
        </trans-unit>
        <trans-unit id="9cec4ae56de9cf1cd9cf2f8a0ff2e2e24864f054" translate="yes" xml:space="preserve">
          <source>The overall complexity of MLLE is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[N (k-D) k^2] + O[d N^2]\).</source>
          <target state="translated">MLLE の全体的な複雑さは、\(O[D \log(k)N \log(N)]+O[D N k^3]+O[N (k-D)k^2]+O[d N^2]となります。)</target>
        </trans-unit>
        <trans-unit id="7fa546000c6a79308906d0c040bf81047f6b149e" translate="yes" xml:space="preserve">
          <source>The overall complexity of spectral embedding is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[d N^2]\).</source>
          <target state="translated">スペクトル埋め込みの全体的な複雑さは、\(O[D \log(k)N \log(N)]+O[D N k^3]+O[d N^2]です。)</target>
        </trans-unit>
        <trans-unit id="808f28c633edaf7537ca8395b6bc52f0086ed496" translate="yes" xml:space="preserve">
          <source>The overall complexity of standard HLLE is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[N d^6] + O[d N^2]\).</source>
          <target state="translated">標準的なHLLEの全体的な複雑さは、\(O[D \log(k)N \log(N)]+O[D N k^3]+O[N d^6]+O[d N^2]です。)</target>
        </trans-unit>
        <trans-unit id="63617858af3d41882021a1ab37e78e76cce39983" translate="yes" xml:space="preserve">
          <source>The overall complexity of standard LLE is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[d N^2]\).</source>
          <target state="translated">標準的なLLEの全体的な複雑さは、\(O[D \log(k)N \log(N)]+O[D N k^3]+O[d N^2]です。)</target>
        </trans-unit>
        <trans-unit id="b88c53c3806a592f7e00e19aef010a599cfaf4dc" translate="yes" xml:space="preserve">
          <source>The overall complexity of standard LTSA is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[k^2 d] + O[d N^2]\).</source>
          <target state="translated">標準的なLTSAの全体的な複雑さは、\(O[D \log(k)N \log(N)]+O[D N k^3]+O[k^2 d]+O[d N^2]です。)</target>
        </trans-unit>
        <trans-unit id="fc12a97c8a559f9672542a072947ef2eae0adcac" translate="yes" xml:space="preserve">
          <source>The p-value, which approximates the probability that the score would be obtained by chance. This is calculated as:</source>
          <target state="translated">スコアが偶然に得られる確率を近似したp値。として計算されます。</target>
        </trans-unit>
        <trans-unit id="7f9aaf22278cea2b541fbd8b88b09de54aad3e99" translate="yes" xml:space="preserve">
          <source>The parallel version of K-Means is broken on OS X when &lt;code&gt;numpy&lt;/code&gt; uses the &lt;code&gt;Accelerate&lt;/code&gt; Framework. This is expected behavior: &lt;code&gt;Accelerate&lt;/code&gt; can be called after a fork but you need to execv the subprocess with the Python binary (which multiprocessing does not do under posix).</source>
          <target state="translated">&lt;code&gt;numpy&lt;/code&gt; が &lt;code&gt;Accelerate&lt;/code&gt; Frameworkを使用すると、K-MeansのパラレルバージョンがOS Xで壊れます。これは予想される動作です。フォークの後に &lt;code&gt;Accelerate&lt;/code&gt; を呼び出すことができますが、サブプロセスをPythonバイナリで実行する必要があります（posixではマルチプロセッシングは行いません）。</target>
        </trans-unit>
        <trans-unit id="26cab4ba2ce5f7f1aa5cff1bedcda88264af63ea" translate="yes" xml:space="preserve">
          <source>The parameter &lt;code&gt;learning_rate&lt;/code&gt; strongly interacts with the parameter &lt;code&gt;n_estimators&lt;/code&gt;, the number of weak learners to fit. Smaller values of &lt;code&gt;learning_rate&lt;/code&gt; require larger numbers of weak learners to maintain a constant training error. Empirical evidence suggests that small values of &lt;code&gt;learning_rate&lt;/code&gt; favor better test error. &lt;a href=&quot;#htf2009&quot; id=&quot;id17&quot;&gt;[HTF2009]&lt;/a&gt; recommend to set the learning rate to a small constant (e.g. &lt;code&gt;learning_rate &amp;lt;= 0.1&lt;/code&gt;) and choose &lt;code&gt;n_estimators&lt;/code&gt; by early stopping. For a more detailed discussion of the interaction between &lt;code&gt;learning_rate&lt;/code&gt; and &lt;code&gt;n_estimators&lt;/code&gt; see &lt;a href=&quot;#r2007&quot; id=&quot;id18&quot;&gt;[R2007]&lt;/a&gt;.</source>
          <target state="translated">パラメータ &lt;code&gt;learning_rate&lt;/code&gt; は、適合させる弱学習器の数であるパラメータ &lt;code&gt;n_estimators&lt;/code&gt; と強く相互作用します。 &lt;code&gt;learning_rate&lt;/code&gt; の値が小さいほど、一定のトレーニングエラーを維持するために多数の弱学習器が必要になります。経験的証拠は、 &lt;code&gt;learning_rate&lt;/code&gt; の値が小さいほど、テストエラーが改善されることを示しています。&lt;a href=&quot;#htf2009&quot; id=&quot;id17&quot;&gt;[HTF2009]&lt;/a&gt;学習率を小さな定数（たとえば &lt;code&gt;learning_rate &amp;lt;= 0.1&lt;/code&gt; ）に &lt;code&gt;n_estimators&lt;/code&gt; 、早期に停止してn_estimatorsを選択することをお勧めします。 &lt;code&gt;learning_rate&lt;/code&gt; と &lt;code&gt;n_estimators&lt;/code&gt; の相互作用の詳細については、&lt;a href=&quot;#r2007&quot; id=&quot;id18&quot;&gt;[ &lt;/a&gt;R2007 ]を参照してください。</target>
        </trans-unit>
        <trans-unit id="0e6ab9c66156a9d2feae55ed2060a2c6f7d6986c" translate="yes" xml:space="preserve">
          <source>The parameter &lt;code&gt;memory&lt;/code&gt; is needed in order to cache the transformers. &lt;code&gt;memory&lt;/code&gt; can be either a string containing the directory where to cache the transformers or a &lt;a href=&quot;https://pythonhosted.org/joblib/memory.html&quot;&gt;joblib.Memory&lt;/a&gt; object:</source>
          <target state="translated">トランスフォーマーをキャッシュするためにパラメーター・ &lt;code&gt;memory&lt;/code&gt; が必要です。 &lt;code&gt;memory&lt;/code&gt; は、トランスフォーマーをキャッシュするディレクトリを含む文字列、または&lt;a href=&quot;https://pythonhosted.org/joblib/memory.html&quot;&gt;joblib.Memory&lt;/a&gt;オブジェクトのいずれかです。</target>
        </trans-unit>
        <trans-unit id="a19846a3c7376460662acabedd23579aba492f87" translate="yes" xml:space="preserve">
          <source>The parameter \(\nu\) is also called the &lt;strong&gt;learning rate&lt;/strong&gt; because it scales the step length the gradient descent procedure; it can be set via the &lt;code&gt;learning_rate&lt;/code&gt; parameter.</source>
          <target state="translated">パラメータ\（\ nu \）は、勾配降下法のステップ長をスケーリングするため、&lt;strong&gt;学習率&lt;/strong&gt;とも呼ばれます。 &lt;code&gt;learning_rate&lt;/code&gt; パラメータで設定できます。</target>
        </trans-unit>
        <trans-unit id="76b14eb14f0e0be16176b9f0182e58f523e33b2d" translate="yes" xml:space="preserve">
          <source>The parameter epsilon controls the number of samples that should be classified as outliers. The smaller the epsilon, the more robust it is to outliers.</source>
          <target state="translated">パラメータεは,外れ値として分類されるべきサンプルの数を制御します.イプシロンが小さければ小さいほど、外れ値に対してよりロバストです。</target>
        </trans-unit>
        <trans-unit id="33a23a95b6f6e50b11cce3982dee22dc3afbeaef" translate="yes" xml:space="preserve">
          <source>The parameter grid to explore, as a dictionary mapping estimator parameters to sequences of allowed values.</source>
          <target state="translated">探索するパラメータグリッドで、エスティメー タのパラメータを許容値のシーケンスにマッピングするディクショナリとして使用します。</target>
        </trans-unit>
        <trans-unit id="95ad4f92539995843fbdd8a8c8263fdd9652cae5" translate="yes" xml:space="preserve">
          <source>The parameter l1_ratio corresponds to alpha in the glmnet R package while alpha corresponds to the lambda parameter in glmnet. More specifically, the optimization objective is:</source>
          <target state="translated">パラメータl1_ratioはglmnet Rパッケージのαに相当し、αはglmnetのラムダパラメータに相当します。より具体的には、最適化の目的は</target>
        </trans-unit>
        <trans-unit id="444f64a3bffea2bee3d3973fdc03c3dbbbbf54d9" translate="yes" xml:space="preserve">
          <source>The parameter l1_ratio corresponds to alpha in the glmnet R package while alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio = 1 is the lasso penalty. Currently, l1_ratio &amp;lt;= 0.01 is not reliable, unless you supply your own sequence of alpha.</source>
          <target state="translated">パラメータl1_ratioはglmnet Rパッケージのアルファに対応し、アルファはglmnetのラムダパラメータに対応します。具体的には、l1_ratio = 1は投げ縄ペナルティです。現在、独自のアルファシーケンスを提供しない限り、l1_ratio &amp;lt;= 0.01は信頼できません。</target>
        </trans-unit>
        <trans-unit id="3d75707f4ee017e7f35982b539c7d0e6825a3d30" translate="yes" xml:space="preserve">
          <source>The parameter nu controlling the smoothness of the learned function. The smaller nu, the less smooth the approximated function is. For nu=inf, the kernel becomes equivalent to the RBF kernel and for nu=0.5 to the absolute exponential kernel. Important intermediate values are nu=1.5 (once differentiable functions) and nu=2.5 (twice differentiable functions). Note that values of nu not in [0.5, 1.5, 2.5, inf] incur a considerably higher computational cost (appr. 10 times higher) since they require to evaluate the modified Bessel function. Furthermore, in contrast to l, nu is kept fixed to its initial value and not optimized.</source>
          <target state="translated">学習した関数の滑らかさを制御するパラメータν.nu が小さいほど,近似関数の平滑度は低くなります.nu=infの場合,カーネルはRBFカーネルと等価になり,nu=0.5の場合は絶対指数カーネルと等価になります.重要な中間値は,nu=1.5 (1回微分可能な関数)とnu=2.5 (2回微分可能な関数)です.0.5,1.5,2.5,inf]以外の値は,修正ベッセル関数を評価する必要があるため,かなり高い計算コストがかかることに注意してください (約10倍)。さらに、lとは対照的に、nuは初期値に固定され、最適化されていません。</target>
        </trans-unit>
        <trans-unit id="fa3c09390eafe53f0b58881f0d7db646d5796fe2" translate="yes" xml:space="preserve">
          <source>The parameters \(\sigma_y\) and \(\mu_y\) are estimated using maximum likelihood.</source>
          <target state="translated">パラメーターは、最尤度を用いて推定されている。</target>
        </trans-unit>
        <trans-unit id="d8334dfb20de589f58500a915aef89a4fab7377d" translate="yes" xml:space="preserve">
          <source>The parameters \(\theta_y\) is estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting:</source>
          <target state="translated">The parameters \(\theta_y)は、最尤の平滑化されたバージョン、つまり相対的な周波数カウントで推定される。</target>
        </trans-unit>
        <trans-unit id="162d2ed9f70c881f87134a87c0c741cb23832c22" translate="yes" xml:space="preserve">
          <source>The parameters implementation of the &lt;a href=&quot;generated/sklearn.mixture.bayesiangaussianmixture#sklearn.mixture.BayesianGaussianMixture&quot;&gt;&lt;code&gt;BayesianGaussianMixture&lt;/code&gt;&lt;/a&gt; class proposes two types of prior for the weights distribution: a finite mixture model with Dirichlet distribution and an infinite mixture model with the Dirichlet Process. In practice Dirichlet Process inference algorithm is approximated and uses a truncated distribution with a fixed maximum number of components (called the Stick-breaking representation). The number of components actually used almost always depends on the data.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.mixture.bayesiangaussianmixture#sklearn.mixture.BayesianGaussianMixture&quot;&gt; &lt;code&gt;BayesianGaussianMixture&lt;/code&gt; &lt;/a&gt;クラスのパラメーター実装は、重み分布に対して2つのタイプの事前分布を提案します。ディリクレ分布を伴う有限混合モデルと、ディリクレプロセスを伴う無限混合モデルです。実際には、ディリクレプロセスの推論アルゴリズムは近似され、固定された最大数のコンポーネント（スティック破壊表現と呼ばれます）で切り捨てられた分布を使用します。実際に使用されるコンポーネントの数は、ほとんどの場合、データに依存します。</target>
        </trans-unit>
        <trans-unit id="957eda9a79d6f1f87ea7b634983b57c94627c96b" translate="yes" xml:space="preserve">
          <source>The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid.</source>
          <target state="translated">これらの方法を適用するために使用される推定器のパラメータは、パラメータグリッド上で交差検証されたグリッドサーチによって最適化されます。</target>
        </trans-unit>
        <trans-unit id="6f20d8d61d83d6376fa9caf869f91f4640e0cc5b" translate="yes" xml:space="preserve">
          <source>The parameters of the estimator used to apply these methods are optimized by cross-validated search over parameter settings.</source>
          <target state="translated">これらの手法を適用するために使用されているエスティメー タのパラメータは、パラメータ設定の上で交差検証された検索によって最適化されています。</target>
        </trans-unit>
        <trans-unit id="1d4a34fa151b21edbbd9fa634476deabc1cb44cf" translate="yes" xml:space="preserve">
          <source>The parameters of the power transformation for the selected features.</source>
          <target state="translated">選択された特徴量の電力変換のパラメータ。</target>
        </trans-unit>
        <trans-unit id="62f294aa209942b08fddf4e74d29c13f78a9e67d" translate="yes" xml:space="preserve">
          <source>The parameters selected are those that maximize the score of the held-out data, according to the scoring parameter.</source>
          <target state="translated">選択されたパラメータは、スコアリングパラメータに応じて、ホールドアウトされたデータのスコアを最大化するパラメータである。</target>
        </trans-unit>
        <trans-unit id="b6d7555a36c15ae2ffe9751024a0eebcf2421d57" translate="yes" xml:space="preserve">
          <source>The parameters selected are those that maximize the score of the left out data, unless an explicit score is passed in which case it is used instead.</source>
          <target state="translated">選択されたパラメータは、明示的なスコアが渡された場合はそれが代わりに使用されますが、明示的なスコアが渡されない限り、残されたデータのスコアを最大にするパラメータが選択されます。</target>
        </trans-unit>
        <trans-unit id="34d594dbc00b197d06cf788b61a5dfe36db335bf" translate="yes" xml:space="preserve">
          <source>The parameters that have been evaluated.</source>
          <target state="translated">評価されたパラメータ。</target>
        </trans-unit>
        <trans-unit id="f139c5090bf57fe74c58422c7266093265927a67" translate="yes" xml:space="preserve">
          <source>The parent of each node. Only returned when a connectivity matrix is specified, elsewhere &amp;lsquo;None&amp;rsquo; is returned.</source>
          <target state="translated">各ノードの親。接続マトリックスが指定されている場合にのみ返されます。それ以外の場合は「なし」が返されます。</target>
        </trans-unit>
        <trans-unit id="89bfd729c83f5c14808628511ada19e6e2b220e5" translate="yes" xml:space="preserve">
          <source>The partial dependence function evaluated on the &lt;code&gt;grid&lt;/code&gt;. For regression and binary classification &lt;code&gt;n_classes==1&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;grid&lt;/code&gt; 評価された部分依存関数。回帰およびバイナリ分類の場合 &lt;code&gt;n_classes==1&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="053dbcb3edc2e230ef9a4eff0313b65bae4689ca" translate="yes" xml:space="preserve">
          <source>The passive-aggressive algorithms are a family of algorithms for large-scale learning. They are similar to the Perceptron in that they do not require a learning rate. However, contrary to the Perceptron, they include a regularization parameter &lt;code&gt;C&lt;/code&gt;.</source>
          <target state="translated">パッシブアグレッシブアルゴリズムは、大規模な学習のためのアルゴリズムのファミリーです。学習率を必要としない点でパーセプトロンに似ています。ただし、パーセプトロンとは異なり、正則化パラメーター &lt;code&gt;C&lt;/code&gt; が含まれています。</target>
        </trans-unit>
        <trans-unit id="cf6ba8c5f2e782ae102fc993ff0f91af87853f26" translate="yes" xml:space="preserve">
          <source>The path of the base directory to use as a data store or None. If None is given, no caching is done and the Memory object is completely transparent. This option replaces cachedir since version 0.12.</source>
          <target state="translated">データストアとして使用するベースディレクトリのパス、または None。None を指定した場合、キャッシュは行われず、Memory オブジェクトは完全に透過的になります。このオプションはバージョン 0.12 以降では cachedir に置き換わります。</target>
        </trans-unit>
        <trans-unit id="e60c1638cc188f171e30720c51f9233ac8e0591d" translate="yes" xml:space="preserve">
          <source>The path to scikit-learn data dir.</source>
          <target state="translated">scikit-learnのデータディレクトリへのパス。</target>
        </trans-unit>
        <trans-unit id="780a6be1d44fa6c4bc32e0e6a573d3b4ad24b942" translate="yes" xml:space="preserve">
          <source>The penalty (aka regularization term) to be used. Defaults to &amp;lsquo;l2&amp;rsquo; which is the standard regularizer for linear SVM models. &amp;lsquo;l1&amp;rsquo; and &amp;lsquo;elasticnet&amp;rsquo; might bring sparsity to the model (feature selection) not achievable with &amp;lsquo;l2&amp;rsquo;.</source>
          <target state="translated">使用するペナルティ（正則化用語）。線形SVMモデルの標準正則化子である 'l2'がデフォルトです。「l1」と「elasticnet」は、「l2」では実現できないスパース性をモデル（機能選択）にもたらす可能性があります。</target>
        </trans-unit>
        <trans-unit id="5ec62ab0e251a48390ce125b16975a3fa4c7ca91" translate="yes" xml:space="preserve">
          <source>The penalty (aka regularization term) to be used. Defaults to None.</source>
          <target state="translated">使用するペナルティ(別名正則化項)。デフォルトは None です。</target>
        </trans-unit>
        <trans-unit id="195880b4735384ca5f4a3196f2d3109be0fc5155" translate="yes" xml:space="preserve">
          <source>The performance is slightly worse for the randomized search, though this is most likely a noise effect and would not carry over to a held-out test set.</source>
          <target state="translated">これはノイズの影響である可能性が高く、テストセットを保持したままの状態には持ち越されませんが、無作為化探索ではパフォーマンスがわずかに低下します。</target>
        </trans-unit>
        <trans-unit id="66758ccad6c0faa66ee36e2739cca75d7cb80119" translate="yes" xml:space="preserve">
          <source>The performance measure reported by &lt;em&gt;k&lt;/em&gt;-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small.</source>
          <target state="translated">&lt;em&gt;k&lt;/em&gt;分割交差検証によって報告されるパフォーマンス測定は、ループで計算された値の平均です。このアプローチは計算コストが高くなる可能性がありますが、（任意の検証セットを修正する場合のように）データを無駄にしないため、サンプル数が非常に少ない逆推論などの問題で大きな利点になります。</target>
        </trans-unit>
        <trans-unit id="3b50299f6e3a476dff9e3b98d46cee16ed70fcdb" translate="yes" xml:space="preserve">
          <source>The performance of the SAMME and SAMME.R &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt; algorithms are compared. SAMME.R uses the probability estimates to update the additive model, while SAMME uses the classifications only. As the example illustrates, the SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations. The error of each algorithm on the test set after each boosting iteration is shown on the left, the classification error on the test set of each tree is shown in the middle, and the boost weight of each tree is shown on the right. All trees have a weight of one in the SAMME.R algorithm and therefore are not shown.</source>
          <target state="translated">SAMMEおよびSAMME.R &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt;アルゴリズムのパフォーマンスが比較されます。SAMME.Rは確率推定を使用して加法モデルを更新しますが、SAMME.Rは分類のみを使用します。例が示すように、SAMME.Rアルゴリズムは通常、SAMMEよりも速く収束し、ブースティングの反復回数を減らしてテストエラーを低減します。各ブースティング反復後のテストセットの各アルゴリズムのエラーは左側に表示され、各ツリーのテストセットの分類エラーは中央に表示され、各ツリーのブーストウェイトは右側に表示されます。SAMME.Rアルゴリズムでは、すべてのツリーの重みが1であるため、表示されていません。</target>
        </trans-unit>
        <trans-unit id="2f1cd52ca3b14d7125644c3724ffbf78c03b2ab3" translate="yes" xml:space="preserve">
          <source>The performance of the selected hyper-parameters and trained model is then measured on a dedicated evaluation set that was not used during the model selection step.</source>
          <target state="translated">次に、選択されたハイパーパラメータと訓練されたモデルの性能を、モデル選択ステップでは使用されなかった専用の評価セットで測定します。</target>
        </trans-unit>
        <trans-unit id="dd106efb0f7012bff421a0ad8f3697f8283515ed" translate="yes" xml:space="preserve">
          <source>The periodicity of the kernel.</source>
          <target state="translated">カーネルの周期性。</target>
        </trans-unit>
        <trans-unit id="5dce1e0dbd7277c2f73689bcac70f69df5d2fc02" translate="yes" xml:space="preserve">
          <source>The perplexity is defined as \(k=2^{(S)}\) where \(S\) is the Shannon entropy of the conditional probability distribution. The perplexity of a \(k\)-sided die is \(k\), so that \(k\) is effectively the number of nearest neighbors t-SNE considers when generating the conditional probabilities. Larger perplexities lead to more nearest neighbors and less sensitive to small structure. Conversely a lower perplexity considers a smaller number of neighbors, and thus ignores more global information in favour of the local neighborhood. As dataset sizes get larger more points will be required to get a reasonable sample of the local neighborhood, and hence larger perplexities may be required. Similarly noisier datasets will require larger perplexity values to encompass enough local neighbors to see beyond the background noise.</source>
          <target state="translated">錯乱度は、条件付き確率分布のシャノンエントロピーと定義されます。\(k\)sided dieのpleplexityは、\(k\)であるから、条件付き確率を生成する際に、t-SNEが考慮する最寄りの隣人の数は、実質的にはT-SNEが考慮した数となる。パープルクシティが大きいほど、より多くの最 近隣人が存在し、小さな構造の影響を受けにくくなります。逆に、より低いパープレキシシティでは、より少ない数の最近傍を考慮するため、局所的な近傍を優先してより多くの大域的な情報を無視することになります。データセットのサイズが大きくなればなるほど,局所近傍の妥当なサンプルを得るためには,より多くのポイントが必要となり,そのため,より大きな過重度が必要となることがある.同様に,ノイズの多いデータセットでは,バックグラウンドノイズを超えて十分なローカル近傍を網羅するために,より大きなパープレキシシティ値が必要となる.</target>
        </trans-unit>
        <trans-unit id="08299376158415917838ffdf1e208cdfe76446d4" translate="yes" xml:space="preserve">
          <source>The perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms. Larger datasets usually require a larger perplexity. Consider selecting a value between 5 and 50. The choice is not extremely critical since t-SNE is quite insensitive to this parameter.</source>
          <target state="translated">錯綜度は、他のマニホールド学習アルゴリズムで使用される最近傍数に関連しています。大きなデータセットでは、通常、より大きなペルプレキシシティが必要になります。5から50の間の値を選択することを検討してください。t-SNEはこのパラメータに対して非常に鈍感なので、この選択は非常に重要ではありません。</target>
        </trans-unit>
        <trans-unit id="3d1257b149fa9d88b1966c8dbdc55bf21ea6a0db" translate="yes" xml:space="preserve">
          <source>The placeholder for the missing values. All occurrences of &lt;code&gt;missing_values&lt;/code&gt; will be imputed.</source>
          <target state="translated">欠損値のプレースホルダー。 &lt;code&gt;missing_values&lt;/code&gt; のすべての出現が帰属されます。</target>
        </trans-unit>
        <trans-unit id="9c95ae315d785709e445ae217c5567fb1ce65f07" translate="yes" xml:space="preserve">
          <source>The placeholder for the missing values. All occurrences of &lt;code&gt;missing_values&lt;/code&gt; will be imputed. For missing values encoded as np.nan, use the string value &amp;ldquo;NaN&amp;rdquo;.</source>
          <target state="translated">欠損値のプレースホルダー。 &lt;code&gt;missing_values&lt;/code&gt; のすべての出現が帰属されます。np.nanとしてエンコードされた欠損値については、文字列値「NaN」を使用します。</target>
        </trans-unit>
        <trans-unit id="50d8f581357d70f0923b3a8bf25734f11fbbdc88" translate="yes" xml:space="preserve">
          <source>The plot represents the learning curve of the classifier: the evolution of classification accuracy over the course of the mini-batches. Accuracy is measured on the first 1000 samples, held out as a validation set.</source>
          <target state="translated">プロットは、分類器の学習曲線を表しています:ミニバッチの間の分類精度の進化です。精度は、最初の1000サンプルで測定され、検証セットとして保存されます。</target>
        </trans-unit>
        <trans-unit id="2108780ce5e305bd6eefeccab3f1f12e712db7ba" translate="yes" xml:space="preserve">
          <source>The plot shows decision boundaries for Linear Discriminant Analysis and Quadratic Discriminant Analysis. The bottom row demonstrates that Linear Discriminant Analysis can only learn linear boundaries, while Quadratic Discriminant Analysis can learn quadratic boundaries and is therefore more flexible.</source>
          <target state="translated">プロットは、線形判別分析と二次判別分析の決定境界を示しています。下の行は、線形判別分析が線形境界しか学習できないのに対し、二次判別分析は二次境界を学習できるため、より柔軟性が高いことを示しています。</target>
        </trans-unit>
        <trans-unit id="17f022b1b904616a8feb650bfceadf60a7908c45" translate="yes" xml:space="preserve">
          <source>The plot shows four one-way and one two-way partial dependence plots. The target variables for the one-way PDP are: median income (&lt;code&gt;MedInc&lt;/code&gt;), avg. occupants per household (&lt;code&gt;AvgOccup&lt;/code&gt;), median house age (&lt;code&gt;HouseAge&lt;/code&gt;), and avg. rooms per household (&lt;code&gt;AveRooms&lt;/code&gt;).</source>
          <target state="translated">このプロットは、4つの一方向および1つの双方向の部分依存プロットを示しています。一方向PDPのターゲット変数は、収入の中央値（ &lt;code&gt;MedInc&lt;/code&gt; ）、avgです。世帯あたりの居住者（ &lt;code&gt;AvgOccup&lt;/code&gt; ）、家の年齢の中央値（ &lt;code&gt;HouseAge&lt;/code&gt; ）、および平均。世帯あたりの部屋数（ &lt;code&gt;AveRooms&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="d1f9d8f4302df476eee3089e3160330e3191c729" translate="yes" xml:space="preserve">
          <source>The plot shows the regions where the discretized encoding is constant.</source>
          <target state="translated">プロットは、離散化された符号化が一定である領域を示しています。</target>
        </trans-unit>
        <trans-unit id="a1523290796b6a8ba4024eaf4b48817249c66e13" translate="yes" xml:space="preserve">
          <source>The plots below illustrate the effect the parameter &lt;code&gt;C&lt;/code&gt; has on the separation line. A large value of &lt;code&gt;C&lt;/code&gt; basically tells our model that we do not have that much faith in our data&amp;rsquo;s distribution, and will only consider points close to line of separation.</source>
          <target state="translated">以下のプロットは、パラメーター &lt;code&gt;C&lt;/code&gt; が分離線に及ぼす影響を示しています。 &lt;code&gt;C&lt;/code&gt; の値が大きいと、基本的にモデルにデータの分布をそれほど信用していないことを伝え、分離線に近い点のみを考慮します。</target>
        </trans-unit>
        <trans-unit id="faaabc25a5f9dd9badc9ef9f18d9770e507882de" translate="yes" xml:space="preserve">
          <source>The plots display firstly what a K-means algorithm would yield using three clusters. It is then shown what the effect of a bad initialization is on the classification process: By setting n_init to only 1 (default is 10), the amount of times that the algorithm will be run with different centroid seeds is reduced. The next plot displays what using eight clusters would deliver and finally the ground truth.</source>
          <target state="translated">プロットは、最初に3つのクラスタを使用して、K-meansアルゴリズムがどのような結果をもたらすかを表示します。次に、初期化が悪かった場合の分類プロセスへの影響が示されています。n_initを1(デフォルトは10)だけに設定することで、異なるセントロイドシードでアルゴリズムが実行される回数が減少します。次のプロットは、8つのクラスターを使用した場合の結果と、最終的な真実を示しています。</target>
        </trans-unit>
        <trans-unit id="3ad47df403d87eb821e2edd090b9e74720bc0be8" translate="yes" xml:space="preserve">
          <source>The plots represent the distribution of the prediction latency as a boxplot.</source>
          <target state="translated">プロットは、予測待ち時間の分布をボックスプロットで表したものです。</target>
        </trans-unit>
        <trans-unit id="ee583f2ef106ae04159c6d135c18b8bf01049699" translate="yes" xml:space="preserve">
          <source>The plots show training points in solid colors and testing points semi-transparent. The lower right shows the classification accuracy on the test set.</source>
          <target state="translated">プロットは学習点を実線で、テスト点を半透明で示しています。右下は、テストセットでの分類精度を示しています。</target>
        </trans-unit>
        <trans-unit id="a10b5c6300ecb650c0782e5a6bff1ffc576100bb" translate="yes" xml:space="preserve">
          <source>The point cloud spanned by the observations above is very flat in one direction: one of the three univariate features can almost be exactly computed using the other two. PCA finds the directions in which the data is not &lt;em&gt;flat&lt;/em&gt;</source>
          <target state="translated">上記の観測にまたがる点群は、一方向に非常に平坦です。3つの一変量特徴の1つは、他の2つを使用してほぼ正確に計算できます。PCAはデータが&lt;em&gt;フラット&lt;/em&gt;でない方向を見つける&lt;em&gt;&lt;/em&gt;</target>
        </trans-unit>
        <trans-unit id="9874b880b8ee39ca50e864dab01006b5213a2351" translate="yes" xml:space="preserve">
          <source>The points.</source>
          <target state="translated">ポイントは</target>
        </trans-unit>
        <trans-unit id="0cdbbfd6a3924811880d5b83f6e26dafaaff0147" translate="yes" xml:space="preserve">
          <source>The polynomial kernel is defined as:</source>
          <target state="translated">多項式カーネルは次のように定義されます。</target>
        </trans-unit>
        <trans-unit id="8b34b53d18875cd269c1341b177c3bcbb9230141" translate="yes" xml:space="preserve">
          <source>The pooled values for each feature cluster.</source>
          <target state="translated">各特徴クラスタのプールされた値。</target>
        </trans-unit>
        <trans-unit id="6285f4c6cbbb9a8676e04ca09291d710e5d4992f" translate="yes" xml:space="preserve">
          <source>The possible options are &amp;lsquo;hinge&amp;rsquo;, &amp;lsquo;log&amp;rsquo;, &amp;lsquo;modified_huber&amp;rsquo;, &amp;lsquo;squared_hinge&amp;rsquo;, &amp;lsquo;perceptron&amp;rsquo;, or a regression loss: &amp;lsquo;squared_loss&amp;rsquo;, &amp;lsquo;huber&amp;rsquo;, &amp;lsquo;epsilon_insensitive&amp;rsquo;, or &amp;lsquo;squared_epsilon_insensitive&amp;rsquo;.</source>
          <target state="translated">可能なオプションは、「hinge」、「log」、「modified_huber」、「squared_hinge」、「perceptron」、または回帰損失です：「squared_loss」、「huber」、「epsilon_insensitive」、または「squared_epsilon_insensitive」。</target>
        </trans-unit>
        <trans-unit id="5a756ae9d1d42224e6a27c29b135c093bd570bbe" translate="yes" xml:space="preserve">
          <source>The power of the Minkowski metric to be used to calculate distance between points.</source>
          <target state="translated">点間距離を計算するために使用するミンコフスキーメトリックのパワー。</target>
        </trans-unit>
        <trans-unit id="581738f7dfe64a35233afe8207a1e82379bfb8cd" translate="yes" xml:space="preserve">
          <source>The power transform is useful as a transformation in modeling problems where homoscedasticity and normality are desired. Below are examples of Box-Cox and Yeo-Johnwon applied to six different probability distributions: Lognormal, Chi-squared, Weibull, Gaussian, Uniform, and Bimodal.</source>
          <target state="translated">べき乗変換は、同種確率と正規性が望まれるモデル化問題における変換として有用である。以下は、6つの異なる確率分布に適用されたBox-CoxとYeo-Johnwonの例です。対数正規分布、カイ二乗分布、ワイブル分布、ガウス分布、一様分布、バイモーダル分布です。</target>
        </trans-unit>
        <trans-unit id="837e2e1ea652ea6696a91670a638bda69523a446" translate="yes" xml:space="preserve">
          <source>The power transform method. Available methods are:</source>
          <target state="translated">電力変換法です。利用可能な方法は以下の通りです。</target>
        </trans-unit>
        <trans-unit id="1605256f2d1d73782f41770777e3757d50960cf1" translate="yes" xml:space="preserve">
          <source>The power transform method. Currently, &amp;lsquo;box-cox&amp;rsquo; (Box-Cox transform) is the only option available.</source>
          <target state="translated">パワー変換メソッド。現在、「box-cox」（Box-Cox変換）のみが使用可能なオプションです。</target>
        </trans-unit>
        <trans-unit id="9e3ef4e072a07501cd2ec598f0a1011b6178f209" translate="yes" xml:space="preserve">
          <source>The precision is the ratio &lt;code&gt;tp / (tp + fp)&lt;/code&gt; where &lt;code&gt;tp&lt;/code&gt; is the number of true positives and &lt;code&gt;fp&lt;/code&gt; the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.</source>
          <target state="translated">精度は比であり、 &lt;code&gt;tp / (tp + fp)&lt;/code&gt; &lt;code&gt;tp&lt;/code&gt; 真陽性の数であり、 &lt;code&gt;fp&lt;/code&gt; を偽陽性の数を。精度は直観的には、分類子が負のサンプルを正としてラベル付けしない能力です。</target>
        </trans-unit>
        <trans-unit id="5e318fd9419ebb0c7685cd2fcf271eb0864ec5bb" translate="yes" xml:space="preserve">
          <source>The precision matrices for each component in the mixture. A precision matrix is the inverse of a covariance matrix. A covariance matrix is symmetric positive definite so the mixture of Gaussian can be equivalently parameterized by the precision matrices. Storing the precision matrices instead of the covariance matrices makes it more efficient to compute the log-likelihood of new samples at test time. The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">混合物の各成分の精度行列。精度行列は、共分散行列の逆です。共分散行列は対称正定行列であるため、ガウス分布の混合は、精度行列によって同等にパラメーター化できます。共分散行列の代わりに精度行列を格納すると、テスト時に新しいサンプルの対数尤度を計算するのがより効率的になります。形状は &lt;code&gt;covariance_type&lt;/code&gt; に依存します。</target>
        </trans-unit>
        <trans-unit id="027e42693446de348f6d01928068eb7453ef8a97" translate="yes" xml:space="preserve">
          <source>The precision matrix associated to the current covariance object.</source>
          <target state="translated">現在の共分散オブジェクトに関連付けられた精度行列.</target>
        </trans-unit>
        <trans-unit id="d78de957c617714f761992022534f1c5cbc37dc0" translate="yes" xml:space="preserve">
          <source>The precision of each components on the mean distribution (Gaussian).</source>
          <target state="translated">平均分布(ガウス分布)上の各成分の精度。</target>
        </trans-unit>
        <trans-unit id="208aaa08d6ec7d945a950063ea09933d0bd5ac66" translate="yes" xml:space="preserve">
          <source>The precision prior on the mean distribution (Gaussian). Controls the extend to where means can be placed. Smaller values concentrate the means of each clusters around &lt;code&gt;mean_prior&lt;/code&gt;.</source>
          <target state="translated">平均分布の事前精度（ガウス）。手段を配置できる場所までの延長を制御します。値が小さいほど、各クラスターの &lt;code&gt;mean_prior&lt;/code&gt; 周りに集中します。</target>
        </trans-unit>
        <trans-unit id="b72296dd4015be0343e22e7dc45bdf0f7e18f582" translate="yes" xml:space="preserve">
          <source>The precision prior on the mean distribution (Gaussian). Controls the extend to where means can be placed. Smaller values concentrate the means of each clusters around &lt;code&gt;mean_prior&lt;/code&gt;. The value of the parameter must be greater than 0. If it is None, it&amp;rsquo;s set to 1.</source>
          <target state="translated">平均分布の事前精度（ガウス）。手段を配置できる場所までの延長を制御します。値が小さいほど、各クラスターの &lt;code&gt;mean_prior&lt;/code&gt; 周りに集中します。パラメータの値は0より大きくなければなりません。Noneの場合、1に設定されます。</target>
        </trans-unit>
        <trans-unit id="03d920e00078b8bd9b4faa0b1aa14a8c65b257bb" translate="yes" xml:space="preserve">
          <source>The precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).</source>
          <target state="translated">精度-リコール曲線は、異なる閾値に対する精度とリコールのトレードオフを示しています。曲線の下の高い領域は、高い再現性と高い精度の両方を表し、高い精度は低い偽陽性率に関連し、高い再現性は低い偽陰性率に関連します。両方とも高いスコアは、分類器が正確な結果を返していること(高精度)と、すべての陽性結果の大部分を返していること(高リコール)を示しています。</target>
        </trans-unit>
        <trans-unit id="b03d17dd4f0a50b42b8760dd1442678e73495423" translate="yes" xml:space="preserve">
          <source>The predicted class C for each sample in X is returned.</source>
          <target state="translated">Xの各サンプルについて予測されたクラスCが返されます.</target>
        </trans-unit>
        <trans-unit id="bfeb54e7bff0bb15dd098a7327cddb86a1801899" translate="yes" xml:space="preserve">
          <source>The predicted class log-probabilities of an input sample is computed as the log of the mean predicted class probabilities of the base estimators in the ensemble.</source>
          <target state="translated">入力サンプルの予測クラス対数確率は、アンサンブル内の基本推定子の平均予測クラス確率の対数として計算される。</target>
        </trans-unit>
        <trans-unit id="cd67555dc49cf41a5e5df91097299e3752772d2d" translate="yes" xml:space="preserve">
          <source>The predicted class log-probabilities of an input sample is computed as the log of the mean predicted class probabilities of the trees in the forest.</source>
          <target state="translated">入力サンプルの予測クラス対数確率は,森林内の木の平均予測クラス確率の対数として計算されます.</target>
        </trans-unit>
        <trans-unit id="eab6e37cbb9069ff9afad75097cea6fc5d34926b" translate="yes" xml:space="preserve">
          <source>The predicted class log-probabilities of an input sample is computed as the weighted mean predicted class log-probabilities of the classifiers in the ensemble.</source>
          <target state="translated">入力サンプルの予測クラス対数確率は,アンサンブル内の分類器の加重平均予測クラス対数確率として計算される.</target>
        </trans-unit>
        <trans-unit id="47d729548b0a8e226d9e4c530c0297fbe0f5665f" translate="yes" xml:space="preserve">
          <source>The predicted class of an input sample is a vote by the trees in the forest, weighted by their probability estimates. That is, the predicted class is the one with highest mean probability estimate across the trees.</source>
          <target state="translated">入力標本の予測されたクラスは,森林内の木による投票であり,その確率推定値で重み付けされている.つまり,予測されたクラスは,木全体の平均確率推定値が最も高いものである.</target>
        </trans-unit>
        <trans-unit id="228bf14c189aaaaad2dc0e65c7c1dff58773904b" translate="yes" xml:space="preserve">
          <source>The predicted class of an input sample is computed as the class with the highest mean predicted probability. If base estimators do not implement a &lt;code&gt;predict_proba&lt;/code&gt; method, then it resorts to voting.</source>
          <target state="translated">入力サンプルの予測クラスは、平均予測確率が最も高いクラスとして計算されます。ベースエスティメータが &lt;code&gt;predict_proba&lt;/code&gt; メソッドを実装しない場合、投票に頼ります。</target>
        </trans-unit>
        <trans-unit id="8652ca51db9ef5a2b15410ff134f217292f6ef55" translate="yes" xml:space="preserve">
          <source>The predicted class of an input sample is computed as the weighted mean prediction of the classifiers in the ensemble.</source>
          <target state="translated">入力サンプルの予測クラスは,アンサンブル内の分類器の重み付き平均予測値として計算されます.</target>
        </trans-unit>
        <trans-unit id="46c6bfcd5571fcbc8ac1e94c5ec5ef7b9ca3bcfc" translate="yes" xml:space="preserve">
          <source>The predicted class probabilities of an input sample are computed as the mean predicted class probabilities of the trees in the forest. The class probability of a single tree is the fraction of samples of the same class in a leaf.</source>
          <target state="translated">入力サンプルの予測クラス確率は、森林内の木の平均予測クラス確率として計算されます。1本の木のクラス確率は,葉の中の同じクラスのサンプルの割合である.</target>
        </trans-unit>
        <trans-unit id="90e3cd15b2277da446a86ed04d9b97d9385dbcbd" translate="yes" xml:space="preserve">
          <source>The predicted class probabilities of an input sample is computed as the mean predicted class probabilities of the base estimators in the ensemble. If base estimators do not implement a &lt;code&gt;predict_proba&lt;/code&gt; method, then it resorts to voting and the predicted class probabilities of an input sample represents the proportion of estimators predicting each class.</source>
          <target state="translated">入力サンプルの予測クラス確率は、集団内の基本推定量の平均予測クラス確率として計算されます。ベース推定器が &lt;code&gt;predict_proba&lt;/code&gt; メソッドを実装しない場合、それは投票に頼り、入力サンプルの予測クラス確率は各クラスを予測する推定器の割合を表します。</target>
        </trans-unit>
        <trans-unit id="f43014d849d28fe556560f6e74f971c02171e223" translate="yes" xml:space="preserve">
          <source>The predicted class probabilities of an input sample is computed as the weighted mean predicted class probabilities of the classifiers in the ensemble.</source>
          <target state="translated">入力サンプルの予測クラス確率は,アンサンブル内の分類器の加重平均予測クラス確率として計算される.</target>
        </trans-unit>
        <trans-unit id="232fb255d370f3424586a7b0c3f74d049bd6d607" translate="yes" xml:space="preserve">
          <source>The predicted class probability is the fraction of samples of the same class in a leaf.</source>
          <target state="translated">予測クラス確率は、葉の中の同じクラスのサンプルの割合です。</target>
        </trans-unit>
        <trans-unit id="45d2cef0bd7682bfccc693d1957f2c12cdb9bae8" translate="yes" xml:space="preserve">
          <source>The predicted class.</source>
          <target state="translated">予測されたクラス。</target>
        </trans-unit>
        <trans-unit id="d01b8d940e0e34c3c7942798bc90fe03e260970d" translate="yes" xml:space="preserve">
          <source>The predicted classes, or the predict values.</source>
          <target state="translated">予測されたクラス、つまり予測値。</target>
        </trans-unit>
        <trans-unit id="b68f3b27cbad35418e1a35aab9bbe1837a195e37" translate="yes" xml:space="preserve">
          <source>The predicted classes.</source>
          <target state="translated">予測されたクラス。</target>
        </trans-unit>
        <trans-unit id="f9c849804a5f2e4c77a6bd9f1a72aeb60a174b11" translate="yes" xml:space="preserve">
          <source>The predicted log-probability of the sample for each class in the model, where classes are ordered as they are in &lt;code&gt;self.classes_&lt;/code&gt;. Equivalent to log(predict_proba(X))</source>
          <target state="translated">モデル内の各クラスのサンプルの予測対数確率。クラスは &lt;code&gt;self.classes_&lt;/code&gt; にあるとおりに順序付けされます。log（predict_proba（X））と同等</target>
        </trans-unit>
        <trans-unit id="b73f981e520b253c83fbe81831bba280a3db569a" translate="yes" xml:space="preserve">
          <source>The predicted probability of the sample for each class in the model, where classes are ordered as they are in &lt;code&gt;self.classes_&lt;/code&gt;.</source>
          <target state="translated">モデル内の各クラスのサンプルの予測確率。クラスは &lt;code&gt;self.classes_&lt;/code&gt; にあるとおりに順序付けされます。</target>
        </trans-unit>
        <trans-unit id="a286cd68d524dde2fbaba23c990f981ec35b78f3" translate="yes" xml:space="preserve">
          <source>The predicted probas.</source>
          <target state="translated">予測されたプロバ</target>
        </trans-unit>
        <trans-unit id="53b10e885ebd4a72b834dc0bd6649d47d09469b6" translate="yes" xml:space="preserve">
          <source>The predicted regression target of an input sample is computed as the mean predicted regression targets of the estimators in the ensemble.</source>
          <target state="translated">入力サンプルの予測回帰目標は、アンサンブル内の推定子の平均予測回帰目標として計算される。</target>
        </trans-unit>
        <trans-unit id="d575d8b045eb46db33f9cbaec364a954b218830a" translate="yes" xml:space="preserve">
          <source>The predicted regression target of an input sample is computed as the mean predicted regression targets of the trees in the forest.</source>
          <target state="translated">入力サンプルの予測回帰目標は、森林内の木の平均予測回帰目標として計算される。</target>
        </trans-unit>
        <trans-unit id="21875214f30fbe829fb7e391b984beb01fcc0748" translate="yes" xml:space="preserve">
          <source>The predicted regression value of an input sample is computed as the weighted median prediction of the classifiers in the ensemble.</source>
          <target state="translated">入力サンプルの予測回帰値は,アンサンブル内の分類器の重み付き中央値予測として計算される.</target>
        </trans-unit>
        <trans-unit id="d728cdaebb39fe6a42651804a843b92d06b095fc" translate="yes" xml:space="preserve">
          <source>The predicted regression values.</source>
          <target state="translated">予測された回帰値。</target>
        </trans-unit>
        <trans-unit id="eb2e0fb384bae49f48977b71692091d27df9ee48" translate="yes" xml:space="preserve">
          <source>The predicted target values.</source>
          <target state="translated">予測された目標値。</target>
        </trans-unit>
        <trans-unit id="16c2ec05bdd825f5e946bffd1661391a4efc1866" translate="yes" xml:space="preserve">
          <source>The predicted value of the input samples.</source>
          <target state="translated">入力サンプルの予測値。</target>
        </trans-unit>
        <trans-unit id="71f7a8826bad533ae16d312cbce730009c206751" translate="yes" xml:space="preserve">
          <source>The predicted values.</source>
          <target state="translated">予測値です。</target>
        </trans-unit>
        <trans-unit id="d3f70f498146a996702d8957eebe13ff93b2e60c" translate="yes" xml:space="preserve">
          <source>The prediction interpolates the observations (at least for regular kernels).</source>
          <target state="translated">予測はオブザベーションを補間する(少なくとも正規のカーネルの場合)。</target>
        </trans-unit>
        <trans-unit id="0a5a460e70a5f18f6e563f520727c4e907b64c8a" translate="yes" xml:space="preserve">
          <source>The prediction is probabilistic (Gaussian) so that one can compute empirical confidence intervals and decide based on those if one should refit (online fitting, adaptive fitting) the prediction in some region of interest.</source>
          <target state="translated">予測は確率的(ガウス分布)なので、経験的信頼区間を計算し、それに基づいて、関心のある領域で予測を再フィット(オンライン・フィッティング、適応的フィッティング)すべきかどうかを決定することができます。</target>
        </trans-unit>
        <trans-unit id="22369282bb204be005e939800f6b06d8c430453c" translate="yes" xml:space="preserve">
          <source>The previously introduced metrics are &lt;strong&gt;not normalized with regards to random labeling&lt;/strong&gt;: this means that depending on the number of samples, clusters and ground truth classes, a completely random labeling will not always yield the same values for homogeneity, completeness and hence v-measure. In particular &lt;strong&gt;random labeling won&amp;rsquo;t yield zero scores especially when the number of clusters is large&lt;/strong&gt;.</source>
          <target state="translated">以前に導入されたメトリックは&lt;strong&gt;、ランダムラベリングに関して正規化&lt;/strong&gt;されて&lt;strong&gt;いません&lt;/strong&gt;。つまり、サンプル、クラスター、グラウンドトゥルースクラスの数によっては、完全にランダムなラベリングでは、均一性、完全性、したがってvメジャーが同じ値になるとは限りません。特に&lt;strong&gt;、クラスターの数が多い場合は特に、ランダムラベリングではスコアがゼロになりません&lt;/strong&gt;。</target>
        </trans-unit>
        <trans-unit id="cd0e1bbfb34ee27f92c4a3d8c327812d7edee1f8" translate="yes" xml:space="preserve">
          <source>The principle behind nearest neighbor methods is to find a predefined number of training samples closest in distance to the new point, and predict the label from these. The number of samples can be a user-defined constant (k-nearest neighbor learning), or vary based on the local density of points (radius-based neighbor learning). The distance can, in general, be any metric measure: standard Euclidean distance is the most common choice. Neighbors-based methods are known as &lt;em&gt;non-generalizing&lt;/em&gt; machine learning methods, since they simply &amp;ldquo;remember&amp;rdquo; all of its training data (possibly transformed into a fast indexing structure such as a &lt;a href=&quot;#ball-tree&quot;&gt;Ball Tree&lt;/a&gt; or &lt;a href=&quot;#kd-tree&quot;&gt;KD Tree&lt;/a&gt;).</source>
          <target state="translated">最近傍法の背後にある原則は、新しい点に最も近い距離にある事前定義済みのトレーニングサンプルを見つけ、それらからラベルを予測することです。サンプル数は、ユーザー定義の定数（k最近傍学習）にすることも、点の局所密度に基づいて変化させることもできます（半径ベースの近傍学習）。距離は、一般に、任意のメートル法の尺度にすることができます。標準のユークリッド距離が最も一般的な選択です。近傍ベースの方法は、すべてのトレーニングデータを単に「記憶」しているため（&lt;a href=&quot;#ball-tree&quot;&gt;ボールツリー&lt;/a&gt;や&lt;a href=&quot;#kd-tree&quot;&gt;KDツリー&lt;/a&gt;などの高速なインデックス構造に変換される可能性があるため）、&lt;em&gt;非一般化&lt;/em&gt;機械学習方法として知られています。</target>
        </trans-unit>
        <trans-unit id="947ce8b8522668844673470c4e6efaad3bb4bafb" translate="yes" xml:space="preserve">
          <source>The prior and posterior of a GP resulting from a &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rationalquadratic#sklearn.gaussian_process.kernels.RationalQuadratic&quot;&gt;&lt;code&gt;RationalQuadratic&lt;/code&gt;&lt;/a&gt; kernel are shown in the following figure:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rationalquadratic#sklearn.gaussian_process.kernels.RationalQuadratic&quot;&gt; &lt;code&gt;RationalQuadratic&lt;/code&gt; &lt;/a&gt;カーネルから生じるGPの前後を次の図に示します。</target>
        </trans-unit>
        <trans-unit id="43f31a8e2502a5518bdd46434b1800e86463052e" translate="yes" xml:space="preserve">
          <source>The prior and posterior of a GP resulting from an ExpSineSquared kernel are shown in the following figure:</source>
          <target state="translated">ExpSineSquaredカーネルから得られるGPの先行値と事後値を次の図に示します。</target>
        </trans-unit>
        <trans-unit id="f92d48b9507eee6a0b35b438f1fb3d6ff89e93fd" translate="yes" xml:space="preserve">
          <source>The prior of the number of degrees of freedom on the covariance distributions (Wishart).</source>
          <target state="translated">共分散分布上の自由度数の事前分布(Wishart)。</target>
        </trans-unit>
        <trans-unit id="f920ca641ad93ad7a821ae4139b67430b9eddb8f" translate="yes" xml:space="preserve">
          <source>The prior of the number of degrees of freedom on the covariance distributions (Wishart). If it is None, it&amp;rsquo;s set to &lt;code&gt;n_features&lt;/code&gt;.</source>
          <target state="translated">共分散分布の自由度の事前分布（ウィシャート）。Noneの場合、 &lt;code&gt;n_features&lt;/code&gt; に設定されます。</target>
        </trans-unit>
        <trans-unit id="cf6f1da4c11f5b6aa97c72a194e67d10417600f3" translate="yes" xml:space="preserve">
          <source>The prior on the covariance distribution (Wishart). If it is None, the emiprical covariance prior is initialized using the covariance of X. The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">共分散分布の事前分布（ウィシャート）。Noneの場合、準共分散事前分布はXの共分散を使用して初期化されます。形状は &lt;code&gt;covariance_type&lt;/code&gt; に依存します。</target>
        </trans-unit>
        <trans-unit id="449bf6ea1f50ae651db1aabfda8187878d697851" translate="yes" xml:space="preserve">
          <source>The prior on the covariance distribution (Wishart). The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">共分散分布の事前分布（ウィシャート）。形状は &lt;code&gt;covariance_type&lt;/code&gt; に依存します。</target>
        </trans-unit>
        <trans-unit id="a7d0d50e2fe2007735b69660533329d841055128" translate="yes" xml:space="preserve">
          <source>The prior on the mean distribution (Gaussian).</source>
          <target state="translated">平均分布(ガウス分布)の優先順位。</target>
        </trans-unit>
        <trans-unit id="70a81456bc5946b8879a5b610e7810c1053668bd" translate="yes" xml:space="preserve">
          <source>The prior on the mean distribution (Gaussian). If it is None, it&amp;rsquo;s set to the mean of X.</source>
          <target state="translated">平均分布の事前分布（ガウス）。Noneの場合は、Xの平均に設定されます。</target>
        </trans-unit>
        <trans-unit id="0c84abbb5dade5fc9d4b47758b34408cb97bc08f" translate="yes" xml:space="preserve">
          <source>The priors over \(\alpha\) and \(\lambda\) are chosen to be &lt;a href=&quot;https://en.wikipedia.org/wiki/Gamma_distribution&quot;&gt;gamma distributions&lt;/a&gt;, the conjugate prior for the precision of the Gaussian.</source>
          <target state="translated">\（\ alpha \）および\（\ lambda \）の事前&lt;a href=&quot;https://en.wikipedia.org/wiki/Gamma_distribution&quot;&gt;分布&lt;/a&gt;は、ガウス分布の精度の共役事前分布であるガンマ分布になるように選択されます。</target>
        </trans-unit>
        <trans-unit id="69c3bd38e0a5cd7a1c6a9d7a1b4382f7891ca245" translate="yes" xml:space="preserve">
          <source>The probability model is created using cross validation, so the results can be slightly different than those obtained by predict. Also, it will produce meaningless results on very small datasets.</source>
          <target state="translated">確率モデルはクロスバリデーションを用いて作成されているため、 predictで得られる結果とは若干異なる結果になる可能性があります。また、非常に小さなデータセットでは意味のない結果が出てしまいます。</target>
        </trans-unit>
        <trans-unit id="5a547056b7368b638d698d05d3f24b68fc3e86df" translate="yes" xml:space="preserve">
          <source>The probability of each class being drawn. Only returned if &lt;code&gt;return_distributions=True&lt;/code&gt;.</source>
          <target state="translated">描画される各クラスの確率。 &lt;code&gt;return_distributions=True&lt;/code&gt; の場合にのみ返されます。</target>
        </trans-unit>
        <trans-unit id="91c6b31a68e2490c2f85bd0b221deab07bc16086" translate="yes" xml:space="preserve">
          <source>The probability of each feature being drawn given each class. Only returned if &lt;code&gt;return_distributions=True&lt;/code&gt;.</source>
          <target state="translated">各クラスを指定して描画される各フィーチャの確率。 &lt;code&gt;return_distributions=True&lt;/code&gt; の場合にのみ返されます。</target>
        </trans-unit>
        <trans-unit id="e0b4e863c306f0bcfef6f345210b44e3fb52ffa9" translate="yes" xml:space="preserve">
          <source>The probability that a coefficient is zero (see notes). Larger values enforce more sparsity.</source>
          <target state="translated">係数がゼロである確率(注を参照)。値が大きいほど疎分散が強くなります。</target>
        </trans-unit>
        <trans-unit id="4678dc803cddccad6a08944794f2a1c194908b2a" translate="yes" xml:space="preserve">
          <source>The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.</source>
          <target state="translated">最適な決定木を学習する問題は、最適性のいくつかの側面において、また単純な概念においてもNP完全であることが知られている。その結果、実用的な決定木学習アルゴリズムは、各ノードで局所的に最適な決定を行う貪欲アルゴリズムのようなヒューリスティックアルゴリズムに基づいている。このようなアルゴリズムは、グローバルに最適な決定木を返すことを保証できません。これは、アンサンブル学習器で複数の木を学習することで緩和することができ、ここでは、特徴とサンプルは置換でランダムにサンプリングされる。</target>
        </trans-unit>
        <trans-unit id="6c819ed9fb97cd33099d0eff9842389e345641fd" translate="yes" xml:space="preserve">
          <source>The problem solved in clustering</source>
          <target state="translated">クラスタリングで解決した問題</target>
        </trans-unit>
        <trans-unit id="2c5b556d82e8f03aa8505b7b204354187717fb43" translate="yes" xml:space="preserve">
          <source>The problem solved in supervised learning</source>
          <target state="translated">教師付き学習で解決した問題</target>
        </trans-unit>
        <trans-unit id="ab484ff296e26d980b5f93762f848e40818065e8" translate="yes" xml:space="preserve">
          <source>The progress meter: the higher the value of &lt;code&gt;verbose&lt;/code&gt;, the more messages:</source>
          <target state="translated">進行状況メーター： &lt;code&gt;verbose&lt;/code&gt; の値が高いほど、メッセージが多くなります。</target>
        </trans-unit>
        <trans-unit id="4163ad4952f27df98667c245d9d8133cfc5f4bae" translate="yes" xml:space="preserve">
          <source>The project mailing list</source>
          <target state="translated">プロジェクトのメーリングリスト</target>
        </trans-unit>
        <trans-unit id="c7689a2c8d3ddf3814b537c25de9417bcceff1dc" translate="yes" xml:space="preserve">
          <source>The projected data.</source>
          <target state="translated">予測されたデータです。</target>
        </trans-unit>
        <trans-unit id="1179732ddee68f2a030de922cca4a7f3acc9e816" translate="yes" xml:space="preserve">
          <source>The proportion of points to be included in the support of the raw MCD estimate. Default is None, which implies that the minimum value of support_fraction will be used within the algorithm: [n_sample + n_features + 1] / 2</source>
          <target state="translated">生のMCD推定値のサポートに含まれるポイントの割合。デフォルトはNoneで、アルゴリズム内ではsupport_fractionの最小値が使用されることを意味します。n_sample+n_features+1]/2</target>
        </trans-unit>
        <trans-unit id="5f706b185c5fa578abf1be5586afd62f084c2356" translate="yes" xml:space="preserve">
          <source>The proportion of points to be included in the support of the raw MCD estimate. If None, the minimum value of support_fraction will be used within the algorithm: &lt;code&gt;[n_sample + n_features + 1] / 2&lt;/code&gt;.</source>
          <target state="translated">生のMCD推定値のサポートに含まれるポイントの割合。Noneの場合、アルゴリズム内でsupport_fractionの最小値が使用されます： &lt;code&gt;[n_sample + n_features + 1] / 2&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="c5ef332af0dc668a636cc813bd9c0d699622c033" translate="yes" xml:space="preserve">
          <source>The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if &lt;code&gt;n_iter_no_change&lt;/code&gt; is set to an integer.</source>
          <target state="translated">早期停止の検証セットとして確保しておくトレーニングデータの割合。0から1の間でなければなりません &lt;code&gt;n_iter_no_change&lt;/code&gt; が整数に設定されている場合にのみ使用されます。</target>
        </trans-unit>
        <trans-unit id="e7040634736e41a179ebec81405480b75c6b1afc" translate="yes" xml:space="preserve">
          <source>The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True</source>
          <target state="translated">早期停止のための検証セットとして設定する学習データの割合。0 から 1 の間でなければなりません。 early_stopping が True の場合にのみ使用されます。</target>
        </trans-unit>
        <trans-unit id="22eea34be82cd504d8c4cf88134dfff15444db48" translate="yes" xml:space="preserve">
          <source>The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True.</source>
          <target state="translated">早期停止のための検証セットとして設定するトレーニングデータの割合。0から1の間でなければなりません。 early_stoppingがTrueの場合にのみ使用されます。</target>
        </trans-unit>
        <trans-unit id="3b21c544ac1d6b2edc3be7d45619235a28b43e4c" translate="yes" xml:space="preserve">
          <source>The proportions of samples assigned to each class. If None, then classes are balanced. Note that if &lt;code&gt;len(weights) == n_classes - 1&lt;/code&gt;, then the last class weight is automatically inferred. More than &lt;code&gt;n_samples&lt;/code&gt; samples may be returned if the sum of &lt;code&gt;weights&lt;/code&gt; exceeds 1.</source>
          <target state="translated">各クラスに割り当てられたサンプルの比率。Noneの場合、クラスはバランスされます。（注）その &lt;code&gt;len(weights) == n_classes - 1&lt;/code&gt; 、最後のクラスの重みを自動的に推測されます。以上 &lt;code&gt;n_samples&lt;/code&gt; の合計場合にサンプルが返されてもよい &lt;code&gt;weights&lt;/code&gt; 1を超えます。</target>
        </trans-unit>
        <trans-unit id="824b6a86b9524b4fdb9e2919749fc7db8e534162" translate="yes" xml:space="preserve">
          <source>The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated by a &amp;lsquo;__&amp;rsquo;, as in the example below. A step&amp;rsquo;s estimator may be replaced entirely by setting the parameter with its name to another estimator, or a transformer removed by setting to None.</source>
          <target state="translated">パイプラインの目的は、さまざまなパラメーターを設定しながら、相互検証できるいくつかのステップを組み立てることです。このため、以下の例のように、名前と「__」で区切られたパラメーター名を使用して、さまざまなステップのパラメーターを設定できます。ステップの推定器は、その名前のパラメーターを別の推定器に設定することによって完全に置き換えるか、Noneに設定することによってトランスフォーマーを削除することができます。</target>
        </trans-unit>
        <trans-unit id="cd8f7005fb1918c1319c0b334e68b25127992a8d" translate="yes" xml:space="preserve">
          <source>The python source code used to generate the model</source>
          <target state="translated">モデルの生成に使用した python のソースコード</target>
        </trans-unit>
        <trans-unit id="f7347085b6a9f16c7df450dca9dbc878bc14f5cc" translate="yes" xml:space="preserve">
          <source>The quantile to predict using the &amp;ldquo;quantile&amp;rdquo; strategy. A quantile of 0.5 corresponds to the median, while 0.0 to the minimum and 1.0 to the maximum.</source>
          <target state="translated">「分位」戦略を使用して予測する分位。分位点0.5は中央値に対応し、0.0は最小値、1.0は最大値に対応します。</target>
        </trans-unit>
        <trans-unit id="38be089aaf53753add8a6e12d9d6e104537de3bf" translate="yes" xml:space="preserve">
          <source>The quantity that we use is the daily variation in quote price: quotes that are linked tend to cofluctuate during a day.</source>
          <target state="translated">使用する量は、気配値の日次変動:連動する気配値が1日の間にコフルクトする傾向があります。</target>
        </trans-unit>
        <trans-unit id="96cf03fa14346bfc08bd6f97110fef23b56f72d1" translate="yes" xml:space="preserve">
          <source>The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor.</source>
          <target state="translated">クエリ点または点。提供されない場合は、インデックス化された各点の隣人が返されます。この場合、問い合わせ点はそれ自身の隣人とはみなされません。</target>
        </trans-unit>
        <trans-unit id="30a7cdee7bb9697635b16e9d03b7cdd4b400cabd" translate="yes" xml:space="preserve">
          <source>The query sample or samples to compute the Local Outlier Factor w.r.t. the training samples.</source>
          <target state="translated">学習サンプルに関連して局所外れ値因子を計算するためのクエリサンプルまたはサンプル.</target>
        </trans-unit>
        <trans-unit id="e9edc4411fbea590103c5860156a749b07db92a2" translate="yes" xml:space="preserve">
          <source>The query sample or samples to compute the Local Outlier Factor w.r.t. to the training samples.</source>
          <target state="translated">局所外れ値因子を計算するためのクエリサンプルまたはサンプル.</target>
        </trans-unit>
        <trans-unit id="2f63e59d1f59e9a5989ccb3ba903c403d9c311f4" translate="yes" xml:space="preserve">
          <source>The radius of the subcluster obtained by merging a new sample and the closest subcluster should be lesser than the threshold. Otherwise a new subcluster is started. Setting this value to be very low promotes splitting and vice-versa.</source>
          <target state="translated">新しいサンプルと最も近いサブクラスターをマージして得られたサブクラスターの半径は、しきい値よりも小さくなければなりません。そうでなければ、新しいサブクラスターが開始されます。この値を非常に低く設定すると分割が促進され,逆に分割が促進されます.</target>
        </trans-unit>
        <trans-unit id="c1fb03bbf68de1d40f0cd10af0722ed019408483" translate="yes" xml:space="preserve">
          <source>The random forest regressor will only ever predict values within the range of observations or closer to zero for each of the targets. As a result the predictions are biased towards the centre of the circle.</source>
          <target state="translated">ランダム・フォレスト回帰器は、オブザベーションの範囲内の値か、ターゲットのそれぞれについてゼロに近い値だけを予測します。その結果、予測は円の中心に偏ります。</target>
        </trans-unit>
        <trans-unit id="ff0bc6a79a727353502babbe6e55a993a0f80508" translate="yes" xml:space="preserve">
          <source>The random number generator is used to generate random chain orders.</source>
          <target state="translated">乱数発生器は、ランダムチェーンオーダーを発生させるために使用されます。</target>
        </trans-unit>
        <trans-unit id="31617e37a4673ca35baf50a5963330e598289ccc" translate="yes" xml:space="preserve">
          <source>The random symmetric, positive-definite matrix.</source>
          <target state="translated">ランダムな対称正定値行列。</target>
        </trans-unit>
        <trans-unit id="0ae670050b82bcbccc27a159ae39c91afa76e218" translate="yes" xml:space="preserve">
          <source>The randomized search and the grid search explore exactly the same space of parameters. The result in parameter settings is quite similar, while the run time for randomized search is drastically lower.</source>
          <target state="translated">ランダム探索とグリッド探索は、全く同じパラメータ空間を探索します。パラメータ設定の結果は非常に似ていますが、ランダム化探索の方が実行時間が大幅に短くなっています。</target>
        </trans-unit>
        <trans-unit id="6f118fa702c125f64290c65f38abc4f6dddff279" translate="yes" xml:space="preserve">
          <source>The raw (unadjusted) Rand index is then given by:</source>
          <target state="translated">そして、生の(調整されていない)ランド指数は次式で与えられます。</target>
        </trans-unit>
        <trans-unit id="6b2fe9bd420d4a6948923a1a40e37c6224028547" translate="yes" xml:space="preserve">
          <source>The raw RI score is then &amp;ldquo;adjusted for chance&amp;rdquo; into the ARI score using the following scheme:</source>
          <target state="translated">次に、次のスキームを使用して、未加工のRIスコアがARIスコアに「調整」されます。</target>
        </trans-unit>
        <trans-unit id="884b3f9d013732b636db9c9b452d7d3559d50f2d" translate="yes" xml:space="preserve">
          <source>The raw robust estimated covariance before correction and re-weighting.</source>
          <target state="translated">補正および再加重前の生のロバスト推定共分散。</target>
        </trans-unit>
        <trans-unit id="70b59f0ad9598471a02599d6a34b12e103ef557b" translate="yes" xml:space="preserve">
          <source>The raw robust estimated location before correction and re-weighting.</source>
          <target state="translated">補正・再加重前の生のロバスト推定位置。</target>
        </trans-unit>
        <trans-unit id="246acb046d6b9bd62c3702633fac1169a8d836d0" translate="yes" xml:space="preserve">
          <source>The real data lies in the &lt;code&gt;filenames&lt;/code&gt; and &lt;code&gt;target&lt;/code&gt; attributes. The target attribute is the integer index of the category:</source>
          <target state="translated">実際のデータは、 &lt;code&gt;filenames&lt;/code&gt; と &lt;code&gt;target&lt;/code&gt; 属性にあります。ターゲット属性は、カテゴリの整数インデックスです。</target>
        </trans-unit>
        <trans-unit id="480e842bb5c6e42d98f06aa4f6c096c0c7aba356" translate="yes" xml:space="preserve">
          <source>The recall is the ratio &lt;code&gt;tp / (tp + fn)&lt;/code&gt; where &lt;code&gt;tp&lt;/code&gt; is the number of true positives and &lt;code&gt;fn&lt;/code&gt; the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.</source>
          <target state="translated">リコールは、比率である &lt;code&gt;tp / (tp + fn)&lt;/code&gt; ここで、 &lt;code&gt;tp&lt;/code&gt; 真陽性の数であり、 &lt;code&gt;fn&lt;/code&gt; の偽陰性の数。再現率は直感的に、すべての陽性サンプルを見つける分類子の能力です。</target>
        </trans-unit>
        <trans-unit id="65f6a86aee18eed07b01f195c73d746d5f2ca909" translate="yes" xml:space="preserve">
          <source>The reconstructed points using the metric MDS and non metric MDS are slightly shifted to avoid overlapping.</source>
          <target state="translated">メトリックデータシートと非メトリックデータシートを用いて再構成された点は、重複を避けるためにわずかにシフトされています。</target>
        </trans-unit>
        <trans-unit id="cefc093c3489c62b159a3ce090f6d8b10ecaa3b1" translate="yes" xml:space="preserve">
          <source>The reconstruction error computed by each routine can be used to choose the optimal output dimension. For a \(d\)-dimensional manifold embedded in a \(D\)-dimensional parameter space, the reconstruction error will decrease as &lt;code&gt;n_components&lt;/code&gt; is increased until &lt;code&gt;n_components == d&lt;/code&gt;.</source>
          <target state="translated">各ルーチンで計算された再構成誤差を使用して、最適な出力次元を選択できます。\（D \）に埋め込まれた次元のマニホールド- - \（D \）のような次元のパラメータ空間、再構成誤差が減少する &lt;code&gt;n_components&lt;/code&gt; にまで増加さ &lt;code&gt;n_components == d&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="cd5de02ec688b7a05331073a7d6e7032a5c96c24" translate="yes" xml:space="preserve">
          <source>The reconstruction with L1 penalization gives a result with zero error (all pixels are successfully labeled with 0 or 1), even if noise was added to the projections. In comparison, an L2 penalization (&lt;a href=&quot;../../modules/generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;sklearn.linear_model.Ridge&lt;/code&gt;&lt;/a&gt;) produces a large number of labeling errors for the pixels. Important artifacts are observed on the reconstructed image, contrary to the L1 penalization. Note in particular the circular artifact separating the pixels in the corners, that have contributed to fewer projections than the central disk.</source>
          <target state="translated">L1ペナルティによる再構成では、ノイズが投影に追加された場合でも、エラーがゼロ（すべてのピクセルが0または1で正常にラベル付けされる）の結果が得られます。比較すると、L2 &lt;a href=&quot;../../modules/generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;sklearn.linear_model.Ridge&lt;/code&gt; &lt;/a&gt;（sklearn.linear_model.Ridge）は、ピクセルに多数のラベリングエラーを生成します。L1ペナルティとは対照的に、再構成された画像には重要なアーティファクトが見られます。特に、中央のディスクよりも少ない投影に寄与している、コーナーのピクセルを分離する円形のアーティファクトに注意してください。</target>
        </trans-unit>
        <trans-unit id="cc89055f829a16401789a0501b3aaaa652548713" translate="yes" xml:space="preserve">
          <source>The reduced distance, defined for some metrics, is a computationally more efficient measure which preserves the rank of the true distance. For example, in the Euclidean distance metric, the reduced distance is the squared-euclidean distance.</source>
          <target state="translated">軽減された距離は、いくつかのメトリックで定義され、真の距離のランクを保持する計算効率の高い尺度です。例えば、ユークリッド距離メトリックでは、縮小された距離は2乗ユークリッド距離です。</target>
        </trans-unit>
        <trans-unit id="d3411437239f8f2dc8ee2706670c4e4a91db1791" translate="yes" xml:space="preserve">
          <source>The reduced samples.</source>
          <target state="translated">縮小されたサンプル。</target>
        </trans-unit>
        <trans-unit id="67c2217cec61f41257c896a393db0ce694f3f635" translate="yes" xml:space="preserve">
          <source>The refitted estimator is made available at the &lt;code&gt;best_estimator_&lt;/code&gt; attribute and permits using &lt;code&gt;predict&lt;/code&gt; directly on this &lt;code&gt;GridSearchCV&lt;/code&gt; instance.</source>
          <target state="translated">再 &lt;code&gt;best_estimator_&lt;/code&gt; された推定量は、best_estimator_属性で利用可能になり、この &lt;code&gt;GridSearchCV&lt;/code&gt; インスタンスで直接 &lt;code&gt;predict&lt;/code&gt; を使用できるようになります。</target>
        </trans-unit>
        <trans-unit id="1178e040e21448dfc3d87635a64add70ee2fc71f" translate="yes" xml:space="preserve">
          <source>The refitted estimator is made available at the &lt;code&gt;best_estimator_&lt;/code&gt; attribute and permits using &lt;code&gt;predict&lt;/code&gt; directly on this &lt;code&gt;RandomizedSearchCV&lt;/code&gt; instance.</source>
          <target state="translated">再 &lt;code&gt;best_estimator_&lt;/code&gt; された推定器は、best_estimator_属性で使用可能になり、この &lt;code&gt;RandomizedSearchCV&lt;/code&gt; インスタンスで直接 &lt;code&gt;predict&lt;/code&gt; を使用できるようになります。</target>
        </trans-unit>
        <trans-unit id="eb2b5c40285ce2a0d935c8174b2b1df487e2e554" translate="yes" xml:space="preserve">
          <source>The regression target or classification labels, if applicable. Dtype is float if numeric, and object if categorical.</source>
          <target state="translated">該当する場合は、回帰対象または分類ラベル。Dtypeは数値の場合はfloat、カテゴリカルな場合はobjectです。</target>
        </trans-unit>
        <trans-unit id="444eaf2365e5ec07e25a0ed19fde31ede0366773" translate="yes" xml:space="preserve">
          <source>The regressor is used to predict and the &lt;code&gt;inverse_func&lt;/code&gt; or &lt;code&gt;inverse_transform&lt;/code&gt; is applied before returning the prediction.</source>
          <target state="translated">&lt;code&gt;inverse_transform&lt;/code&gt; は予測に使用され、 &lt;code&gt;inverse_func&lt;/code&gt; またはreverse_transformは予測を返す前に適用されます。</target>
        </trans-unit>
        <trans-unit id="4f20e2edcb6e4e7d6bdbe8de69b9b24c7725d7e1" translate="yes" xml:space="preserve">
          <source>The regularised covariance is:</source>
          <target state="translated">正則化共分散は</target>
        </trans-unit>
        <trans-unit id="4dfecc4cc3d2dd0e68757701d8e0aef7bde77c4d" translate="yes" xml:space="preserve">
          <source>The regularization mixing parameter, with 0 &amp;lt;= l1_ratio &amp;lt;= 1. For l1_ratio = 0 the penalty is an elementwise L2 penalty (aka Frobenius Norm). For l1_ratio = 1 it is an elementwise L1 penalty. For 0 &amp;lt; l1_ratio &amp;lt; 1, the penalty is a combination of L1 and L2.</source>
          <target state="translated">0 &amp;lt;= l1_ratio &amp;lt;= 1の正則化混合パラメーター。l1_ratio= 0の場合、ペナルティは要素ごとのL2ペナルティ（別名フロベニウスノルム）です。l1_ratio = 1の場合、要素ごとのL1ペナルティです。0 &amp;lt;l1_ratio &amp;lt;1の場合、ペナルティはL1とL2の組み合わせです。</target>
        </trans-unit>
        <trans-unit id="f1711bca786b8ddb98e81cd17b706bc6925eee44" translate="yes" xml:space="preserve">
          <source>The regularization parameter C in the LogisticRegression. When C is an array, fit will take each regularization parameter in C one by one for LogisticRegression and store results for each one in &lt;code&gt;all_scores_&lt;/code&gt;, where columns and rows represent corresponding reg_parameters and features.</source>
          <target state="translated">LogisticRegressionの正則化パラメーターC。Cが配列の場合、fitはCの各正則化パラメーターを1つずつLogisticRegressionに &lt;code&gt;all_scores_&lt;/code&gt; 、それぞれの結果をall_scores_に格納します。列と行は対応するreg_parametersと機能を表します。</target>
        </trans-unit>
        <trans-unit id="7865779166b38ce9dcfe2e41f3eba5295cbd3874" translate="yes" xml:space="preserve">
          <source>The regularization parameter alpha parameter in the Lasso. Warning: this is not the alpha parameter in the stability selection article which is scaling.</source>
          <target state="translated">Lassoの正則化パラメータαパラメータです。警告:これは安定性選択記事のαパラメータではなく、スケーリングです。</target>
        </trans-unit>
        <trans-unit id="0ed37f5cbd4d43ac0f2b6fe6eac86f942c91256a" translate="yes" xml:space="preserve">
          <source>The regularization parameter: the higher alpha, the more regularization, the sparser the inverse covariance.</source>
          <target state="translated">正則化パラメータ:αが高いほど正則化が進み、逆共分散が疎かになります。</target>
        </trans-unit>
        <trans-unit id="7a1b8c0c02e4613adc42b58f49402bda71930b51" translate="yes" xml:space="preserve">
          <source>The regularized (shrunk) covariance is given by:</source>
          <target state="translated">正則化された(縮小された)共分散は次式で与えられます。</target>
        </trans-unit>
        <trans-unit id="0a5a9291b6a07681a32efc9a88d6f2d7eab96435" translate="yes" xml:space="preserve">
          <source>The regularized (shrunk) covariance is:</source>
          <target state="translated">正則化された(縮小された)共分散は</target>
        </trans-unit>
        <trans-unit id="5061099398d935daccb5dfd0421b959c5f17fce1" translate="yes" xml:space="preserve">
          <source>The regularized covariance is given by:</source>
          <target state="translated">正則化された共分散は次式で与えられる。</target>
        </trans-unit>
        <trans-unit id="bba73ee876f52c89494b029f9c7d24e1239abc01" translate="yes" xml:space="preserve">
          <source>The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.</source>
          <target state="translated">レギュラライザは、二乗ユークリッドノルムL2または絶対ノルムL1、またはその両方の組み合わせ(Elastic Net)を使用して、モデルパラメータをゼロベクトルに向かって縮小する損失関数に追加されるペナルティです。レギュラライザーのためにパラメータの更新が0.0を超えると、更新は0.0に切り捨てられ、疎なモデルの学習とオンライン特徴選択が可能になります。</target>
        </trans-unit>
        <trans-unit id="dccfd8ff5de1af25843574f310f33b70fd7f2fcc" translate="yes" xml:space="preserve">
          <source>The relationship between recall and precision can be observed in the stairstep area of the plot - at the edges of these steps a small change in the threshold considerably reduces precision, with only a minor gain in recall.</source>
          <target state="translated">リコールと精度の関係は、プロットの階段状の領域で観察することができます-これらのステップの端では、しきい値の小さな変化が精度を大幅に低下させ、リコールはわずかに増加するだけです。</target>
        </trans-unit>
        <trans-unit id="e4d930448408dd13aba80cfbcc12949bce572769" translate="yes" xml:space="preserve">
          <source>The relative importance of the fat noisy tail of the singular values profile if &lt;code&gt;effective_rank&lt;/code&gt; is not None.</source>
          <target state="translated">&lt;code&gt;effective_rank&lt;/code&gt; がNoneでない場合の特異値プロファイルの太いノイズの多いテールの相対的な重要性。</target>
        </trans-unit>
        <trans-unit id="5b4ee2ad411363b437ce1738486767191903cc20" translate="yes" xml:space="preserve">
          <source>The relative importance of the fat noisy tail of the singular values profile.</source>
          <target state="translated">特異値プロファイルの太いノイジーテールの相対的な重要性。</target>
        </trans-unit>
        <trans-unit id="1bf8ab4629789502195bd390130a1b2aa7e78e4e" translate="yes" xml:space="preserve">
          <source>The relative increment in the results before declaring convergence.</source>
          <target state="translated">収束を宣言する前の結果の相対的な増分。</target>
        </trans-unit>
        <trans-unit id="ae073568b2b2b27a72266212b55fc2cb2d4cd169" translate="yes" xml:space="preserve">
          <source>The relative rank (i.e. depth) of a feature used as a decision node in a tree can be used to assess the relative importance of that feature with respect to the predictability of the target variable. Features used at the top of the tree contribute to the final prediction decision of a larger fraction of the input samples. The &lt;strong&gt;expected fraction of the samples&lt;/strong&gt; they contribute to can thus be used as an estimate of the &lt;strong&gt;relative importance of the features&lt;/strong&gt;. In scikit-learn, the fraction of samples a feature contributes to is combined with the decrease in impurity from splitting them to create a normalized estimate of the predictive power of that feature.</source>
          <target state="translated">ツリーの決定ノードとして使用される特徴の相対ランク（つまり、深さ）を使用して、ターゲット変数の予測可能性に関するその特徴の相対的重要度を評価できます。ツリーの最上部で使用される機能は、入力サンプルの大部分の最終予測決定に役立ちます。したがって&lt;strong&gt;、サンプル&lt;/strong&gt;が寄与する&lt;strong&gt;サンプル&lt;/strong&gt;の&lt;strong&gt;予想される割合&lt;/strong&gt;&lt;strong&gt;は、特徴の相対的重要度の&lt;/strong&gt;推定値として使用でき&lt;strong&gt;ます&lt;/strong&gt;。scikit-learnでは、機能が寄与するサンプルの割合と、それらを分割することによる不純物の減少を組み合わせて、その機能の予測力の正規化された推定値を作成します。</target>
        </trans-unit>
        <trans-unit id="0fc6d12d2c584bef7c5a793374c798219700e42f" translate="yes" xml:space="preserve">
          <source>The remaining singular values&amp;rsquo; tail is fat, decreasing as:</source>
          <target state="translated">残りの特異値のテールは太く、次のように減少します。</target>
        </trans-unit>
        <trans-unit id="006806051caab55f94063393a09eaea2afaeb022" translate="yes" xml:space="preserve">
          <source>The reported averages include micro average (averaging the total true positives, false negatives and false positives), macro average (averaging the unweighted mean per label), weighted average (averaging the support-weighted mean per label) and sample average (only for multilabel classification). See also &lt;a href=&quot;sklearn.metrics.precision_recall_fscore_support#sklearn.metrics.precision_recall_fscore_support&quot;&gt;&lt;code&gt;precision_recall_fscore_support&lt;/code&gt;&lt;/a&gt; for more details on averages.</source>
          <target state="translated">報告された平均には、ミクロ平均（真陽性、偽陰性、偽陽性の合計の平均）、マクロ平均（ラベルごとの加重平均の平均）、加重平均（ラベルごとのサポート加重平均の平均）、およびサンプル平均（マルチラベルの場合のみ）が含まれます分類）。平均の詳細については、&lt;a href=&quot;sklearn.metrics.precision_recall_fscore_support#sklearn.metrics.precision_recall_fscore_support&quot;&gt; &lt;code&gt;precision_recall_fscore_support&lt;/code&gt; &lt;/a&gt;も参照してください。</target>
        </trans-unit>
        <trans-unit id="8f7b2580f38f68e2972536327271d77d45324cd3" translate="yes" xml:space="preserve">
          <source>The residual matrix of X (Xk+1) block is obtained by the deflation on the current X score: x_score.</source>
          <target state="translated">X(Xk+1)ブロックの残差行列は、現在のXスコア:x_scoureに対するデフレによって得られる。</target>
        </trans-unit>
        <trans-unit id="18a2377c2d81e0ebb00644fcb1d33204e8d98249" translate="yes" xml:space="preserve">
          <source>The residual matrix of Y (Yk+1) block is obtained by deflation on the current X score. This performs the PLS regression known as PLS2. This mode is prediction oriented.</source>
          <target state="translated">Y(Yk+1)ブロックの残差行列は、現在のXスコアに対するデフレによって得られる。これにより、PLS2として知られるPLS回帰が実行される。このモードは予測指向である。</target>
        </trans-unit>
        <trans-unit id="953a5ab9533846fbfb5f76815143b2efa25824fa" translate="yes" xml:space="preserve">
          <source>The residual matrix of Y (Yk+1) block is obtained by deflation on the current Y score.</source>
          <target state="translated">Y(Yk+1)ブロックの残差行列は、現在のYスコアにデフレをかけて得られる。</target>
        </trans-unit>
        <trans-unit id="4786c768ceb1dac6ca78b9b9b4bd8e26183b7ef9" translate="yes" xml:space="preserve">
          <source>The residual matrix of Y (Yk+1) block is obtained by deflation on the current Y score. This performs a canonical symmetric version of the PLS regression. But slightly different than the CCA. This is mostly used for modeling.</source>
          <target state="translated">Y (Yk+1)ブロックの残差行列は、現在の Y スコアでのデフレによって得られる。これはPLS回帰の正準対称版を実行する。しかし、CCAとはわずかに異なる。これは主にモデリングに用いられます。</target>
        </trans-unit>
        <trans-unit id="3050c2e87895e094a17bdbd4ce41119b71fa1e6b" translate="yes" xml:space="preserve">
          <source>The result of &lt;a href=&quot;../../modules/linear_model#least-angle-regression&quot;&gt;Least Angle Regression&lt;/a&gt; is much more strongly biased: the difference is reminiscent of the local intensity value of the original image.</source>
          <target state="translated">&lt;a href=&quot;../../modules/linear_model#least-angle-regression&quot;&gt;最小角度回帰&lt;/a&gt;の結果ははるかに強く偏っています。その差は、元の画像の局所的な強度値を連想させます。</target>
        </trans-unit>
        <trans-unit id="2844a2c76b7226f0533d494b8e60503f59b9c4e8" translate="yes" xml:space="preserve">
          <source>The result of &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt;&lt;code&gt;cross_val_predict&lt;/code&gt;&lt;/a&gt; may be different from those obtained using &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; as the elements are grouped in different ways. The function &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; takes an average over cross-validation folds, whereas &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt;&lt;code&gt;cross_val_predict&lt;/code&gt;&lt;/a&gt; simply returns the labels (or probabilities) from several distinct models undistinguished. Thus, &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt;&lt;code&gt;cross_val_predict&lt;/code&gt;&lt;/a&gt; is not an appropriate measure of generalisation error.</source>
          <target state="translated">結果&lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt; &lt;code&gt;cross_val_predict&lt;/code&gt; を&lt;/a&gt;使用して得られたものとは異なっていてもよい&lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; の&lt;/a&gt;要素が異なる方法でグループ化されているように。関数&lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; は&lt;/a&gt;に対し、交差検定フォールドの平均をとる&lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt; &lt;code&gt;cross_val_predict&lt;/code&gt; は、&lt;/a&gt;単にいくつかの異なるモデルからの標識（又は確率）は平凡返します。したがって、&lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt; &lt;code&gt;cross_val_predict&lt;/code&gt; &lt;/a&gt;は、汎化誤差の適切な尺度ではありません。</target>
        </trans-unit>
        <trans-unit id="a51791cdd2f13d5121648ef37a39961811acb402" translate="yes" xml:space="preserve">
          <source>The result of calling &lt;code&gt;fit&lt;/code&gt; on a &lt;code&gt;GridSearchCV&lt;/code&gt; object is a classifier that we can use to &lt;code&gt;predict&lt;/code&gt;:</source>
          <target state="translated">呼び出しの結果 &lt;code&gt;fit&lt;/code&gt; 上 &lt;code&gt;GridSearchCV&lt;/code&gt; のオブジェクトは、我々がするために使用できる分類器である &lt;code&gt;predict&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="b0c66fbb583b621a3ed191db16f52e8d9fa96072" translate="yes" xml:space="preserve">
          <source>The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efficiently since only the diagonal is evaluated.</source>
          <target state="translated">この方法の結果はnp.diag(self(X))と同じですが、対角線のみを評価するので、より効率的に評価できます。</target>
        </trans-unit>
        <trans-unit id="629d23e2107cca0c4a5d2061641bb34723082f2c" translate="yes" xml:space="preserve">
          <source>The result points are &lt;em&gt;not&lt;/em&gt; necessarily sorted by distance to their query point.</source>
          <target state="translated">結果ポイントは、クエリポイントまでの距離でソートされるとは&lt;em&gt;限りません&lt;/em&gt;。</target>
        </trans-unit>
        <trans-unit id="dc861dceeba49e88611e2b04a0b3ec8cf37af89c" translate="yes" xml:space="preserve">
          <source>The resulting Calinski-Harabaz score.</source>
          <target state="translated">結果的にカリンスキー-ハラバズのスコア。</target>
        </trans-unit>
        <trans-unit id="91d97654f948931244cdd794d37fd9ac58fe871c" translate="yes" xml:space="preserve">
          <source>The resulting Davies-Bouldin score.</source>
          <target state="translated">結果としてのデイヴィス=ボルダンのスコア。</target>
        </trans-unit>
        <trans-unit id="cfd8b506a89d0c11900fefa11e6003ff64d7a508" translate="yes" xml:space="preserve">
          <source>The resulting Fowlkes-Mallows score.</source>
          <target state="translated">結果的に得られたファウルクス・マロウズのスコア。</target>
        </trans-unit>
        <trans-unit id="82343548ff27f39a450415f81d355404a35e8f50" translate="yes" xml:space="preserve">
          <source>The resulting bicluster structure is block-diagonal, since each row and each column belongs to exactly one bicluster.</source>
          <target state="translated">各行と各列は正確に1つの双クラスタに属しているため、結果として得られる双クラスタ構造はブロック対角です。</target>
        </trans-unit>
        <trans-unit id="af7844c7ade28bb6e966130c36de65d0b613a4b9" translate="yes" xml:space="preserve">
          <source>The resulting dataset contains ordinal attributes which can be further used in a &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">結果のデータセットには、&lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt; で&lt;/a&gt;さらに使用できる序数属性が含まれています。</target>
        </trans-unit>
        <trans-unit id="2b9ee0d5d80ffdad0cbeed5368095411cec46727" translate="yes" xml:space="preserve">
          <source>The resulting kernel is defined as k_exp(X, Y) = k(X, Y) ** exponent</source>
          <target state="translated">結果として得られるカーネルは k_exp(X,Y)=k(X,Y)**指数として定義されます。</target>
        </trans-unit>
        <trans-unit id="5f2e205585c93945d55d6e2cae73c2d9f60e4b99" translate="yes" xml:space="preserve">
          <source>The resulting kernel is defined as k_prod(X, Y) = k1(X, Y) * k2(X, Y)</source>
          <target state="translated">結果として得られるカーネルは k_prod(X,Y)=k1(X,Y)*k2(X,Y)と定義されます。</target>
        </trans-unit>
        <trans-unit id="65c4e9abf2f65b5e239efef34833d44fb903de78" translate="yes" xml:space="preserve">
          <source>The resulting kernel is defined as k_sum(X, Y) = k1(X, Y) + k2(X, Y)</source>
          <target state="translated">結果として得られるカーネルは k_sum(X,Y)=k1(X,Y)+k2(X,Y)と定義されます。</target>
        </trans-unit>
        <trans-unit id="fb9ca9651a528caa2f6deb96ee39233de0fa48d4" translate="yes" xml:space="preserve">
          <source>The resulting model is called &lt;em&gt;Bayesian Ridge Regression&lt;/em&gt;, and is similar to the classical &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt;. The parameters \(w\), \(\alpha\) and \(\lambda\) are estimated jointly during the fit of the model. The remaining hyperparameters are the parameters of the gamma priors over \(\alpha\) and \(\lambda\). These are usually chosen to be &lt;em&gt;non-informative&lt;/em&gt;. The parameters are estimated by maximizing the &lt;em&gt;marginal log likelihood&lt;/em&gt;.</source>
          <target state="translated">結果のモデルは&lt;em&gt;ベイジアンリッジ回帰&lt;/em&gt;と呼ばれ、従来の&lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; &lt;/a&gt;似ています。パラメータ\（w \）、\（\ alpha \）、および\（\ lambda \）は、モデルの適合中に一緒に推定されます。残りのハイパーパラメーターは、\（\ alpha \）と\（\ lambda \）に対するガンマ事前分布のパラメーターです。これらは通常、&lt;em&gt;情報を提供&lt;/em&gt;しないように選択されます。パラメータは、&lt;em&gt;限界対数尤度を&lt;/em&gt;最大化することによって推定され&lt;em&gt;ます&lt;/em&gt;。</target>
        </trans-unit>
        <trans-unit id="2ff5ad65586245c919e0a8e1ef11c4948b5606a5" translate="yes" xml:space="preserve">
          <source>The resulting patches are allocated in a dedicated array.</source>
          <target state="translated">結果として得られたパッチは、専用の配列に割り当てられます。</target>
        </trans-unit>
        <trans-unit id="64dfac2f5dc0c167f2eb731c8522fdbbf80022e7" translate="yes" xml:space="preserve">
          <source>The resulting transformer has then learned a supervised, sparse, high-dimensional categorical embedding of the data.</source>
          <target state="translated">結果として得られた変換器は、その後、データの教師付き、疎な、高次元のカテゴリ埋め込みを学習します。</target>
        </trans-unit>
        <trans-unit id="3f9dd224b05fada5edc12dcb7a4c8d5421d61c2d" translate="yes" xml:space="preserve">
          <source>The return value is a cross-validator which generates the train/test splits via the &lt;code&gt;split&lt;/code&gt; method.</source>
          <target state="translated">戻り値は、 &lt;code&gt;split&lt;/code&gt; メソッドを介してトレイン/テストスプリットを生成するクロスバリデーターです。</target>
        </trans-unit>
        <trans-unit id="63a7df0fd8542a7b486adfe1466de16955c3587a" translate="yes" xml:space="preserve">
          <source>The returned dataset is a &lt;code&gt;scikit-learn&lt;/code&gt; &amp;ldquo;bunch&amp;rdquo;: a simple holder object with fields that can be both accessed as python &lt;code&gt;dict&lt;/code&gt; keys or &lt;code&gt;object&lt;/code&gt; attributes for convenience, for instance the &lt;code&gt;target_names&lt;/code&gt; holds the list of the requested category names:</source>
          <target state="translated">返されるデータセットは &lt;code&gt;scikit-learn&lt;/code&gt; の「束」です &lt;code&gt;dict&lt;/code&gt; キーまたは &lt;code&gt;object&lt;/code&gt; 属性としてアクセスできるフィールドを持つ単純なホルダーオブジェクトです。たとえば、 &lt;code&gt;target_names&lt;/code&gt; はリクエストされたカテゴリ名のリストを保持しています。</target>
        </trans-unit>
        <trans-unit id="c5d262a3b65ccb350b8a1d10b0eaf6eba41fc62d" translate="yes" xml:space="preserve">
          <source>The returned estimates for all classes are ordered by label of classes.</source>
          <target state="translated">返された全クラスの推定値は、クラスのラベル順に並べられています。</target>
        </trans-unit>
        <trans-unit id="8520c10c8edb7f58383ef36900c902f5398bf785" translate="yes" xml:space="preserve">
          <source>The returned estimates for all classes are ordered by the label of classes.</source>
          <target state="translated">返された全クラスの推定値は、クラスのラベル順に並べられています。</target>
        </trans-unit>
        <trans-unit id="9fa1a308376d0d44aa58fe9b54fd102c1f0b956a" translate="yes" xml:space="preserve">
          <source>The returned object is a MemorizedFunc object, that is callable (behaves like a function), but offers extra methods for cache lookup and management. See the documentation for &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/memory.html#joblib.memory.MemorizedFunc&quot;&gt;&lt;code&gt;joblib.memory.MemorizedFunc&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">返されるオブジェクトは、呼び出し可能な（関数のように動作する）MemorizedFuncオブジェクトですが、キャッシュの検索と管理のための追加のメソッドを提供します。&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/memory.html#joblib.memory.MemorizedFunc&quot;&gt; &lt;code&gt;joblib.memory.MemorizedFunc&lt;/code&gt; &lt;/a&gt;のドキュメントを参照してください。</target>
        </trans-unit>
        <trans-unit id="4a1e44716730f4f771067bf0b441674e2034e883" translate="yes" xml:space="preserve">
          <source>The richer dictionary on the right is not larger in size, heavier subsampling is performed in order to stay on the same order of magnitude.</source>
          <target state="translated">右側のリッチな辞書はサイズが大きくないので、同じオーダーにとどまるように重いサブサンプリングが行われます。</target>
        </trans-unit>
        <trans-unit id="6f396634dd18eef11c3d60404319f2831b13040b" translate="yes" xml:space="preserve">
          <source>The right figures correspond to the same plots but using instead a bagging ensemble of decision trees. In both figures, we can observe that the bias term is larger than in the previous case. In the upper right figure, the difference between the average prediction (in cyan) and the best possible model is larger (e.g., notice the offset around &lt;code&gt;x=2&lt;/code&gt;). In the lower right figure, the bias curve is also slightly higher than in the lower left figure. In terms of variance however, the beam of predictions is narrower, which suggests that the variance is lower. Indeed, as the lower right figure confirms, the variance term (in green) is lower than for single decision trees. Overall, the bias- variance decomposition is therefore no longer the same. The tradeoff is better for bagging: averaging several decision trees fit on bootstrap copies of the dataset slightly increases the bias term but allows for a larger reduction of the variance, which results in a lower overall mean squared error (compare the red curves int the lower figures). The script output also confirms this intuition. The total error of the bagging ensemble is lower than the total error of a single decision tree, and this difference indeed mainly stems from a reduced variance.</source>
          <target state="translated">右の図は同じプロットに対応していますが、代わりに決定木のバギング集団を使用しています。どちらの図でも、バイアス項が前のケースよりも大きいことがわかります。右上の図では、平均予測（シアン色）と可能な限り最良のモデルとの差が大きくなっています（たとえば、 &lt;code&gt;x=2&lt;/code&gt; あたりのオフセットに注意してください））。右下の図では、バイアス曲線も左下の図よりわずかに高くなっています。ただし、分散に関しては、予測のビームは狭く、分散が低いことを示唆しています。確かに、右下の図で確認できるように、（緑色の）分散項は単一決定木の場合よりも低くなっています。したがって、全体として、バイアス分散分解は同じではなくなります。トレードオフはバギングに適しています。データセットのブートストラップコピーに適合するいくつかの決定木を平均化すると、バイアス項がわずかに増加しますが、分散の大幅な削減が可能になり、全体的な平均二乗誤差が低くなります（下の赤い曲線を比較してください）図）。スクリプト出力もこの直感を確認します。バギング集団の総誤差は、単一の決定木の総誤差よりも低く、そしてこの違いは確かに主に分散の減少に起因しています。</target>
        </trans-unit>
        <trans-unit id="4281560832d700a912bb9768ad9253748f3986db" translate="yes" xml:space="preserve">
          <source>The right plot shows the mean squared error between the coefficients found by the model and the chosen vector w. Less regularised models retrieve the exact coefficients (error is equal to 0), stronger regularised models increase the error.</source>
          <target state="translated">右のプロットは,モデルによって発見された係数と選択されたベクトルwとの間の平均二乗誤差を示しています.</target>
        </trans-unit>
        <trans-unit id="e91440f2fdf83c048c6ec145aa4ba8b2408d7a77" translate="yes" xml:space="preserve">
          <source>The robust MCD, that has a low error provided \(n_\text{samples} &amp;gt; 5n_\text{features}\)</source>
          <target state="translated">提供されるエラーの少ない堅牢なMCD \（n_ \ text {samples}&amp;gt; 5n_ \ text {features} \）</target>
        </trans-unit>
        <trans-unit id="17f09e161fdb50b4bf88ea2669cf0485f026fd48" translate="yes" xml:space="preserve">
          <source>The rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length and Petal Width.</source>
          <target state="translated">行はサンプル、列は以下の通りです。セパルの長さ、セパルの幅、花びらの長さ、花びらの幅。</target>
        </trans-unit>
        <trans-unit id="075c19426795ecca36fadcd5142db0c818eae0c2" translate="yes" xml:space="preserve">
          <source>The s parameter used to randomly scale the penalty of different features. Should be between 0 and 1.</source>
          <target state="translated">s パラメータは,異なる特徴のペナルティをランダムにスケーリングするために使用されます.0から1の間でなければなりません.</target>
        </trans-unit>
        <trans-unit id="8e103a284fd35ca6afde93378ca71e413fedf538" translate="yes" xml:space="preserve">
          <source>The same group will not appear in two different folds (the number of distinct groups has to be at least equal to the number of folds).</source>
          <target state="translated">同じグループが二つ折りになっても出てこない(はっきりとしたグループの数は、少なくとも二つ折りの数と同じでなければならない)。</target>
        </trans-unit>
        <trans-unit id="0bd48a9cd63a739602e7de633cedbe34c0721a4f" translate="yes" xml:space="preserve">
          <source>The same instance of the transformer can then be applied to some new test data unseen during the fit call: the same scaling and shifting operations will be applied to be consistent with the transformation performed on the train data:</source>
          <target state="translated">変換器の同じ イ ン ス タ ン ス を、 はめ込みの呼び出 し の際には見られなかった新しいテ ス ト デー タ に適用す る こ と がで き ます:訓練デー タ で実行 さ れた変換 と 一致す る よ う 、 同じ ス ケー リ ン グ と シ フ ト 演算が適用 さ れます。</target>
        </trans-unit>
        <trans-unit id="c16a50cac2b1dc1101733e88917cf565ad0ad55a" translate="yes" xml:space="preserve">
          <source>The sample counts that are shown are weighted with any sample_weights that might be present.</source>
          <target state="translated">表示されるサンプル数は、存在する可能性のあるsample_weightsで重み付けされています。</target>
        </trans-unit>
        <trans-unit id="da56cc1e3278be97e394d14d7afaac125d975593" translate="yes" xml:space="preserve">
          <source>The sample weighting rescales the C parameter, which means that the classifier puts more emphasis on getting these points right. The effect might often be subtle. To emphasize the effect here, we particularly weight outliers, making the deformation of the decision boundary very visible.</source>
          <target state="translated">サンプルの重み付けは,Cパラメータを再スケーリングします.これは,分類器がこれらのポイントを正しく取得することをより重視することを意味します.この効果は,しばしば微妙なものかもしれません.ここでは,この効果を強調するために,特に外れ値に重みをつけて,決定境界の変形を非常に目に見えるようにしています.</target>
        </trans-unit>
        <trans-unit id="67f0f1888d07b20765c25e213bac379b2147854b" translate="yes" xml:space="preserve">
          <source>The sampled subsets of integer. The subset of selected integer might not be randomized, see the method argument.</source>
          <target state="translated">整数のサンプリングされた部分集合。選択された整数の部分集合はランダム化されないかもしれません。</target>
        </trans-unit>
        <trans-unit id="582c77a6e6e223c6a451ff779c949c2d01a1e4b1" translate="yes" xml:space="preserve">
          <source>The samples in this dataset correspond to 30&amp;times;30m patches of forest in the US, collected for the task of predicting each patch&amp;rsquo;s cover type, i.e. the dominant species of tree. There are seven covertypes, making this a multiclass classification problem. Each sample has 54 features, described on the &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Covertype&quot;&gt;dataset&amp;rsquo;s homepage&lt;/a&gt;. Some of the features are boolean indicators, while others are discrete or continuous measurements.</source>
          <target state="translated">このデータセットのサンプルは、米国の30&amp;times;30mの森林のパッチに対応しており、各パッチのカバータイプ、つまり主要な樹種を予測するために収集されています。7つのカバータイプがあるため、これはマルチクラス分類問題になります。各サンプルには、&lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Covertype&quot;&gt;データセットのホームページに&lt;/a&gt;記載されている54の機能があります。一部の機能はブールインジケーターですが、その他の機能は離散測定または連続測定です。</target>
        </trans-unit>
        <trans-unit id="3faf112740a094590f64233617c65a5fa097ee8f" translate="yes" xml:space="preserve">
          <source>The samples.</source>
          <target state="translated">サンプルです。</target>
        </trans-unit>
        <trans-unit id="1aa98782517735bf609d139ad8030ba79e53e38f" translate="yes" xml:space="preserve">
          <source>The scaler instance can then be used on new data to transform it the same way it did on the training set:</source>
          <target state="translated">スケーラーのインスタンスを新しいデータ上で使用して、学習セットと同じように変換することができます。</target>
        </trans-unit>
        <trans-unit id="fae25cf01c011421f9b169e383c16cc5b0e7dc5d" translate="yes" xml:space="preserve">
          <source>The scikit-learn project provides a set of machine learning tools that can be used both for novelty or outlier detection. This strategy is implemented with objects learning in an unsupervised way from the data:</source>
          <target state="translated">scikit-learnプロジェクトは、新規性や外れ値検出の両方に使用できる機械学習ツールのセットを提供します。この戦略は、データから教師なしの方法でオブジェクトを学習して実装されています。</target>
        </trans-unit>
        <trans-unit id="b2bcbd53d39d84e38eaccf61f40a5b7e3d9f1072" translate="yes" xml:space="preserve">
          <source>The scikit-learn provides an object &lt;a href=&quot;generated/sklearn.covariance.ellipticenvelope#sklearn.covariance.EllipticEnvelope&quot;&gt;&lt;code&gt;covariance.EllipticEnvelope&lt;/code&gt;&lt;/a&gt; that fits a robust covariance estimate to the data, and thus fits an ellipse to the central data points, ignoring points outside the central mode.</source>
          <target state="translated">scikit-learnは、ロバストな共分散推定をデータに適合させるオブジェクト&lt;a href=&quot;generated/sklearn.covariance.ellipticenvelope#sklearn.covariance.EllipticEnvelope&quot;&gt; &lt;code&gt;covariance.EllipticEnvelope&lt;/code&gt; &lt;/a&gt;を提供し、中央モードの外側の点を無視して楕円を中央データポイントに適合させます。</target>
        </trans-unit>
        <trans-unit id="c6f20ecec2f178819b742529ff67a9012c4e74b9" translate="yes" xml:space="preserve">
          <source>The score above which features should be selected.</source>
          <target state="translated">どの機能を選択すべきかの上のスコア。</target>
        </trans-unit>
        <trans-unit id="fbc2c7bdd77bdb62a30109845f32d883a40f3289" translate="yes" xml:space="preserve">
          <source>The score array for test scores on each cv split.</source>
          <target state="translated">各 cv 分割のテストスコアを表すスコア配列.</target>
        </trans-unit>
        <trans-unit id="21374122c0da50ea660a65ea3274fa15ca9abbe5" translate="yes" xml:space="preserve">
          <source>The score array for train scores on each cv split. This is available only if &lt;code&gt;return_train_score&lt;/code&gt; parameter is &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">各CV分割のトレーニングスコアのスコア配列。これは、 &lt;code&gt;return_train_score&lt;/code&gt; パラメータが &lt;code&gt;True&lt;/code&gt; の場合にのみ使用できます。</target>
        </trans-unit>
        <trans-unit id="f9f0e8fbf15571f500f19345d44e98db663f1949" translate="yes" xml:space="preserve">
          <source>The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. Scores around zero indicate overlapping clusters.</source>
          <target state="translated">スコアは,正しくないクラスタリングの場合は-1,非常に密なクラスタリングの場合は+1の間に制限される.ゼロ付近のスコアは,クラスタが重複していることを示す.</target>
        </trans-unit>
        <trans-unit id="10419dac5247bd799d768931a57ece7edc36ff14" translate="yes" xml:space="preserve">
          <source>The score is defined as ratio between the within-cluster dispersion and the between-cluster dispersion.</source>
          <target state="translated">スコアは、クラスタ内分散とクラスタ間分散の比率として定義されます。</target>
        </trans-unit>
        <trans-unit id="a5fc2e46656049d91b95dd6363e2ef13687eb710" translate="yes" xml:space="preserve">
          <source>The score is defined as the ratio of within-cluster distances to between-cluster distances.</source>
          <target state="translated">スコアは、クラスタ内距離とクラスタ間距離の比として定義されます。</target>
        </trans-unit>
        <trans-unit id="a9ccc42adfd31440d43d06c5c4aab96f5e8222f0" translate="yes" xml:space="preserve">
          <source>The score is fast to compute</source>
          <target state="translated">スコアの計算が速い</target>
        </trans-unit>
        <trans-unit id="298450881e1d83a617f17a47cba5b823081f8332" translate="yes" xml:space="preserve">
          <source>The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster.</source>
          <target state="translated">クラスターが密集していて、よく分離されているときにスコアが高くなり、これはクラスターの標準的な概念に関係しています。</target>
        </trans-unit>
        <trans-unit id="8ebec04d9506729ea096f793265427e501ac6359" translate="yes" xml:space="preserve">
          <source>The score ranges from 0 to 1, or when &lt;code&gt;adjusted=True&lt;/code&gt; is used, it rescaled to the range \(\frac{1}{1 - \text{n\_classes}}\) to 1, inclusive, with performance at random scoring 0.</source>
          <target state="translated">スコアの範囲は0〜1です。または、 &lt;code&gt;adjusted=True&lt;/code&gt; を使用すると、ランダムなパフォーマンスで、\（\ frac {1} {1-\ text {n \ _classes}} \）〜1の範囲に再スケーリングされます。スコア0。</target>
        </trans-unit>
        <trans-unit id="99f201771620c880cb8fcf0a4290517d17eb61e7" translate="yes" xml:space="preserve">
          <source>The score ranges from 0 to 1. A high value indicates a good similarity between two clusters.</source>
          <target state="translated">スコアは0から1の範囲です。高い値は、2つのクラスタ間の類似性が高いことを示します。</target>
        </trans-unit>
        <trans-unit id="67703aed8c056d27307c0c36c3685d8f65de746b" translate="yes" xml:space="preserve">
          <source>The scorer callable object / function must have its signature as &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt;.</source>
          <target state="translated">スコアラー呼び出し可能オブジェクト/関数には、 &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt; としての署名が必要です。</target>
        </trans-unit>
        <trans-unit id="5e77d6fe812bc8f046068c5fa078dda3a1146e44" translate="yes" xml:space="preserve">
          <source>The scorer.</source>
          <target state="translated">点取り屋さん。</target>
        </trans-unit>
        <trans-unit id="dd1c18a79b397962bdc5e0312d35f12a484f084a" translate="yes" xml:space="preserve">
          <source>The scores for each feature along the path.</source>
          <target state="translated">パスに沿った各特徴のスコア。</target>
        </trans-unit>
        <trans-unit id="81a1b1406082100f034b3df6c2ec24562a123854" translate="yes" xml:space="preserve">
          <source>The scores obtained for each permutations.</source>
          <target state="translated">各パーマネテーションについて得られたスコア。</target>
        </trans-unit>
        <trans-unit id="12753b21f50efe6accfab886a283f46c3614c6fe" translate="yes" xml:space="preserve">
          <source>The scores of HuberRegressor may not be compared directly to both TheilSen and RANSAC because it does not attempt to completely filter the outliers but lessen their effect.</source>
          <target state="translated">HuberRegressorのスコアは、外れ値を完全にフィルタリングするのではなく、その影響を軽減しようとしているため、TheilSenとRANSACの両方と直接比較することはできません。</target>
        </trans-unit>
        <trans-unit id="48d352a6767e745c328aec9195da7218a62bd613" translate="yes" xml:space="preserve">
          <source>The scores of all the scorers are available in the &lt;code&gt;cv_results_&lt;/code&gt; dict at keys ending in &lt;code&gt;'_&amp;lt;scorer_name&amp;gt;'&lt;/code&gt; (&lt;code&gt;'mean_test_precision'&lt;/code&gt;, &lt;code&gt;'rank_test_precision'&lt;/code&gt;, etc&amp;hellip;)</source>
          <target state="translated">すべてのスコアラーのスコアは、 &lt;code&gt;'_&amp;lt;scorer_name&amp;gt;'&lt;/code&gt; 終わるキーの &lt;code&gt;cv_results_&lt;/code&gt; dictで利用できます（ &lt;code&gt;'mean_test_precision'&lt;/code&gt; 、 &lt;code&gt;'rank_test_precision'&lt;/code&gt; など...）</target>
        </trans-unit>
        <trans-unit id="1676248dcc2b8fd0688c9d8da4bc193152185788" translate="yes" xml:space="preserve">
          <source>The search for the optimal penalization parameter (alpha) is done on an iteratively refined grid: first the cross-validated scores on a grid are computed, then a new refined grid is centered around the maximum, and so on.</source>
          <target state="translated">最適なペナルティパラメータ(α)の探索は、反復的に洗練されたグリッド上で行われます:最初にグリッド上のクロスバリデートされたスコアが計算され、次に最大値を中心とした新たな洗練されたグリッドが計算されます。</target>
        </trans-unit>
        <trans-unit id="929b69ae47a09aabafb756d8ff0633d79e90c985" translate="yes" xml:space="preserve">
          <source>The searched parameter.</source>
          <target state="translated">検索されたパラメータ。</target>
        </trans-unit>
        <trans-unit id="0d9a4b24aef1596fe1105c7b9e28ffdd6e589d45" translate="yes" xml:space="preserve">
          <source>The second base-kernel of the product-kernel</source>
          <target state="translated">製品カーネルの第二ベースカーネル</target>
        </trans-unit>
        <trans-unit id="2d0afedea31e6e04d617f4edd60a150835c5916d" translate="yes" xml:space="preserve">
          <source>The second base-kernel of the sum-kernel</source>
          <target state="translated">和カーネルの第二基底カーネル</target>
        </trans-unit>
        <trans-unit id="544eb81771c23c2f620b00755dbcd12f00cacc66" translate="yes" xml:space="preserve">
          <source>The second example shows the ability of the Minimum Covariance Determinant robust estimator of covariance to concentrate on the main mode of the data distribution: the location seems to be well estimated, although the covariance is hard to estimate due to the banana-shaped distribution. Anyway, we can get rid of some outlying observations. The One-Class SVM is able to capture the real data structure, but the difficulty is to adjust its kernel bandwidth parameter so as to obtain a good compromise between the shape of the data scatter matrix and the risk of over-fitting the data.</source>
          <target state="translated">2番目の事例は、データ分布のメイン・モードに集中する共分散の最小共分散決定子ロバスト推定器の能力を示しています:共分散はバナナ型分布のために推定しにくいですが、位置はよく推定されているようです。いずれにしても、いくつかの外れたオブザベーションを取り除くことができます。ワンクラスSVMは実際のデータ構造を捉えることができますが、困難なのは、データ散乱行列の形状とデータにオーバーフィットするリスクとの間で良い妥協点を得るために、そのカーネル帯域幅パラメータを調整することです。</target>
        </trans-unit>
        <trans-unit id="5a46c67db9268b64cb76670e9e1b845e906b608f" translate="yes" xml:space="preserve">
          <source>The second figure shows the calibration curve of a linear support-vector classifier (LinearSVC). LinearSVC shows the opposite behavior as Gaussian naive Bayes: the calibration curve has a sigmoid curve, which is typical for an under-confident classifier. In the case of LinearSVC, this is caused by the margin property of the hinge loss, which lets the model focus on hard samples that are close to the decision boundary (the support vectors).</source>
          <target state="translated">2番目の図は、線形支持ベクトル分類器(LinearSVC)の検量線を示しています。LinearSVCは、ガウスナイーブベイズとは逆の挙動を示します:校正曲線はシグモイド曲線を持ち、これは信頼度の低い分類器に典型的なものです。LinearSVCの場合、これはヒンジ損失のマージン特性によって引き起こされ、モデルは決定境界に近いハードサンプル(支持ベクトル)に焦点を合わせることができます。</target>
        </trans-unit>
        <trans-unit id="e56142dda4889d67d8f5448567e56cb678c48e3c" translate="yes" xml:space="preserve">
          <source>The second figure shows the log-marginal-likelihood for different choices of the kernel&amp;rsquo;s hyperparameters, highlighting the two choices of the hyperparameters used in the first figure by black dots.</source>
          <target state="translated">2番目の図は、カーネルのハイパーパラメーターのさまざまな選択肢の対数限界尤度を示しており、最初の図で使用されているハイパーパラメーターの2つの選択肢を黒い点で強調しています。</target>
        </trans-unit>
        <trans-unit id="69a6b8988bb4a9da22ae7a8129c60d4bfa19ab9d" translate="yes" xml:space="preserve">
          <source>The second loader is typically used for the face verification task: each sample is a pair of two picture belonging or not to the same person:</source>
          <target state="translated">第2のローダは、典型的には顔認証タスクに使用されます:各サンプルは、同一人物に属するか否かを問わず、2枚の画像のペアです。</target>
        </trans-unit>
        <trans-unit id="80a536b57ba05b05dbab01bd549517eccd6caab7" translate="yes" xml:space="preserve">
          <source>The second model is a Bayesian Gaussian Mixture Model with a Dirichlet process prior fit with variational inference. The low value of the concentration prior makes the model favor a lower number of active components. This models &amp;ldquo;decides&amp;rdquo; to focus its modeling power on the big picture of the structure of the dataset: groups of points with alternating directions modeled by non-diagonal covariance matrices. Those alternating directions roughly capture the alternating nature of the original sine signal.</source>
          <target state="translated">2番目のモデルは、変分推論で事前にディリクレプロセスで近似したベイジアンガウス混合モデルです。以前の濃度の値が低いと、モデルはより少ない数のアクティブなコンポーネントを優先します。このモデルは、データセットの構造の全体像（非対角共分散行列でモデル化された方向が交互に変化する点のグループ）にモデリング力を集中させるように「決定」します。これらの交互の方向は、元のサイン信号の交互の性質を大まかに捉えます。</target>
        </trans-unit>
        <trans-unit id="ce63f012cd3f9d5ba6717aef4dc1ee5e80e67c9d" translate="yes" xml:space="preserve">
          <source>The second one has a smaller noise level and shorter length scale, which explains most of the variation by the noise-free functional relationship. The second model has a higher likelihood; however, depending on the initial value for the hyperparameters, the gradient-based optimization might also converge to the high-noise solution. It is thus important to repeat the optimization several times for different initializations.</source>
          <target state="translated">2番目のモデルは、ノイズレベルが小さく、長さのスケールが短いため、ノイズフリーの関数関係による変動のほとんどを説明することができます。2番目のモデルの方が尤度が高いですが、ハイパーパラメータの初期値によっては、勾配ベースの最適化も高ノイズ解に収束する可能性があります。そのため、異なる初期化に対して数回の最適化を繰り返すことが重要です。</target>
        </trans-unit>
        <trans-unit id="d5dcf1b11e556ea9365934aae9c750b0508ecb89" translate="yes" xml:space="preserve">
          <source>The second plot demonstrate one single run of the &lt;code&gt;MiniBatchKMeans&lt;/code&gt; estimator using a &lt;code&gt;init=&quot;random&quot;&lt;/code&gt; and &lt;code&gt;n_init=1&lt;/code&gt;. This run leads to a bad convergence (local optimum) with estimated centers stuck between ground truth clusters.</source>
          <target state="translated">2番目のプロットは、 &lt;code&gt;init=&quot;random&quot;&lt;/code&gt; と &lt;code&gt;n_init=1&lt;/code&gt; を使用した &lt;code&gt;MiniBatchKMeans&lt;/code&gt; 推定器の1回の実行を示しています。この実行により、グランドトゥルースクラスター間にスタックされた推定中心で、収束が悪くなります（局所最適）。</target>
        </trans-unit>
        <trans-unit id="41338f596d871499307d96f69f697b91afdfa1c4" translate="yes" xml:space="preserve">
          <source>The second plot is a heatmap of the classifier&amp;rsquo;s cross-validation accuracy as a function of &lt;code&gt;C&lt;/code&gt; and &lt;code&gt;gamma&lt;/code&gt;. For this example we explore a relatively large grid for illustration purposes. In practice, a logarithmic grid from \(10^{-3}\) to \(10^3\) is usually sufficient. If the best parameters lie on the boundaries of the grid, it can be extended in that direction in a subsequent search.</source>
          <target state="translated">2番目のプロットは、 &lt;code&gt;C&lt;/code&gt; および &lt;code&gt;gamma&lt;/code&gt; の関数としての分類器の交差検定精度のヒートマップです。この例では、説明のために比較的大きなグリッドを調べます。実際には、通常、\（10 ^ {-3} \）から\（10 ^ 3 \）までの対数グリッドで十分です。最適なパラメータがグリッドの境界にある場合、後続の検索でその方向に拡張できます。</target>
        </trans-unit>
        <trans-unit id="57fb8e025f7ec94b6d1e30748cbff5561f1e9901" translate="yes" xml:space="preserve">
          <source>The second plot shows that an increase of the admissible distortion &lt;code&gt;eps&lt;/code&gt; allows to reduce drastically the minimal number of dimensions &lt;code&gt;n_components&lt;/code&gt; for a given number of samples &lt;code&gt;n_samples&lt;/code&gt;</source>
          <target state="translated">2番目のプロットは、許容可能な歪み &lt;code&gt;eps&lt;/code&gt; の増加により、特定のサンプル数 &lt;code&gt;n_components&lt;/code&gt; 最小次元数n_componentsを大幅に削減できることを示しています &lt;code&gt;n_samples&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="9c6c0ec72e2e2da2def0a20a979b162b66b4f25f" translate="yes" xml:space="preserve">
          <source>The second plot visualized the decision surfaces of the RBF kernel SVM and the linear SVM with approximate kernel maps. The plot shows decision surfaces of the classifiers projected onto the first two principal components of the data. This visualization should be taken with a grain of salt since it is just an interesting slice through the decision surface in 64 dimensions. In particular note that a datapoint (represented as a dot) does not necessarily be classified into the region it is lying in, since it will not lie on the plane that the first two principal components span.</source>
          <target state="translated">第2のプロットは、RBFカーネルSVMと近似カーネルマップを持つ線形SVMの決定面を可視化したものである。このプロットは、データの最初の2つの主成分に投影された分類器の決定面を示しています。この可視化は、64次元での決定面の興味深いスライスなので、大目に見てください。特に、データポイント(ドットで表される)は、最初の2つの主成分が横たわる平面上にはないので、必ずしもそのデータポイントが横たわっている領域に分類されるとは限らないことに注意してください。</target>
        </trans-unit>
        <trans-unit id="a7aa029b23a556153d8e76aebf5393705fc5a931" translate="yes" xml:space="preserve">
          <source>The second use case is to build a completely custom scorer object from a simple python function using &lt;a href=&quot;generated/sklearn.metrics.make_scorer#sklearn.metrics.make_scorer&quot;&gt;&lt;code&gt;make_scorer&lt;/code&gt;&lt;/a&gt;, which can take several parameters:</source>
          <target state="translated">2番目の使用例は、&lt;a href=&quot;generated/sklearn.metrics.make_scorer#sklearn.metrics.make_scorer&quot;&gt; &lt;code&gt;make_scorer&lt;/code&gt; &lt;/a&gt;を使用して単純なpython関数から完全にカスタムのスコアラーオブジェクトを構築することです。これは、いくつかのパラメーターを取ることができます。</target>
        </trans-unit>
        <trans-unit id="c00cf63770db0bb2225569e421e3076426129a55" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator for adding small noise to continuous variables in order to remove repeated values. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">繰り返し値を削除するために連続変数に小さなノイズを追加するための疑似乱数ジェネレータのシード。intの場合、random_stateは乱数ジェネレータによって使用されるシードです。RandomStateインスタンスの場合、random_stateは乱数ジェネレータです。Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。</target>
        </trans-unit>
        <trans-unit id="06cdf003d2aac9179d5700a91f249ff0a94d6f8e" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator that selects a random feature to update. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;selection&lt;/code&gt; == &amp;lsquo;random&amp;rsquo;</source>
          <target state="translated">更新するランダムな機能を選択する疑似乱数ジェネレータのシード。intの場合、random_stateは乱数ジェネレータによって使用されるシードです。RandomStateインスタンスの場合、random_stateは乱数ジェネレータです。Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。 &lt;code&gt;selection&lt;/code&gt; == 'random'のときに使用されます</target>
        </trans-unit>
        <trans-unit id="0bc2f2e46810cbdc913e04d68694f60b2b83e0d8" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator that selects a random feature to update. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;selection&lt;/code&gt; == &amp;lsquo;random&amp;rsquo;.</source>
          <target state="translated">更新するランダムな機能を選択する疑似乱数ジェネレータのシード。intの場合、random_stateは乱数ジェネレータによって使用されるシードです。RandomStateインスタンスの場合、random_stateは乱数ジェネレータです。Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。 &lt;code&gt;selection&lt;/code&gt; == 'random'の場合に使用されます。</target>
        </trans-unit>
        <trans-unit id="c9b5b4205f687d2590f4c80ac04919e87b2e36db" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use when shuffling the data for the dual coordinate descent (if &lt;code&gt;dual=True&lt;/code&gt;). When &lt;code&gt;dual=False&lt;/code&gt; the underlying implementation of &lt;a href=&quot;#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; is not random and &lt;code&gt;random_state&lt;/code&gt; has no effect on the results. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">二重座標降下のデータをシャッフルするときに使用する疑似乱数ジェネレータのシード（ &lt;code&gt;dual=True&lt;/code&gt; の場合）。 &lt;code&gt;dual=False&lt;/code&gt; の場合、&lt;a href=&quot;#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;の基本的な実装はランダムではなく、 &lt;code&gt;random_state&lt;/code&gt; は結果に影響を与えません。 intの場合、random_stateは乱数ジェネレータによって使用されるシードです。 RandomStateインスタンスの場合、random_stateは乱数ジェネレータです。 Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。</target>
        </trans-unit>
        <trans-unit id="f24e839d9f0c07b405eb078c6f50d58ca84b7fe6" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use when shuffling the data. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">データをシャッフルするときに使用する疑似乱数ジェネレータのシード。intの場合、random_stateは乱数ジェネレータによって使用されるシードです。RandomStateインスタンスの場合、random_stateは乱数ジェネレータです。Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。</target>
        </trans-unit>
        <trans-unit id="58318e33f8140e9303ba15931c5e93b11c5df63e" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use when shuffling the data. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;solver&lt;/code&gt; == &amp;lsquo;sag&amp;rsquo; or &amp;lsquo;liblinear&amp;rsquo;.</source>
          <target state="translated">データをシャッフルするときに使用する疑似乱数ジェネレータのシード。intの場合、random_stateは乱数ジェネレータによって使用されるシードです。RandomStateインスタンスの場合、random_stateは乱数ジェネレータです。Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。 &lt;code&gt;solver&lt;/code&gt; == 'sag'または 'liblinear'の場合に使用されます。</target>
        </trans-unit>
        <trans-unit id="1e64edd7c479eb06717df1495b4d044bfaa961d0" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use when shuffling the data. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;solver&lt;/code&gt; == &amp;lsquo;sag&amp;rsquo;.</source>
          <target state="translated">データをシャッフルするときに使用する疑似乱数ジェネレータのシード。intの場合、random_stateは乱数ジェネレータによって使用されるシードです。RandomStateインスタンスの場合、random_stateは乱数ジェネレータです。Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。 &lt;code&gt;solver&lt;/code&gt; == 'サグ'のときに使用されます。</target>
        </trans-unit>
        <trans-unit id="db7139d208134a3c43811feaafd4124e4ee369a8" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator used when shuffling the data for probability estimates. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">確率推定のためにデータをシャッフルするときに使用される疑似乱数ジェネレータのシード。intの場合、random_stateは乱数ジェネレータによって使用されるシードです。RandomStateインスタンスの場合、random_stateは乱数ジェネレータです。Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。</target>
        </trans-unit>
        <trans-unit id="f1665069fc9c4c9d9dabb36f1931e17db4945528" translate="yes" xml:space="preserve">
          <source>The set of F values.</source>
          <target state="translated">Fの値の集合。</target>
        </trans-unit>
        <trans-unit id="d03a062cac2973dfcc9173b5fc93f7520db21987" translate="yes" xml:space="preserve">
          <source>The set of labels can be different for each output variable. For instance, a sample could be assigned &amp;ldquo;pear&amp;rdquo; for an output variable that takes possible values in a finite set of species such as &amp;ldquo;pear&amp;rdquo;, &amp;ldquo;apple&amp;rdquo;; and &amp;ldquo;blue&amp;rdquo; or &amp;ldquo;green&amp;rdquo; for a second output variable that takes possible values in a finite set of colors such as &amp;ldquo;green&amp;rdquo;, &amp;ldquo;red&amp;rdquo;, &amp;ldquo;blue&amp;rdquo;, &amp;ldquo;yellow&amp;rdquo;&amp;hellip;</source>
          <target state="translated">ラベルのセットは、出力変数ごとに異なる場合があります。たとえば、サンプルには、「pear」、「apple」などの種の有限セットで可能な値を取る出力変数の「pear」を割り当てることができます。そして「青」または「緑」は、「緑」、「赤」、「青」、「黄色」などの色の有限セットで可能な値を取る2番目の出力変数です。</target>
        </trans-unit>
        <trans-unit id="143c46009e14705cc3449ca19bc2f349662d5283" translate="yes" xml:space="preserve">
          <source>The set of labels for each sample such that &lt;code&gt;y[i]&lt;/code&gt; consists of &lt;code&gt;classes_[j]&lt;/code&gt; for each &lt;code&gt;yt[i, j] == 1&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;y[i]&lt;/code&gt; &lt;code&gt;classes_[j]&lt;/code&gt; が各 &lt;code&gt;yt[i, j] == 1&lt;/code&gt; のclasses_ [j]で構成されるような各サンプルのラベルのセット。</target>
        </trans-unit>
        <trans-unit id="2d10d69a1c09af3eb9fb75910b7cd4ebd942ac2e" translate="yes" xml:space="preserve">
          <source>The set of labels to include when &lt;code&gt;average != 'binary'&lt;/code&gt;, and their order if &lt;code&gt;average is None&lt;/code&gt;. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class, while labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in &lt;code&gt;y_true&lt;/code&gt; and &lt;code&gt;y_pred&lt;/code&gt; are used in sorted order.</source>
          <target state="translated">&lt;code&gt;average != 'binary'&lt;/code&gt; ときに含めるラベルのセットと、 &lt;code&gt;average is None&lt;/code&gt; 場合の順序。データに存在するラベルは除外できます。たとえば、過半数の否定的なクラスを無視してマルチクラス平均を計算できますが、データに存在しないラベルは、マクロ平均のコンポーネントが0になります。マルチラベルターゲットの場合、ラベルは列インデックスです。デフォルトでは、 &lt;code&gt;y_true&lt;/code&gt; と &lt;code&gt;y_pred&lt;/code&gt; のすべてのラベルがソートされた順序で使用されます。</target>
        </trans-unit>
        <trans-unit id="fc018ce1a1ad3dcbb813a9b943d602142a80bfe5" translate="yes" xml:space="preserve">
          <source>The set of p-values.</source>
          <target state="translated">p値の集合。</target>
        </trans-unit>
        <trans-unit id="ae935a83c454932c18aabaf5527bf9d40a05884a" translate="yes" xml:space="preserve">
          <source>The set of regressors that will be tested sequentially.</source>
          <target state="translated">順次テストされるレグレッサーのセット。</target>
        </trans-unit>
        <trans-unit id="418cd7fd5c32b01bfeb33989472034a267c4e833" translate="yes" xml:space="preserve">
          <source>The shape (Nx, Ny) array of pairwise distances between points in X and Y.</source>
          <target state="translated">XとYの点間の対になる距離の形状(Nx,Ny)の配列。</target>
        </trans-unit>
        <trans-unit id="be8e19650b3e56af8959e9d1bdb9305ca252ca97" translate="yes" xml:space="preserve">
          <source>The shape of &lt;code&gt;dual_coef_&lt;/code&gt; is &lt;code&gt;[n_class-1, n_SV]&lt;/code&gt; with a somewhat hard to grasp layout. The columns correspond to the support vectors involved in any of the &lt;code&gt;n_class * (n_class - 1) / 2&lt;/code&gt; &amp;ldquo;one-vs-one&amp;rdquo; classifiers. Each of the support vectors is used in &lt;code&gt;n_class - 1&lt;/code&gt; classifiers. The &lt;code&gt;n_class - 1&lt;/code&gt; entries in each row correspond to the dual coefficients for these classifiers.</source>
          <target state="translated">&lt;code&gt;dual_coef_&lt;/code&gt; の形状は &lt;code&gt;[n_class-1, n_SV]&lt;/code&gt; で、レイアウトを把握するのがやや難しいです。列は、 &lt;code&gt;n_class * (n_class - 1) / 2&lt;/code&gt; 「1対1」の分類子のいずれかに含まれるサポートベクターに対応しています。各サポートベクターは、 &lt;code&gt;n_class - 1&lt;/code&gt; 分類器で使用されます。 &lt;code&gt;n_class - 1&lt;/code&gt; これらの分類のための二重係数に各行対応のエントリ。</target>
        </trans-unit>
        <trans-unit id="79705c107a83df8d45ef47d82375b7c707f4d5ae" translate="yes" xml:space="preserve">
          <source>The shape of the result.</source>
          <target state="translated">結果の形。</target>
        </trans-unit>
        <trans-unit id="770a88634d7b4928209cc52a1f8aea4bce68a8e8" translate="yes" xml:space="preserve">
          <source>The shift offset allows a zero threshold for being an outlier. Only available for novelty detection (when novelty is set to True). The argument X is supposed to contain &lt;em&gt;new data&lt;/em&gt;: if X contains a point from training, it considers the later in its own neighborhood. Also, the samples in X are not considered in the neighborhood of any point.</source>
          <target state="translated">シフトオフセットにより、外れ値となるしきい値をゼロにすることができます。新規性検出にのみ使用できます（新規性がTrueに設定されている場合）。引数Xには&lt;em&gt;新しいデータ&lt;/em&gt;が含まれることになっています。Xにトレーニングからのポイントが含まれている場合、Xは自身の近傍の後者を考慮します。また、Xのサンプルは、どの点の近傍でも考慮されません。</target>
        </trans-unit>
        <trans-unit id="fd58d8b5ba5725b529e01c853ef09676528984ae" translate="yes" xml:space="preserve">
          <source>The shifted opposite of the Local Outlier Factor of each input samples. The lower, the more abnormal. Negative scores represent outliers, positive scores represent inliers.</source>
          <target state="translated">各入力サンプルの局所外れ値係数の反対側にシフトした値。低いほど異常であることを示す。否定的なスコアは外れ値を表し、肯定的なスコアはインライアを表す。</target>
        </trans-unit>
        <trans-unit id="13c37fa5739748416ca3f9521f51dec2de533ef6" translate="yes" xml:space="preserve">
          <source>The similarity of two sets of biclusters.</source>
          <target state="translated">バイクラスターの2つのセットの類似性。</target>
        </trans-unit>
        <trans-unit id="7becf18fe4f5e5f9bf982cb2425cc3a834e0c404" translate="yes" xml:space="preserve">
          <source>The simplest metric &lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt;&lt;code&gt;MDS&lt;/code&gt;&lt;/a&gt; model, called &lt;em&gt;absolute MDS&lt;/em&gt;, disparities are defined by \(\hat{d}_{ij} = S_{ij}\). With absolute MDS, the value \(S_{ij}\) should then correspond exactly to the distance between point \(i\) and \(j\) in the embedding point.</source>
          <target state="translated">&lt;em&gt;絶対MDS&lt;/em&gt;と呼ばれる最も単純なメトリック&lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt; &lt;code&gt;MDS&lt;/code&gt; &lt;/a&gt;モデルで、視差は\（\ hat {d} _ {ij} = S_ {ij} \）によって定義されます。絶対MDSでは、値\（S_ {ij} \）は、埋め込みポイントのポイント\（i \）と\（j \）の間の距離に正確に対応する必要があります。&lt;em&gt;&lt;/em&gt;</target>
        </trans-unit>
        <trans-unit id="1d805d3b9d3166d024fed65dbe7dc1ddecd0fb40" translate="yes" xml:space="preserve">
          <source>The simplest possible classifier is the &lt;a href=&quot;https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm&quot;&gt;nearest neighbor&lt;/a&gt;: given a new observation &lt;code&gt;X_test&lt;/code&gt;, find in the training set (i.e. the data used to train the estimator) the observation with the closest feature vector. (Please see the &lt;a href=&quot;../../modules/neighbors#neighbors&quot;&gt;Nearest Neighbors section&lt;/a&gt; of the online Scikit-learn documentation for more information about this type of classifier.)</source>
          <target state="translated">最も簡単な分類子は&lt;a href=&quot;https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm&quot;&gt;最近傍&lt;/a&gt;です。新しい観測値 &lt;code&gt;X_test&lt;/code&gt; が与えられた場合、トレーニングセット（つまり、推定器の学習に使用されるデータ）で、最も近い特徴ベクトルを使用して観測値を見つけます。（このタイプの分類器の詳細については、Scikit-learnのオンラインドキュメントの「&lt;a href=&quot;../../modules/neighbors#neighbors&quot;&gt;Nearest Neighbors」セクション&lt;/a&gt;を参照してください。）</target>
        </trans-unit>
        <trans-unit id="1fac5c5ae1360f0509b6fb6c9bee7a89fd16eea5" translate="yes" xml:space="preserve">
          <source>The simplest way to accomplish this dimensionality reduction is by taking a random projection of the data. Though this allows some degree of visualization of the data structure, the randomness of the choice leaves much to be desired. In a random projection, it is likely that the more interesting structure within the data will be lost.</source>
          <target state="translated">この次元削減を達成する最も単純な方法は、データのランダムな投影を取ることです。これはデータ構造のある程度の可視化を可能にしますが、選択のランダム性は望まれる多くのことを残します。ランダム投影では、データ内のより興味深い構造が失われる可能性があります。</target>
        </trans-unit>
        <trans-unit id="a8c7e68caf35057fba3785bb16a14da344816784" translate="yes" xml:space="preserve">
          <source>The simplest way to use cross-validation is to call the &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; helper function on the estimator and the dataset.</source>
          <target state="translated">交差検証を使用する最も簡単な方法は、推定器とデータセットで&lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt;ヘルパー関数を呼び出すことです。</target>
        </trans-unit>
        <trans-unit id="6b61610397fd4bcbb9699a5ba6221c6a9dfd8212" translate="yes" xml:space="preserve">
          <source>The singular value decomposition, \(A_n = U \Sigma V^\top\), provides the partitions of the rows and columns of \(A\). A subset of the left singular vectors gives the row partitions, and a subset of the right singular vectors gives the column partitions.</source>
          <target state="translated">特異値分解は、「\(A_n=U \(A)」の行と列の分割を与える。)左の特異値の部分集合が行の分割を、右の特異値の部分集合が列の分割を与える。</target>
        </trans-unit>
        <trans-unit id="f785df3fd21ed481c16151f3b91e613616eec382" translate="yes" xml:space="preserve">
          <source>The singular values corresponding to each of the selected components. The singular values are equal to the 2-norms of the &lt;code&gt;n_components&lt;/code&gt; variables in the lower-dimensional space.</source>
          <target state="translated">選択した各コンポーネントに対応する特異値。特異値は、低次元空間の &lt;code&gt;n_components&lt;/code&gt; 変数の2ノルムに等しくなります。</target>
        </trans-unit>
        <trans-unit id="fda37f3c2ceb4ddf1d93860ac5de12a5ffc950e8" translate="yes" xml:space="preserve">
          <source>The size of &lt;code&gt;grid_scores_&lt;/code&gt; is equal to &lt;code&gt;ceil((n_features - min_features_to_select) / step) + 1&lt;/code&gt;, where step is the number of features removed at each iteration.</source>
          <target state="translated">&lt;code&gt;grid_scores_&lt;/code&gt; のサイズは &lt;code&gt;ceil((n_features - min_features_to_select) / step) + 1&lt;/code&gt; に等しく、stepは各反復で削除されるフィーチャの数です。</target>
        </trans-unit>
        <trans-unit id="9867bd15789365ba5d58a7dbdcd5815e87ddbf26" translate="yes" xml:space="preserve">
          <source>The size of the model with the default parameters is \(O( M * N * log (N) )\), where \(M\) is the number of trees and \(N\) is the number of samples. In order to reduce the size of the model, you can change these parameters: &lt;code&gt;min_samples_split&lt;/code&gt;, &lt;code&gt;max_leaf_nodes&lt;/code&gt;, &lt;code&gt;max_depth&lt;/code&gt; and &lt;code&gt;min_samples_leaf&lt;/code&gt;.</source>
          <target state="translated">既定のパラメーターを使用したモデルのサイズは\（O（M * N * log（N））\）です。ここで、\（M \）はツリーの数、\（N \）はサンプルの数です。モデルのサイズを小さくするために、これらのパラメーターを変更できます： &lt;code&gt;min_samples_split&lt;/code&gt; 、 &lt;code&gt;max_leaf_nodes&lt;/code&gt; 、 &lt;code&gt;max_depth&lt;/code&gt; 、および &lt;code&gt;min_samples_leaf&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="1f17f4e69474d346e8934f03ff8be8d8add88c9f" translate="yes" xml:space="preserve">
          <source>The size of the random matrix to generate.</source>
          <target state="translated">生成するランダム行列のサイズ。</target>
        </trans-unit>
        <trans-unit id="b31452681b2b7098990045ccc11256312f76473c" translate="yes" xml:space="preserve">
          <source>The size of the regression tree base learners defines the level of variable interactions that can be captured by the gradient boosting model. In general, a tree of depth &lt;code&gt;h&lt;/code&gt; can capture interactions of order &lt;code&gt;h&lt;/code&gt; . There are two ways in which the size of the individual regression trees can be controlled.</source>
          <target state="translated">回帰木の基本学習器のサイズは、勾配ブースティングモデルで取得できる変数の相互作用のレベルを定義します。一般に、深さ &lt;code&gt;h&lt;/code&gt; のツリーは、次数 &lt;code&gt;h&lt;/code&gt; の相互作用をキャプチャできます。個々の回帰ツリーのサイズを制御する方法は2つあります。</target>
        </trans-unit>
        <trans-unit id="eb0fc3f910e5dcbfcfe6af4540bbfd0a452530ef" translate="yes" xml:space="preserve">
          <source>The size of the sample to use when computing the Silhouette Coefficient on a random subset of the data. If &lt;code&gt;sample_size is None&lt;/code&gt;, no sampling is used.</source>
          <target state="translated">データのランダムなサブセットでシルエット係数を計算するときに使用するサンプルのサイズ。 &lt;code&gt;sample_size is None&lt;/code&gt; 場合、サンプリングは使用されません。</target>
        </trans-unit>
        <trans-unit id="b3605bfba06cab15c1207c4102bf5bbb9eee0a53" translate="yes" xml:space="preserve">
          <source>The size of the set to sample from.</source>
          <target state="translated">サンプリングするセットのサイズ。</target>
        </trans-unit>
        <trans-unit id="d5051ed3b169b87ed97be7e20bda0cdf9d82ecae" translate="yes" xml:space="preserve">
          <source>The size, the distance and the shape of clusters may vary upon initialization, perplexity values and does not always convey a meaning.</source>
          <target state="translated">クラスターの大きさ、距離、形状は、初期化時に変化する可能性があり、錯乱値であり、必ずしも意味を伝えるものではありません。</target>
        </trans-unit>
        <trans-unit id="6830208b16eb851287384293d35fab1e64fb8f9a" translate="yes" xml:space="preserve">
          <source>The skewed chi squared kernel is given by:</source>
          <target state="translated">歪んだカイ二乗カーネルは次式で与えられる。</target>
        </trans-unit>
        <trans-unit id="bdf0bd9101d435249fe017ac2196fd5049ca1688" translate="yes" xml:space="preserve">
          <source>The smoothing priors \(\alpha \ge 0\) accounts for features not present in the learning samples and prevents zero probabilities in further computations. Setting \(\alpha = 1\) is called Laplace smoothing, while \(\alpha &amp;lt; 1\) is called Lidstone smoothing.</source>
          <target state="translated">事前平滑化\（\ alpha \ ge 0 \）は、学習サンプルに存在しない特徴を考慮に入れ、以降の計算で確率がゼロになるのを防ぎます。\（\ alpha = 1 \）の設定はラプラス平滑化と呼ばれ、\（\ alpha &amp;lt;1 \）はリッドストーン平滑化と呼ばれます。</target>
        </trans-unit>
        <trans-unit id="4842d9fce84143997f4f4321a8717acf3b670822" translate="yes" xml:space="preserve">
          <source>The solver &amp;ldquo;liblinear&amp;rdquo; uses a coordinate descent (CD) algorithm, and relies on the excellent C++ &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;LIBLINEAR library&lt;/a&gt;, which is shipped with scikit-learn. However, the CD algorithm implemented in liblinear cannot learn a true multinomial (multiclass) model; instead, the optimization problem is decomposed in a &amp;ldquo;one-vs-rest&amp;rdquo; fashion so separate binary classifiers are trained for all classes. This happens under the hood, so &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; instances using this solver behave as multiclass classifiers. For L1 penalization &lt;a href=&quot;generated/sklearn.svm.l1_min_c#sklearn.svm.l1_min_c&quot;&gt;&lt;code&gt;sklearn.svm.l1_min_c&lt;/code&gt;&lt;/a&gt; allows to calculate the lower bound for C in order to get a non &amp;ldquo;null&amp;rdquo; (all feature weights to zero) model.</source>
          <target state="translated">ソルバー「liblinear」は座標降下（CD）アルゴリズムを使用し、scikit-learnに付属する優れたC ++ &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;LIBLINEARライブラリ&lt;/a&gt;に依存しています。ただし、liblinearで実装されたCDアルゴリズムは、真の多項式（マルチクラス）モデルを学習できません。代わりに、最適化問題は「1対レスト」の方法で分解されるため、個別のバイナリ分類子がすべてのクラスに対してトレーニングされます。これは内部で発生するため、このソルバーを使用する&lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt;インスタンスはマルチクラス分類子として動作します。 L1 &lt;a href=&quot;generated/sklearn.svm.l1_min_c#sklearn.svm.l1_min_c&quot;&gt; &lt;code&gt;sklearn.svm.l1_min_c&lt;/code&gt; &lt;/a&gt;場合、sklearn.svm.l1_min_cは、Cの下限を計算して、非「null」（すべての機能の重みがゼロ）モデルを取得できます。</target>
        </trans-unit>
        <trans-unit id="ed7150245f4b6e455142137cba3f8af716071c7a" translate="yes" xml:space="preserve">
          <source>The solver for weight optimization.</source>
          <target state="translated">重量最適化のためのソルバーです。</target>
        </trans-unit>
        <trans-unit id="7be0c6e2677e69b218f8a95f391cfbcac99c36a1" translate="yes" xml:space="preserve">
          <source>The solvers implemented in the class &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; are &amp;ldquo;liblinear&amp;rdquo;, &amp;ldquo;newton-cg&amp;rdquo;, &amp;ldquo;lbfgs&amp;rdquo;, &amp;ldquo;sag&amp;rdquo; and &amp;ldquo;saga&amp;rdquo;:</source>
          <target state="translated">クラス&lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; に&lt;/a&gt;実装されているソルバーは、「liblinear」、「newton-cg」、「lbfgs」、「sag」、「saga」です。</target>
        </trans-unit>
        <trans-unit id="2562620e10231c5093b1e84475f4d077e3e41917" translate="yes" xml:space="preserve">
          <source>The sought maximum memory for temporary distance matrix chunks. When None (default), the value of &lt;code&gt;sklearn.get_config()['working_memory']&lt;/code&gt; is used.</source>
          <target state="translated">一時的な距離行列のチャンクに求められる最大メモリ。なし（デフォルト）の場合、 &lt;code&gt;sklearn.get_config()['working_memory']&lt;/code&gt; の値が使用されます。</target>
        </trans-unit>
        <trans-unit id="d35f1b775d7d16bbabc524099db10f7048897e6d" translate="yes" xml:space="preserve">
          <source>The source can also be found &lt;a href=&quot;https://github.com/scikit-learn/scikit-learn/tree/master/doc/tutorial/text_analytics&quot;&gt;on Github&lt;/a&gt;.</source>
          <target state="translated">ソースは&lt;a href=&quot;https://github.com/scikit-learn/scikit-learn/tree/master/doc/tutorial/text_analytics&quot;&gt;Githubに&lt;/a&gt;もあります。</target>
        </trans-unit>
        <trans-unit id="351456aed7b7d0fc1d0034839135c70dde192f05" translate="yes" xml:space="preserve">
          <source>The source of this tutorial can be found within your scikit-learn folder:</source>
          <target state="translated">このチュートリアルのソースは scikit-learn フォルダ内にあります。</target>
        </trans-unit>
        <trans-unit id="aa3459bc05f5ce02810c5dff6ad6cfbeb1ca9a04" translate="yes" xml:space="preserve">
          <source>The spacing between points of the grid, in degrees</source>
          <target state="translated">グリッドの点間の間隔を度単位で指定します。</target>
        </trans-unit>
        <trans-unit id="464028094b69433f3db44d7a263916deccf86cb6" translate="yes" xml:space="preserve">
          <source>The sparse code factor in the matrix factorization.</source>
          <target state="translated">行列因数分解における疎な符号因子。</target>
        </trans-unit>
        <trans-unit id="f9d9c0a7d6ff3b6246478d922234360e15ed6ab9" translate="yes" xml:space="preserve">
          <source>The sparse code such that each column of this matrix has exactly n_nonzero_coefs non-zero items (X).</source>
          <target state="translated">この行列の各列が正確に n_nonzero_coefs 非ゼロ項目(X)を持つような疎なコード。</target>
        </trans-unit>
        <trans-unit id="ee7d500960f94d10a937d21e436dcc2adbbc9a2a" translate="yes" xml:space="preserve">
          <source>The sparse codes</source>
          <target state="translated">疎なコード</target>
        </trans-unit>
        <trans-unit id="0e1ce62165b4e0dbd0b6b1199e83c88e1c88959d" translate="yes" xml:space="preserve">
          <source>The sparse implementation produces slightly different results than the dense implementation due to a shrunk learning rate for the intercept.</source>
          <target state="translated">疎な実装では,切片の学習率が縮小されているため,密な実装とはわずかに異なる結果が得られます.</target>
        </trans-unit>
        <trans-unit id="e4890de0c56a0e7645deea9088355f84adac629f" translate="yes" xml:space="preserve">
          <source>The sparse vector</source>
          <target state="translated">疎なベクトル</target>
        </trans-unit>
        <trans-unit id="df0fd56f5b70016c4466d03cfc8859f7c3ea7663" translate="yes" xml:space="preserve">
          <source>The sparsity is actually imposed on the cholesky factor of the matrix. Thus alpha does not translate directly into the filling fraction of the matrix itself.</source>
          <target state="translated">スパーシティは実際には行列のコレスキー係数に課せられています.したがって,アルファは行列自体の充填率には直接変換されません.</target>
        </trans-unit>
        <trans-unit id="56d99fb198a4da207bde97c2ceeeb3653451f496" translate="yes" xml:space="preserve">
          <source>The sparsity-inducing \(\ell_1\) norm also prevents learning components from noise when few training samples are available. The degree of penalization (and thus sparsity) can be adjusted through the hyperparameter &lt;code&gt;alpha&lt;/code&gt;. Small values lead to a gently regularized factorization, while larger values shrink many coefficients to zero.</source>
          <target state="translated">スパース性を誘発する\（\ ell_1 \）ノルムはまた、利用可能なトレーニングサンプルが少ない場合にノイズからコンポーネントを学習することを防ぎます。ペナルティの程度（およびスパース性）は、ハイパーパラメーター &lt;code&gt;alpha&lt;/code&gt; を使用して調整できます。値が小さいと緩やかに正則化された因数分解が行われ、値が大きいと多くの係数がゼロに縮小されます。</target>
        </trans-unit>
        <trans-unit id="1e3f3420100e85d456b50524681a1e1c7bbc338a" translate="yes" xml:space="preserve">
          <source>The split code for a single sample has length &lt;code&gt;2 * n_components&lt;/code&gt; and is constructed using the following rule: First, the regular code of length &lt;code&gt;n_components&lt;/code&gt; is computed. Then, the first &lt;code&gt;n_components&lt;/code&gt; entries of the &lt;code&gt;split_code&lt;/code&gt; are filled with the positive part of the regular code vector. The second half of the split code is filled with the negative part of the code vector, only with a positive sign. Therefore, the split_code is non-negative.</source>
          <target state="translated">単一のサンプルの分割コードの長さは &lt;code&gt;2 * n_components&lt;/code&gt; で、次のルールを使用して作成されます。最初に、長さ &lt;code&gt;n_components&lt;/code&gt; の通常のコードが計算されます。そして、第1 &lt;code&gt;n_components&lt;/code&gt; 用のエントリ &lt;code&gt;split_code&lt;/code&gt; は、通常のコードベクトルの正の部分が充填されています。分割コードの後半は、コードベクトルの負の部分で満たされ、正の符号のみが含まれます。したがって、split_codeは負ではありません。</target>
        </trans-unit>
        <trans-unit id="619ea10ad996c929a97e389d99c020b2211157d4" translate="yes" xml:space="preserve">
          <source>The standard LLE algorithm comprises three stages:</source>
          <target state="translated">標準的なLLEアルゴリズムは3つのステージから構成されています。</target>
        </trans-unit>
        <trans-unit id="a5bdc246aecc7e3bec9121428d3f9c450814299f" translate="yes" xml:space="preserve">
          <source>The standard deviation of the clusters.</source>
          <target state="translated">クラスターの標準偏差。</target>
        </trans-unit>
        <trans-unit id="b61516300476f785958503bfbc63e8e96de11d18" translate="yes" xml:space="preserve">
          <source>The standard deviation of the gaussian noise applied to the output.</source>
          <target state="translated">出力に適用されるガウスノイズの標準偏差。</target>
        </trans-unit>
        <trans-unit id="e5c05e9131e22b6718043e99a2bae1b92b538981" translate="yes" xml:space="preserve">
          <source>The standard deviation of the gaussian noise.</source>
          <target state="translated">ガウスノイズの標準偏差。</target>
        </trans-unit>
        <trans-unit id="3689d32cff3e62dded279fb6b657e94942cdc7dc" translate="yes" xml:space="preserve">
          <source>The stepwise interpolating function that covers the input domain &lt;code&gt;X&lt;/code&gt;.</source>
          <target state="translated">入力ドメイン &lt;code&gt;X&lt;/code&gt; をカバーする段階的な内挿関数。</target>
        </trans-unit>
        <trans-unit id="4d8cf8d82bcb0bc25727f81a7a8d626fa0a70639" translate="yes" xml:space="preserve">
          <source>The stopping criterion. If it is not None, the iterations will stop when (loss &amp;gt; previous_loss - tol). Defaults to None. Defaults to 1e-3 from 0.21.</source>
          <target state="translated">停止基準。Noneでない場合、（loss&amp;gt; previous_loss-tol）のときに反復が停止します。デフォルトはNoneです。デフォルトは0.21から1e-3です。</target>
        </trans-unit>
        <trans-unit id="ba1aa4a75fb51f2479b447aa001f5e6386a1f799" translate="yes" xml:space="preserve">
          <source>The strategy to use to assign labels in the embedding space. There are two ways to assign labels after the laplacian embedding. k-means can be applied and is a popular choice. But it can also be sensitive to initialization. Discretization is another approach which is less sensitive to random initialization.</source>
          <target state="translated">埋め込み空間でラベルを割り当てるために使用する戦略。ラプラシアン埋め込み後にラベルを割り当てるには2つの方法がありますが、k-meansを適用することができ、一般的な選択です。しかし、初期化に敏感になることもあります。離散化は、ランダムな初期化の影響を受けにくいもう一つの方法です。</target>
        </trans-unit>
        <trans-unit id="0a02a9e0399d776c0fdc23972d432af0d079552e" translate="yes" xml:space="preserve">
          <source>The strategy to use to assign labels in the embedding space. There are two ways to assign labels after the laplacian embedding. k-means can be applied and is a popular choice. But it can also be sensitive to initialization. Discretization is another approach which is less sensitive to random initialization. See the &amp;lsquo;Multiclass spectral clustering&amp;rsquo; paper referenced below for more details on the discretization approach.</source>
          <target state="translated">埋め込みスペースでラベルを割り当てるために使用する戦略。ラプラシアン埋め込み後にラベルを割り当てる方法は2つあります。k-meansは適用可能であり、一般的な選択肢です。ただし、初期化の影響を受けやすい場合もあります。離散化は、ランダムな初期化の影響を受けにくい別のアプローチです。離散化アプローチの詳細については、以下で参照される「マルチクラススペクトルクラスタリング」ペーパーを参照してください。</target>
        </trans-unit>
        <trans-unit id="067b32fbfa4aafe8139a50a2cd0dc62d48f6ce7c" translate="yes" xml:space="preserve">
          <source>The strategy used to choose the split at each node. Supported strategies are &amp;ldquo;best&amp;rdquo; to choose the best split and &amp;ldquo;random&amp;rdquo; to choose the best random split.</source>
          <target state="translated">各ノードで分割を選択するために使用される戦略。サポートされている戦略は、最良の分割を選択する「最善」と、最良のランダム分割を選択する「ランダム」です。</target>
        </trans-unit>
        <trans-unit id="e4b87188ae01dfb2ea23e8aa9287a121677cbc35" translate="yes" xml:space="preserve">
          <source>The strength of recall versus precision in the F-score.</source>
          <target state="translated">Fスコアにおけるリコール対精度の強さ。</target>
        </trans-unit>
        <trans-unit id="9380c762fbcadf9826438d5ff503ef39938c2642" translate="yes" xml:space="preserve">
          <source>The strength of the LOF algorithm is that it takes both local and global properties of datasets into consideration: it can perform well even in datasets where abnormal samples have different underlying densities. The question is not, how isolated the sample is, but how isolated it is with respect to the surrounding neighborhood.</source>
          <target state="translated">LOFアルゴリズムの強みは、データセットのローカル特性とグローバル特性の両方を考慮に入れていることです。問題は、サンプルがどれだけ孤立しているかではなく、そのサンプルが周囲の近傍からどれだけ孤立しているかということです。</target>
        </trans-unit>
        <trans-unit id="ef559a266ab9339add9416970528df4288b9fe07" translate="yes" xml:space="preserve">
          <source>The string to decode</source>
          <target state="translated">デコードする文字列</target>
        </trans-unit>
        <trans-unit id="71af025de90c113777cd083adeb4a7dc41c643ae" translate="yes" xml:space="preserve">
          <source>The string value &amp;ldquo;auto&amp;rdquo; determines whether y should increase or decrease based on the Spearman correlation estimate&amp;rsquo;s sign.</source>
          <target state="translated">文字列値「auto」は、スピアマン相関推定の符号に基づいて、yを増減するかどうかを決定します。</target>
        </trans-unit>
        <trans-unit id="68913ceeaf791c0f89f5da0e6ea267a6c64604e5" translate="yes" xml:space="preserve">
          <source>The submatrix corresponding to bicluster i.</source>
          <target state="translated">バイクラスターiに対応する部分行列。</target>
        </trans-unit>
        <trans-unit id="a10436d8ac7a1230da5ccfb4c02351e0bfbb4701" translate="yes" xml:space="preserve">
          <source>The subset of drawn features for each base estimator.</source>
          <target state="translated">各ベース推定器の描画特徴量のサブセット。</target>
        </trans-unit>
        <trans-unit id="57675bd8615ef838a99f713d239feb9810e82664" translate="yes" xml:space="preserve">
          <source>The subset of drawn samples for each base estimator.</source>
          <target state="translated">各基底推定量のための描画サンプルのサブセット。</target>
        </trans-unit>
        <trans-unit id="58b06737b1ee81d6af2156c3790929db0bc0c464" translate="yes" xml:space="preserve">
          <source>The sum of the features (number of words if documents) is drawn from a Poisson distribution with this expected value.</source>
          <target state="translated">この期待値でポアソン分布から特徴量(文書の場合は単語数)の和を描きます。</target>
        </trans-unit>
        <trans-unit id="690aa5752a21a529c84a7d2bed72d6956dfbf2f1" translate="yes" xml:space="preserve">
          <source>The support is the number of occurrences of each class in &lt;code&gt;y_true&lt;/code&gt;.</source>
          <target state="translated">サポートは、 &lt;code&gt;y_true&lt;/code&gt; の各クラスの出現回数です。</target>
        </trans-unit>
        <trans-unit id="b8f29f24bf343b8a8c6a9702bbb3373c30b3886a" translate="yes" xml:space="preserve">
          <source>The support vector machines in scikit-learn support both dense (&lt;code&gt;numpy.ndarray&lt;/code&gt; and convertible to that by &lt;code&gt;numpy.asarray&lt;/code&gt;) and sparse (any &lt;code&gt;scipy.sparse&lt;/code&gt;) sample vectors as input. However, to use an SVM to make predictions for sparse data, it must have been fit on such data. For optimal performance, use C-ordered &lt;code&gt;numpy.ndarray&lt;/code&gt; (dense) or &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; (sparse) with &lt;code&gt;dtype=float64&lt;/code&gt;.</source>
          <target state="translated">scikit-learnのサポートベクターマシンは、入力として密（ &lt;code&gt;numpy.ndarray&lt;/code&gt; と &lt;code&gt;numpy.asarray&lt;/code&gt; によって変換可能）および疎（任意の &lt;code&gt;scipy.sparse&lt;/code&gt; ）サンプルベクトルの両方をサポートします。ただし、SVMを使用してスパースデータを予測するには、そのようなデータに適合している必要があります。最適なパフォーマンスを得るには、Cタイプの &lt;code&gt;numpy.ndarray&lt;/code&gt; （密）または &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; （疎）を &lt;code&gt;dtype=float64&lt;/code&gt; で使用します。</target>
        </trans-unit>
        <trans-unit id="b8e93494175fc764f9336519319d2c72af2d3469" translate="yes" xml:space="preserve">
          <source>The target features for which the partial dependecy should be computed (size should be smaller than 3 for visual renderings).</source>
          <target state="translated">部分依存性が計算されるべきターゲット特徴(視覚的なレンダリングの場合は3よりも小さいサイズでなければなりません)。</target>
        </trans-unit>
        <trans-unit id="c6694d34ee3bb1ae73f71f475a9064ea6bb5aa61" translate="yes" xml:space="preserve">
          <source>The target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set.</source>
          <target state="translated">目標は、訓練セット内の最も近い隣人に関連付けられた目標を局所的に補間することで予測されます。</target>
        </trans-unit>
        <trans-unit id="836251639a1d2dd67d806e7910ad6656af016095" translate="yes" xml:space="preserve">
          <source>The target values (class labels in classification, real numbers in regression).</source>
          <target state="translated">目標値(分類ではクラスラベル、回帰では実数)。</target>
        </trans-unit>
        <trans-unit id="581ab16d01401b52de3a8a770522ae3215b0d2a3" translate="yes" xml:space="preserve">
          <source>The target values (class labels) as integers or strings.</source>
          <target state="translated">対象となる値(クラスラベル)を整数または文字列として指定します。</target>
        </trans-unit>
        <trans-unit id="c42151a6b9abff52d7ec8ba07a5200409a48a3c6" translate="yes" xml:space="preserve">
          <source>The target values (class labels).</source>
          <target state="translated">対象となる値(クラスラベル)。</target>
        </trans-unit>
        <trans-unit id="af111c076ceef9b210aa6e236368271feda238ba" translate="yes" xml:space="preserve">
          <source>The target values (integers that correspond to classes in classification, real numbers in regression).</source>
          <target state="translated">目標値(分類ではクラスに対応する整数、回帰では実数)。</target>
        </trans-unit>
        <trans-unit id="94e263fd180ba7303394b0318de33e42ec7bd528" translate="yes" xml:space="preserve">
          <source>The target values (real numbers).</source>
          <target state="translated">目標値(実数)です。</target>
        </trans-unit>
        <trans-unit id="4f52bd7c58bfce68aafb35b3e2f5dd531ddc572f" translate="yes" xml:space="preserve">
          <source>The target values (real numbers). Use &lt;code&gt;dtype=np.float64&lt;/code&gt; and &lt;code&gt;order='C'&lt;/code&gt; for maximum efficiency.</source>
          <target state="translated">ターゲット値（実数）。使用 &lt;code&gt;dtype=np.float64&lt;/code&gt; 及び &lt;code&gt;order='C'&lt;/code&gt; 最大効率のために。</target>
        </trans-unit>
        <trans-unit id="53a447e32fbe96637e9d7be27a641b24353eba6e" translate="yes" xml:space="preserve">
          <source>The target values.</source>
          <target state="translated">目標値です。</target>
        </trans-unit>
        <trans-unit id="863f0df40c91ba7090e8eb769163050f217d7bc5" translate="yes" xml:space="preserve">
          <source>The target variable for supervised learning problems.</source>
          <target state="translated">教師付き学習問題の対象変数。</target>
        </trans-unit>
        <trans-unit id="f60efda845afa3f0dc7adc040cd9b2450fbcc2aa" translate="yes" xml:space="preserve">
          <source>The target variable for supervised learning problems. Stratification is done based on the y labels.</source>
          <target state="translated">教師付き学習問題の対象変数。層化はyラベルに基づいて行われる.</target>
        </trans-unit>
        <trans-unit id="aa20ad6bc67663b59dda9b76a4a065ca1f86af8f" translate="yes" xml:space="preserve">
          <source>The target variable is the median house value for California districts.</source>
          <target state="translated">対象変数は、カリフォルニア州の地区の住宅価値の中央値である。</target>
        </trans-unit>
        <trans-unit id="40a696d29fc2e13fd302babe7b309a6f769a208a" translate="yes" xml:space="preserve">
          <source>The target variable to try to predict in the case of supervised learning.</source>
          <target state="translated">教師付き学習の場合に予測しようとする対象変数。</target>
        </trans-unit>
        <trans-unit id="a34872d2673c11b79098f51c73d69310c6a49da3" translate="yes" xml:space="preserve">
          <source>The task at hand is to predict disease progression from physiological variables.</source>
          <target state="translated">目下の課題は、生理学的変数から疾患の進行を予測することである。</target>
        </trans-unit>
        <trans-unit id="609ac3008273ee503e4809fb5a54a64b3f8be85d" translate="yes" xml:space="preserve">
          <source>The ten features are standard independent Gaussian and the target &lt;code&gt;y&lt;/code&gt; is defined by:</source>
          <target state="translated">10個の特徴は標準の独立ガウス分布であり、ターゲット &lt;code&gt;y&lt;/code&gt; は次のように定義されます。</target>
        </trans-unit>
        <trans-unit id="b7581ec7a4d469cfac096612cf94e3958274d6aa" translate="yes" xml:space="preserve">
          <source>The term &amp;ldquo;discrete features&amp;rdquo; is used instead of naming them &amp;ldquo;categorical&amp;rdquo;, because it describes the essence more accurately. For example, pixel intensities of an image are discrete features (but hardly categorical) and you will get better results if mark them as such. Also note, that treating a continuous variable as discrete and vice versa will usually give incorrect results, so be attentive about that.</source>
          <target state="translated">「離散的特徴」という用語は、本質をより正確に説明するため、「カテゴリー的」という名前の代わりに使用されます。たとえば、画像のピクセル強度は個別の特徴であり（ほとんどカテゴリではありません）、そのようにマークを付けるとより良い結果が得られます。また、連続変数を離散として、またはその逆として扱うと、通常は正しくない結果が得られるので注意してください。</target>
        </trans-unit>
        <trans-unit id="1748689c159b7c280435a293a84c60a8fa11ddf6" translate="yes" xml:space="preserve">
          <source>The test points for the data. Same format as the training data.</source>
          <target state="translated">データのテストポイント。訓練データと同じ形式。</target>
        </trans-unit>
        <trans-unit id="c4412981e13c1e09172f9e595c07a27a82a32abb" translate="yes" xml:space="preserve">
          <source>The testing set indices for that split.</source>
          <target state="translated">その分割のためのテストセットのインデックス。</target>
        </trans-unit>
        <trans-unit id="c079148b8f468ab34a1c911c5b93e7fb1e4fbf43" translate="yes" xml:space="preserve">
          <source>The text feature extractors in scikit-learn know how to decode text files, but only if you tell them what encoding the files are in. The &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt; takes an &lt;code&gt;encoding&lt;/code&gt; parameter for this purpose. For modern text files, the correct encoding is probably UTF-8, which is therefore the default (&lt;code&gt;encoding=&quot;utf-8&quot;&lt;/code&gt;).</source>
          <target state="translated">scikit-learnのテキスト機能エクストラクターは、テキストファイルをデコードする方法を知っていますが、ファイルのエンコーディングをユーザーに通知する場合に限られます&lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt;は、この目的のために &lt;code&gt;encoding&lt;/code&gt; パラメーターを取ります。最新のテキストファイルの場合、正しいエンコーディングはおそらくUTF-8です。したがって、これがデフォルトです（ &lt;code&gt;encoding=&quot;utf-8&quot;&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="5da204cc914d9d1b370d2a7e3d3f9fed2399ad38" translate="yes" xml:space="preserve">
          <source>The theory says that in order to achieve prediction consistency, the penalty parameter should be kept constant as the number of samples grow.</source>
          <target state="translated">予測の一貫性を得るためには、サンプル数の増加に応じてペナルティパラメータを一定に保つ必要があるという理論です。</target>
        </trans-unit>
        <trans-unit id="ed8c4048d538066672a56cef623687584e4d51a1" translate="yes" xml:space="preserve">
          <source>The third figure compares kernel density estimates for a distribution of 100 samples in 1 dimension. Though this example uses 1D distributions, kernel density estimation is easily and efficiently extensible to higher dimensions as well.</source>
          <target state="translated">3番目の図は、1次元の100サンプルの分布のカーネル密度推定値を比較したものです。この例では1次元分布を使用していますが、カーネル密度推定は高次元にも簡単かつ効率的に拡張できます。</target>
        </trans-unit>
        <trans-unit id="0742a25a1bc73abd7670f1b33a5e8f933c99aa55" translate="yes" xml:space="preserve">
          <source>The third model is also a Bayesian Gaussian mixture model with a Dirichlet process prior but this time the value of the concentration prior is higher giving the model more liberty to model the fine-grained structure of the data. The result is a mixture with a larger number of active components that is similar to the first model where we arbitrarily decided to fix the number of components to 10.</source>
          <target state="translated">3番目のモデルは、同じくディリクレ過程先行を持つベイズ・ガウス混合モデルですが、今回は濃度先行の値がより高くなっており、データの微細な構造をモデルに与える自由度が高くなっています。結果は、我々が任意に成分数を10に固定することを決定した最初のモデルと同様に、より多くの活性成分を持つ混合物です。</target>
        </trans-unit>
        <trans-unit id="f1f7cd9ed7ac82273a71a8430edb1e09f61b8cab" translate="yes" xml:space="preserve">
          <source>The threshold value to use for feature selection. Features whose importance is greater or equal are kept while the others are discarded. If &amp;ldquo;median&amp;rdquo; (resp. &amp;ldquo;mean&amp;rdquo;), then the &lt;code&gt;threshold&lt;/code&gt; value is the median (resp. the mean) of the feature importances. A scaling factor (e.g., &amp;ldquo;1.25*mean&amp;rdquo;) may also be used. If None and if the estimator has a parameter penalty set to l1, either explicitly or implicitly (e.g, Lasso), the threshold used is 1e-5. Otherwise, &amp;ldquo;mean&amp;rdquo; is used by default.</source>
          <target state="translated">機能の選択に使用するしきい値。重要性がそれ以上の機能は保持され、他の機能は破棄されます。「中央値」（または「平均」）の場合、 &lt;code&gt;threshold&lt;/code&gt; は機能の重要度の中央値（または平均）です。スケーリング係数（たとえば、「1.25 *平均」）も使用できます。Noneで、推定量のパラメーターペナルティがl1に明示的または暗黙的に（たとえばLasso）設定されている場合、使用されるしきい値は1e-5です。それ以外の場合は、デフォルトで「平均」が使用されます。</target>
        </trans-unit>
        <trans-unit id="91a1a785c1bc7a11d4e20e6e119c885bf4142111" translate="yes" xml:space="preserve">
          <source>The threshold value used for feature selection.</source>
          <target state="translated">特徴の選択に使用される閾値。</target>
        </trans-unit>
        <trans-unit id="5a48126d50b83afd18e220a4fcf0cc7ffc7180d9" translate="yes" xml:space="preserve">
          <source>The time complexity of this implementation is &lt;code&gt;O(d ** 2)&lt;/code&gt; assuming d ~ n_features ~ n_components.</source>
          <target state="translated">この実装の時間の複雑さは、d〜n_features〜n_componentsを想定した &lt;code&gt;O(d ** 2)&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="ff8097e0ec52614ae168c4cd444f04e3f78b14e2" translate="yes" xml:space="preserve">
          <source>The time for fitting the estimator on the train set for each cv split.</source>
          <target state="translated">cv 分割ごとの列車集合に推定器を適合させるための時間.</target>
        </trans-unit>
        <trans-unit id="dc0535c66e14595ad26a449dc475b0e83c5091ba" translate="yes" xml:space="preserve">
          <source>The time for scoring the estimator on the test set for each cv split. (Note time for scoring on the train set is not included even if &lt;code&gt;return_train_score&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;</source>
          <target state="translated">各CV分割のテストセットで推定量をスコアリングする時間。（ &lt;code&gt;return_train_score&lt;/code&gt; が &lt;code&gt;True&lt;/code&gt; に設定されている場合でも、トレインセットのスコアリングの時間は含まれません。</target>
        </trans-unit>
        <trans-unit id="1cd8d7a025bee860396ab665c8f7316a009f2f5a" translate="yes" xml:space="preserve">
          <source>The tolerance for the elastic net solver used to calculate the descent direction. This parameter controls the accuracy of the search direction for a given column update, not of the overall parameter estimate. Only used for mode=&amp;rsquo;cd&amp;rsquo;.</source>
          <target state="translated">降下方向の計算に使用されるエラスティックネットソルバーの許容誤差。このパラメーターは、全体的なパラメーター推定値ではなく、特定の列更新の検索方向の精度を制御します。mode = 'cd'にのみ使用されます。</target>
        </trans-unit>
        <trans-unit id="54800ee92ded7e21b226653650e94d2e245f5dc0" translate="yes" xml:space="preserve">
          <source>The tolerance for the optimization: if the updates are smaller than &lt;code&gt;tol&lt;/code&gt;, the optimization code checks the dual gap for optimality and continues until it is smaller than &lt;code&gt;tol&lt;/code&gt;.</source>
          <target state="translated">最適化の許容範囲：更新が &lt;code&gt;tol&lt;/code&gt; よりも小さい場合、最適化コードは最適化についてデュアルギャップをチェックし、 &lt;code&gt;tol&lt;/code&gt; より小さくなるまで続行します。</target>
        </trans-unit>
        <trans-unit id="7abdf76511114d1a589dafaf8c3b9fea94410d4e" translate="yes" xml:space="preserve">
          <source>The tolerance to declare convergence: if the dual gap goes below this value, iterations are stopped.</source>
          <target state="translated">収束を宣言するための許容範囲:デュアルギャップがこの値を下回った場合、反復処理を停止します。</target>
        </trans-unit>
        <trans-unit id="1174cbfc6c5cd8bb9e51e269de526360c060c73a" translate="yes" xml:space="preserve">
          <source>The tomography projection operation is a linear transformation. In addition to the data-fidelity term corresponding to a linear regression, we penalize the L1 norm of the image to account for its sparsity. The resulting optimization problem is called the &lt;a href=&quot;../../modules/linear_model#lasso&quot;&gt;Lasso&lt;/a&gt;. We use the class &lt;a href=&quot;../../modules/generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt;&lt;code&gt;sklearn.linear_model.Lasso&lt;/code&gt;&lt;/a&gt;, that uses the coordinate descent algorithm. Importantly, this implementation is more computationally efficient on a sparse matrix, than the projection operator used here.</source>
          <target state="translated">トモグラフィー投影操作は線形変換です。線形回帰に対応するデータ忠実度の項に加えて、画像のL1ノルムにペナルティを課して、スパース性を考慮します。結果として生じる最適化問題は、&lt;a href=&quot;../../modules/linear_model#lasso&quot;&gt;投げ縄&lt;/a&gt;と呼ばれます。座標降下アルゴリズムを使用するクラス&lt;a href=&quot;../../modules/generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt; &lt;code&gt;sklearn.linear_model.Lasso&lt;/code&gt; &lt;/a&gt;を使用します。重要なのは、この実装は、ここで使用されている射影演算子よりも、スパース行列での計算効率が高いことです。</target>
        </trans-unit>
        <trans-unit id="6067d7bbaca20238e188da5c4c1f4f9b915cbe46" translate="yes" xml:space="preserve">
          <source>The total number of features.</source>
          <target state="translated">機能の総数。</target>
        </trans-unit>
        <trans-unit id="52b059ac9ef2b76cd7b57a438578db960faf18a1" translate="yes" xml:space="preserve">
          <source>The total number of features. These comprise &lt;code&gt;n_informative&lt;/code&gt; informative features, &lt;code&gt;n_redundant&lt;/code&gt; redundant features, &lt;code&gt;n_repeated&lt;/code&gt; duplicated features and &lt;code&gt;n_features-n_informative-n_redundant-n_repeated&lt;/code&gt; useless features drawn at random.</source>
          <target state="translated">機能の総数。これらは、 &lt;code&gt;n_informative&lt;/code&gt; されるn_informative有益な機能、 &lt;code&gt;n_redundant&lt;/code&gt; 冗長機能、 &lt;code&gt;n_repeated&lt;/code&gt; 複製機能、および &lt;code&gt;n_features-n_informative-n_redundant-n_repeated&lt;/code&gt; 役に立たない機能で構成されます。</target>
        </trans-unit>
        <trans-unit id="1b95194d002b1fa90370f5ac460e11ae527d46c4" translate="yes" xml:space="preserve">
          <source>The total number of input features.</source>
          <target state="translated">入力特徴量の総数。</target>
        </trans-unit>
        <trans-unit id="75f4c14304ea0320f6751402bd1abbcf764fb2d4" translate="yes" xml:space="preserve">
          <source>The total number of points equally divided among classes.</source>
          <target state="translated">クラス間で等分されたポイント数の合計。</target>
        </trans-unit>
        <trans-unit id="6fd1a66b2c352f2c105828a73ec9201a20677da3" translate="yes" xml:space="preserve">
          <source>The total number of points generated.</source>
          <target state="translated">生成されたポイントの合計数です。</target>
        </trans-unit>
        <trans-unit id="b463685bc7194f4e1bd9c9e9566855d8af2442c3" translate="yes" xml:space="preserve">
          <source>The total number of points generated. If odd, the inner circle will have one point more than the outer circle.</source>
          <target state="translated">生成されたポイントの総数です。奇数の場合、内側の円は外側の円よりも1点多くなります。</target>
        </trans-unit>
        <trans-unit id="30df1d74b9688e22831b35a50354a3af5ea3a9b9" translate="yes" xml:space="preserve">
          <source>The total number of polynomial output features. The number of output features is computed by iterating over all suitably sized combinations of input features.</source>
          <target state="translated">多項式出力特徴量の総数。出力特徴量の数は,入力特徴量のすべての適切なサイズの組み合わせを反復して計算されます.</target>
        </trans-unit>
        <trans-unit id="2341cd15b4f720c4e4ca39627124f453602ba043" translate="yes" xml:space="preserve">
          <source>The traditional way to compute the principal eigenvector is to use the power iteration method:</source>
          <target state="translated">主固有ベクトルを計算する伝統的な方法は、パワーイテレーション法を使用しています。</target>
        </trans-unit>
        <trans-unit id="486bc8b3be25b906a521777296790c0e7db3392b" translate="yes" xml:space="preserve">
          <source>The training algorithm implemented in &lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt;&lt;code&gt;BernoulliRBM&lt;/code&gt;&lt;/a&gt; is known as Stochastic Maximum Likelihood (SML) or Persistent Contrastive Divergence (PCD). Optimizing maximum likelihood directly is infeasible because of the form of the data likelihood:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt; &lt;code&gt;BernoulliRBM&lt;/code&gt; に&lt;/a&gt;実装されているトレーニングアルゴリズムは、確率的最尤（SML）または持続的コントラスト発散（PCD）として知られています。最大尤度を直接最適化することは、データ尤度の形式のために実行不可能です。</target>
        </trans-unit>
        <trans-unit id="03936b9df8c7a04402a186159fb53bde128b3907" translate="yes" xml:space="preserve">
          <source>The training data</source>
          <target state="translated">学習データ</target>
        </trans-unit>
        <trans-unit id="a7dac9ee1d012d3a04e96a076bd4bb0255f4fa3a" translate="yes" xml:space="preserve">
          <source>The training data contains outliers which are defined as observations that are far from the others. Outlier detection estimators thus try to fit the regions where the training data is the most concentrated, ignoring the deviant observations.</source>
          <target state="translated">訓練データは,他のものから離れたオブザベーションとして定義される外れ値を含む.したがって,外れ値検出推定器は,逸脱したオブザベーションを無視して,訓練データが最も集中している領域に適合しようとする.</target>
        </trans-unit>
        <trans-unit id="58b640148d7ae1e7f191ee9d800aaef902a6aef0" translate="yes" xml:space="preserve">
          <source>The training data is not polluted by outliers and we are interested in detecting whether a &lt;strong&gt;new&lt;/strong&gt; observation is an outlier. In this context an outlier is also called a novelty.</source>
          <target state="translated">トレーニングデータは外れ値によって汚染されておらず、&lt;strong&gt;新しい&lt;/strong&gt;観測値が外れ値であるかどうかを検出することに関心があります。この文脈では、外れ値は新規性とも呼ばれます。</target>
        </trans-unit>
        <trans-unit id="431a9d8e158a3b53d56cef19b322f4a781297a84" translate="yes" xml:space="preserve">
          <source>The training data, e.g. a reference to an immutable snapshot</source>
          <target state="translated">学習データ、例えば不変のスナップショットへの参照</target>
        </trans-unit>
        <trans-unit id="44e706b0239b6ac2fad3df8ff059d735dbfbf296" translate="yes" xml:space="preserve">
          <source>The training input samples.</source>
          <target state="translated">学習入力サンプル。</target>
        </trans-unit>
        <trans-unit id="92c6c3d94fd9608db44f0ede45f9bbe4de664d1d" translate="yes" xml:space="preserve">
          <source>The training input samples. Internally, it will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt; and if a sparse matrix is provided to a sparse &lt;code&gt;csc_matrix&lt;/code&gt;.</source>
          <target state="translated">トレーニング入力サンプル。内部的に、それは &lt;code&gt;dtype=np.float32&lt;/code&gt; に変換され、疎行列が疎 &lt;code&gt;csc_matrix&lt;/code&gt; に提供される場合。</target>
        </trans-unit>
        <trans-unit id="fdce812a7f2c7f82334342af14e6e75d01b61ef1" translate="yes" xml:space="preserve">
          <source>The training input samples. Internally, its dtype will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt;. If a sparse matrix is provided, it will be converted into a sparse &lt;code&gt;csc_matrix&lt;/code&gt;.</source>
          <target state="translated">トレーニング入力サンプル。内部的には、そのdtypeは &lt;code&gt;dtype=np.float32&lt;/code&gt; に変換されます。スパース行列が指定されている場合は、スパース &lt;code&gt;csc_matrix&lt;/code&gt; に変換されます。</target>
        </trans-unit>
        <trans-unit id="549582a79c03c6cf3a88a3f5ae8ab3477fe6f4f0" translate="yes" xml:space="preserve">
          <source>The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.</source>
          <target state="translated">訓練入力サンプル。疎な行列は、基底推定器でサポートされている場合にのみ受け入れられる。</target>
        </trans-unit>
        <trans-unit id="ed9fac6749e01b46bdd0e4eebbecd7ed42fc3bac" translate="yes" xml:space="preserve">
          <source>The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK and LIL are converted to CSR.</source>
          <target state="translated">学習入力サンプル.スパース行列は、CSC,CSR,COO,DOK,LIL のいずれかになります。DOK と LIL は CSR に変換されます.</target>
        </trans-unit>
        <trans-unit id="dba95905d10a3a6d61cb924560b16f35840a95de" translate="yes" xml:space="preserve">
          <source>The training points for the data. Each point has three fields:</source>
          <target state="translated">データのトレーニングポイントです。各ポイントには3つのフィールドがあります。</target>
        </trans-unit>
        <trans-unit id="49afe52f19d67077f586b0d86a3a80bb7b9051b3" translate="yes" xml:space="preserve">
          <source>The training set has size &lt;code&gt;i * n_samples // (n_splits + 1)
+ n_samples % (n_splits + 1)&lt;/code&gt; in the &lt;code&gt;i``th split,
with a test set of size ``n_samples//(n_splits + 1)&lt;/code&gt;, where &lt;code&gt;n_samples&lt;/code&gt; is the number of samples.</source>
          <target state="translated">トレーニングセットのサイズは、 &lt;code&gt;i * n_samples // (n_splits + 1) + n_samples % (n_splits + 1)&lt;/code&gt; で、 &lt;code&gt;i``th split, with a test set of size ``n_samples//(n_splits + 1)&lt;/code&gt; あり、 &lt;code&gt;n_samples&lt;/code&gt; サンプル数です。</target>
        </trans-unit>
        <trans-unit id="7d0d65f5b4df90d39c92efe2c541dfa70477a3f3" translate="yes" xml:space="preserve">
          <source>The training set indices for that split.</source>
          <target state="translated">その分割のためのトレーニングセットのインデックス。</target>
        </trans-unit>
        <trans-unit id="1fe1c9ebc66c3c60358a4b7d7ce6958b71f5be6a" translate="yes" xml:space="preserve">
          <source>The transformation can be triggered by setting either &lt;code&gt;transformer&lt;/code&gt; or the pair of functions &lt;code&gt;func&lt;/code&gt; and &lt;code&gt;inverse_func&lt;/code&gt;. However, setting both options will raise an error.</source>
          <target state="translated">変換は、 &lt;code&gt;transformer&lt;/code&gt; または関数 &lt;code&gt;func&lt;/code&gt; と &lt;code&gt;inverse_func&lt;/code&gt; ペアを設定することでトリガーできます。ただし、両方のオプションを設定するとエラーが発生します。</target>
        </trans-unit>
        <trans-unit id="c1ce2cd56f04ea729576a873f5ef18af794ea62c" translate="yes" xml:space="preserve">
          <source>The transformation is applied on each feature independently. The cumulative density function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.</source>
          <target state="translated">変換は、各特徴量に対して独立して適用されます。特徴量の累積密度関数は、元の値を投影するために使用されます。フィットされた範囲よりも下または上にある新しい/未見のデータの特徴量は,出力分布の境界にマッピングされます.この変換は非線形であることに注意してください。これは,同じスケールで測定された変数間の線形相関を歪めるかもしれないが,異なるスケールで測定された変数をより直接的に比較可能にする.</target>
        </trans-unit>
        <trans-unit id="ddd72b82186574150d900e6dfd8721345fa62073" translate="yes" xml:space="preserve">
          <source>The transformation is given by:</source>
          <target state="translated">変換は次のように与えられます。</target>
        </trans-unit>
        <trans-unit id="1db1e441acce63afd6d696933447c0b2d453bb8c" translate="yes" xml:space="preserve">
          <source>The transformed data</source>
          <target state="translated">変換されたデータ</target>
        </trans-unit>
        <trans-unit id="0a52b9359f518099e81b711eeaccc5e0c7c17eb0" translate="yes" xml:space="preserve">
          <source>The transformed data is then used to train a naive Bayes classifier, and a clear difference in prediction accuracies is observed wherein the dataset which is scaled before PCA vastly outperforms the unscaled version.</source>
          <target state="translated">変換されたデータはナイーブベイズ分類器の訓練に使用され、PCAの前にスケーリングされたデータセットは、スケーリングされていないバージョンを大幅に上回るという、予測精度の明確な違いが観察されます。</target>
        </trans-unit>
        <trans-unit id="80e852f03673351dd5ee00932a57dc4a209cdf2d" translate="yes" xml:space="preserve">
          <source>The transformed data.</source>
          <target state="translated">変換されたデータです。</target>
        </trans-unit>
        <trans-unit id="831c10597508e086776cd00760f56c708f8d2bba" translate="yes" xml:space="preserve">
          <source>The tree algorithm to use. Valid options are [&amp;lsquo;kd_tree&amp;rsquo;|&amp;rsquo;ball_tree&amp;rsquo;|&amp;rsquo;auto&amp;rsquo;]. Default is &amp;lsquo;auto&amp;rsquo;.</source>
          <target state="translated">使用するツリーアルゴリズム。有効なオプションは['kd_tree' | 'ball_tree' | 'auto']です。デフォルトは「auto」です。</target>
        </trans-unit>
        <trans-unit id="d745501ed4c4af3a67688bd307560f44fb29c904" translate="yes" xml:space="preserve">
          <source>The tree data structure consists of nodes with each node consisting of a number of subclusters. The maximum number of subclusters in a node is determined by the branching factor. Each subcluster maintains a linear sum, squared sum and the number of samples in that subcluster. In addition, each subcluster can also have a node as its child, if the subcluster is not a member of a leaf node.</source>
          <target state="translated">木データ構造は、各ノードがいくつかのサブクラスターからなるノードで構成されています。ノード内のサブクラスターの最大数は分岐係数によって決定されます。各サブクラスタは、線形和、二乗和、およびそのサブクラスタ内のサンプル数を保持する。さらに、各サブクラスタは、サブクラスタがリーフノードのメンバーでない場合、その子としてノードを持つこともできる。</target>
        </trans-unit>
        <trans-unit id="fbc6ef1abc08faa2debda525a78475d34850618e" translate="yes" xml:space="preserve">
          <source>The true probability in each bin (fraction of positives).</source>
          <target state="translated">各ビンの真の確率(陽性の割合)。</target>
        </trans-unit>
        <trans-unit id="89aec1004fa9767eced66561c3ed3161000c45a1" translate="yes" xml:space="preserve">
          <source>The true score without permuting targets.</source>
          <target state="translated">ターゲットをパーマネントしない真のスコア。</target>
        </trans-unit>
        <trans-unit id="311da69a1a719737450cc586668ba277c4bde011" translate="yes" xml:space="preserve">
          <source>The tutorial folder should contain the following sub-folders:</source>
          <target state="translated">チュートリアルフォルダには、以下のサブフォルダが含まれている必要があります。</target>
        </trans-unit>
        <trans-unit id="2e8b6623a8185bbb598cfdf6b3f71f1ee2c2a837" translate="yes" xml:space="preserve">
          <source>The two figures below plot the values of &lt;code&gt;C&lt;/code&gt; on the &lt;code&gt;x-axis&lt;/code&gt; and the corresponding cross-validation scores on the &lt;code&gt;y-axis&lt;/code&gt;, for several different fractions of a generated data-set.</source>
          <target state="translated">以下の2つの図は、生成されたデータセットのいくつかの異なる部分について、 &lt;code&gt;x-axis&lt;/code&gt; に &lt;code&gt;C&lt;/code&gt; の値を、 &lt;code&gt;y-axis&lt;/code&gt; に対応する相互検証スコアをプロットしています。</target>
        </trans-unit>
        <trans-unit id="3ed5804ea6261422d9ec471fffa8f8a41a307d64" translate="yes" xml:space="preserve">
          <source>The two species are:</source>
          <target state="translated">2種です。</target>
        </trans-unit>
        <trans-unit id="4aa36c5d8992165f7895075f7b16a37f9864b092" translate="yes" xml:space="preserve">
          <source>The type of criterion to use.</source>
          <target state="translated">使用する基準の種類。</target>
        </trans-unit>
        <trans-unit id="6bc8093d847369045cfca5abe49cd94f035f902d" translate="yes" xml:space="preserve">
          <source>The type of feature values. Passed to Numpy array/scipy.sparse matrix constructors as the dtype argument.</source>
          <target state="translated">特徴量の型を指定します.Numpy array/scipy.sparse matrix constructors に dtype 引数として渡されます.</target>
        </trans-unit>
        <trans-unit id="51cef5c60b8823e16a67a0821a4b5311abdcd5ed" translate="yes" xml:space="preserve">
          <source>The type of feature values. Passed to scipy.sparse matrix constructors as the dtype argument. Do not set this to bool, np.boolean or any unsigned integer type.</source>
          <target state="translated">特徴量の型.dtype 引数として scipy.sparse matrix constructors に渡されます。bool、np.boolean、または任意の符号なし整数型には設定しないでください。</target>
        </trans-unit>
        <trans-unit id="25d83e9ee3b7a100418b9b6d2bb7936470b09825" translate="yes" xml:space="preserve">
          <source>The type of norm used to compute the error. Available error types: - &amp;lsquo;frobenius&amp;rsquo; (default): sqrt(tr(A^t.A)) - &amp;lsquo;spectral&amp;rsquo;: sqrt(max(eigenvalues(A^t.A)) where A is the error &lt;code&gt;(comp_cov - self.covariance_)&lt;/code&gt;.</source>
          <target state="translated">エラーの計算に使用されるノルムのタイプ。利用可能なエラータイプ：-'frobenius'（デフォルト）：sqrt（tr（A ^ tA））-'spectral'：sqrt（max（eigenvalues（A ^ tA）））ここで、Aはエラー &lt;code&gt;(comp_cov - self.covariance_)&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="f0b5e941b7e074477ab9a734aa8fbc101aeb347c" translate="yes" xml:space="preserve">
          <source>The unchanged dictionary atoms</source>
          <target state="translated">変更されていない辞書の原子</target>
        </trans-unit>
        <trans-unit id="776a52daee9321c0497d40c79109e87f34af6b7e" translate="yes" xml:space="preserve">
          <source>The underlying &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; implementation uses a random number generator to select features when fitting the model with a dual coordinate descent (i.e when &lt;code&gt;dual&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;). It is thus not uncommon, to have slightly different results for the same input data. If that happens, try with a smaller tol parameter. This randomness can also be controlled with the &lt;code&gt;random_state&lt;/code&gt; parameter. When &lt;code&gt;dual&lt;/code&gt; is set to &lt;code&gt;False&lt;/code&gt; the underlying implementation of &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; is not random and &lt;code&gt;random_state&lt;/code&gt; has no effect on the results.</source>
          <target state="translated">基本となる&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;実装は、モデルをデュアル座標降下で近似するとき（つまり、 &lt;code&gt;dual&lt;/code&gt; が &lt;code&gt;True&lt;/code&gt; に設定されているとき）、乱数ジェネレータを使用して機能を選択します。したがって、同じ入力データに対してわずかに異なる結果が得られることは珍しくありません。その場合は、tolパラメータを小さくしてみてください。このランダム性は、 &lt;code&gt;random_state&lt;/code&gt; パラメータでも制御できます。 &lt;code&gt;dual&lt;/code&gt; が &lt;code&gt;False&lt;/code&gt; に設定されている場合、&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;の基本的な実装はランダムではなく、 &lt;code&gt;random_state&lt;/code&gt; は結果に影響を与えません。</target>
        </trans-unit>
        <trans-unit id="68c40938fee4ae1976f2ebe09bc5c5a404f12d37" translate="yes" xml:space="preserve">
          <source>The underlying C implementation uses a random number generator to select features when fitting the model. It is thus not uncommon to have slightly different results for the same input data. If that happens, try with a smaller &lt;code&gt;tol&lt;/code&gt; parameter.</source>
          <target state="translated">基礎となるCの実装では、モデルを近似するときに、乱数ジェネレータを使用して特徴を選択します。したがって、同じ入力データに対してわずかに異なる結果が得られることは珍しくありません。その場合は、 &lt;code&gt;tol&lt;/code&gt; パラメータを小さくしてみてください。</target>
        </trans-unit>
        <trans-unit id="2f30e1e8a65635cf877334fb55d57ec3cedb0460" translate="yes" xml:space="preserve">
          <source>The underlying C implementation uses a random number generator to select features when fitting the model. It is thus not uncommon, to have slightly different results for the same input data. If that happens, try with a smaller tol parameter.</source>
          <target state="translated">基礎となるC言語の実装では、モデルのフィッティング時に特徴を選択するために乱数発生器を使用します。そのため、同じ入力データに対してわずかに異なる結果が出ることは珍しくありません。そのような場合は、tolパラメータを小さくしてみてください。</target>
        </trans-unit>
        <trans-unit id="4ed849766bc74a6b6038e401c289ce378db99dcc" translate="yes" xml:space="preserve">
          <source>The underlying Tree object. Please refer to &lt;code&gt;help(sklearn.tree._tree.Tree)&lt;/code&gt; for attributes of Tree object and &lt;a href=&quot;../../auto_examples/tree/plot_unveil_tree_structure#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py&quot;&gt;Understanding the decision tree structure&lt;/a&gt; for basic usage of these attributes.</source>
          <target state="translated">基になるTreeオブジェクト。Treeオブジェクトの属性については、 &lt;code&gt;help(sklearn.tree._tree.Tree)&lt;/code&gt; を参照してください。これらの属性の基本的な使用法&lt;a href=&quot;../../auto_examples/tree/plot_unveil_tree_structure#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py&quot;&gt;については、意思決定ツリーの構造&lt;/a&gt;を理解してください。</target>
        </trans-unit>
        <trans-unit id="eaa9f70240ca573ce446579a8e3077ce3fd3b2de" translate="yes" xml:space="preserve">
          <source>The underlying implementation is MurmurHash3_x86_32 generating low latency 32bits hash suitable for implementing lookup tables, Bloom filters, count min sketch or feature hashing.</source>
          <target state="translated">ベースとなる実装はMurmurHash3_x86_32で、ルックアップテーブル、ブルームフィルタ、カウントミニスケッチ、フィーチャーハッシュの実装に適した低レイテンシの32ビットハッシュを生成します。</target>
        </trans-unit>
        <trans-unit id="8b27403493b3e1cf67d088cabd49f20f60beabb5" translate="yes" xml:space="preserve">
          <source>The underlying implementation, liblinear, uses a sparse internal representation for the data that will incur a memory copy.</source>
          <target state="translated">基礎となる実装であるliblinearは、メモリコピーが発生するデータに対して疎な内部表現を使用しています。</target>
        </trans-unit>
        <trans-unit id="22bad6cd0a0fca07f198954a6d755d20a0363412" translate="yes" xml:space="preserve">
          <source>The univariate position of the sample according to the main dimension of the points in the manifold.</source>
          <target state="translated">マニホールド内の点の主次元に応じた試料の一変位置。</target>
        </trans-unit>
        <trans-unit id="e1bff9cf5ae34029868daf8dfe6afee04fc2666d" translate="yes" xml:space="preserve">
          <source>The unmixing matrix.</source>
          <target state="translated">非混合マトリックス。</target>
        </trans-unit>
        <trans-unit id="4c5425dc309e3cab0153df4bbf4dcfe21bae1aa0" translate="yes" xml:space="preserve">
          <source>The unsupervised data reduction and the supervised estimator can be chained in one step. See &lt;a href=&quot;compose#pipeline&quot;&gt;Pipeline: chaining estimators&lt;/a&gt;.</source>
          <target state="translated">教師なしデータ削減と教師あり推定量は、1つのステップで連鎖できます。&lt;a href=&quot;compose#pipeline&quot;&gt;Pipeline：chaining estimatorsを&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="fca2372915cd9dbf5bde8febd27812d65a387223" translate="yes" xml:space="preserve">
          <source>The upper left figure illustrates the predictions (in dark red) of a single decision tree trained over a random dataset LS (the blue dots) of a toy 1d regression problem. It also illustrates the predictions (in light red) of other single decision trees trained over other (and different) randomly drawn instances LS of the problem. Intuitively, the variance term here corresponds to the width of the beam of predictions (in light red) of the individual estimators. The larger the variance, the more sensitive are the predictions for &lt;code&gt;x&lt;/code&gt; to small changes in the training set. The bias term corresponds to the difference between the average prediction of the estimator (in cyan) and the best possible model (in dark blue). On this problem, we can thus observe that the bias is quite low (both the cyan and the blue curves are close to each other) while the variance is large (the red beam is rather wide).</source>
          <target state="translated">左上の図は、おもちゃの1d回帰問題のランダムなデータセットLS（青い点）でトレーニングされた単一の決定木の予測（濃い赤）を示しています。また、問題の他の（および異なる）ランダムに描画されたインスタンスLSでトレーニングされた他の単一の決定木の予測（薄い赤）も示しています。直感的に、ここでの分散項は、個々の推定量の予測のビームの幅（薄い赤）に対応しています。分散が大きいほど、 &lt;code&gt;x&lt;/code&gt; の予測は敏感になりますトレーニングセットの小さな変更に。バイアス項は、推定器（シアン）の平均予測と可能な限り最高のモデル（紺色）の差に対応します。したがって、この問題では、分散が大きい（赤いビームはかなり広い）一方で、バイアスが非常に低い（シアンと青の両方の曲線が互いに近い）ことがわかります。</target>
        </trans-unit>
        <trans-unit id="1125be6cb70e3f587339b27b5931d4f81c0d0d9b" translate="yes" xml:space="preserve">
          <source>The usage of &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; is described in detail in &lt;a href=&quot;../modules/kernel_approximation#kernel-approximation&quot;&gt;Kernel Approximation&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt;の使用法は、&lt;a href=&quot;../modules/kernel_approximation#kernel-approximation&quot;&gt;カーネル近似&lt;/a&gt;で詳しく説明されています。</target>
        </trans-unit>
        <trans-unit id="7012369902c9ebdfcf4a40c832356073db31ae9a" translate="yes" xml:space="preserve">
          <source>The usage of centroid distance limits the distance metric to Euclidean space.</source>
          <target state="translated">セントロイド距離の使用は、距離メトリックをユークリッド空間に限定します。</target>
        </trans-unit>
        <trans-unit id="4f2ed911398b7e07d993b089b964fc574c420c21" translate="yes" xml:space="preserve">
          <source>The usage of the &lt;a href=&quot;generated/sklearn.kernel_approximation.skewedchi2sampler#sklearn.kernel_approximation.SkewedChi2Sampler&quot;&gt;&lt;code&gt;SkewedChi2Sampler&lt;/code&gt;&lt;/a&gt; is the same as the usage described above for the &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt;. The only difference is in the free parameter, that is called \(c\). For a motivation for this mapping and the mathematical details see &lt;a href=&quot;#ls2010&quot; id=&quot;id7&quot;&gt;[LS2010]&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.kernel_approximation.skewedchi2sampler#sklearn.kernel_approximation.SkewedChi2Sampler&quot;&gt; &lt;code&gt;SkewedChi2Sampler&lt;/code&gt; &lt;/a&gt;の使用法は、RBFSamplerについて上記で説明した使用法と同じ&lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt;。唯一の違いは、\（c \）と呼ばれるfreeパラメータにあります。このマッピングの動機と数学的詳細については、&lt;a href=&quot;#ls2010&quot; id=&quot;id7&quot;&gt;[LS2010]を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="fabe9be99adc410c1907ea1daa7576972e4b19e8" translate="yes" xml:space="preserve">
          <source>The use of multi-output nearest neighbors for regression is demonstrated in &lt;a href=&quot;../auto_examples/plot_multioutput_face_completion#sphx-glr-auto-examples-plot-multioutput-face-completion-py&quot;&gt;Face completion with a multi-output estimators&lt;/a&gt;. In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of the lower half of those faces.</source>
          <target state="translated">回帰のための多出力最近傍の使用は&lt;a href=&quot;../auto_examples/plot_multioutput_face_completion#sphx-glr-auto-examples-plot-multioutput-face-completion-py&quot;&gt;、多出力推定量を使用した面補完で&lt;/a&gt;示されています。この例では、入力Xは顔の上半分のピクセルで、出力Yは顔の下半分のピクセルです。</target>
        </trans-unit>
        <trans-unit id="adaab44ae672254ed2b0f1474d8d83b9d0ff364a" translate="yes" xml:space="preserve">
          <source>The use of multi-output trees for classification is demonstrated in &lt;a href=&quot;../auto_examples/plot_multioutput_face_completion#sphx-glr-auto-examples-plot-multioutput-face-completion-py&quot;&gt;Face completion with a multi-output estimators&lt;/a&gt;. In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of the lower half of those faces.</source>
          <target state="translated">分類に多出力ツリーを使用する方法は&lt;a href=&quot;../auto_examples/plot_multioutput_face_completion#sphx-glr-auto-examples-plot-multioutput-face-completion-py&quot;&gt;、多出力推定量を使用した面補完に&lt;/a&gt;示されています。この例では、入力Xは顔の上半分のピクセルで、出力Yは顔の下半分のピクセルです。</target>
        </trans-unit>
        <trans-unit id="e18a96fd5f8412fd8540943bfa9bb84448ef6eef" translate="yes" xml:space="preserve">
          <source>The use of multi-output trees for regression is demonstrated in &lt;a href=&quot;../auto_examples/tree/plot_tree_regression_multioutput#sphx-glr-auto-examples-tree-plot-tree-regression-multioutput-py&quot;&gt;Multi-output Decision Tree Regression&lt;/a&gt;. In this example, the input X is a single real value and the outputs Y are the sine and cosine of X.</source>
          <target state="translated">回帰にマルチ出力ツリーを使用する方法は、マルチ&lt;a href=&quot;../auto_examples/tree/plot_tree_regression_multioutput#sphx-glr-auto-examples-tree-plot-tree-regression-multioutput-py&quot;&gt;出力ディシジョンツリー回帰に&lt;/a&gt;示されています。この例では、入力Xは単一の実数値で、出力YはXの正弦と余弦です。</target>
        </trans-unit>
        <trans-unit id="6d9188616eccce81f2896ac591395f1642a23655" translate="yes" xml:space="preserve">
          <source>The used categories can be found in the &lt;code&gt;categories_&lt;/code&gt; attribute.</source>
          <target state="translated">使用されている &lt;code&gt;categories_&lt;/code&gt; は、categories_属性にあります。</target>
        </trans-unit>
        <trans-unit id="ada4ba90a5864aca46a2fd4279e91da24ee2ef32" translate="yes" xml:space="preserve">
          <source>The user-provided initial means, defaults to None, If it None, means are initialized using the &lt;code&gt;init_params&lt;/code&gt; method.</source>
          <target state="translated">ユーザーが指定した初期手段は、デフォルトでNone です &lt;code&gt;init_params&lt;/code&gt; 場合は、init_paramsメソッドを使用して初期化されます。</target>
        </trans-unit>
        <trans-unit id="a1cbbec0505b4f4802950c392df8579b38a3220d" translate="yes" xml:space="preserve">
          <source>The user-provided initial precisions (inverse of the covariance matrices), defaults to None. If it None, precisions are initialized using the &amp;lsquo;init_params&amp;rsquo; method. The shape depends on &amp;lsquo;covariance_type&amp;rsquo;:</source>
          <target state="translated">ユーザー提供の初期精度（共分散行列の逆数）のデフォルトはNoneです。Noneの場合、精度は「init_params」メソッドを使用して初期化されます。形状は「covariance_type」に依存します：</target>
        </trans-unit>
        <trans-unit id="930a8f090bbd3f5ab357e6a55436cb80968a37a5" translate="yes" xml:space="preserve">
          <source>The user-provided initial weights, defaults to None. If it None, weights are initialized using the &lt;code&gt;init_params&lt;/code&gt; method.</source>
          <target state="translated">ユーザーが指定した初期の重み。デフォルトはNoneです。Noneの場合、重みは &lt;code&gt;init_params&lt;/code&gt; メソッドを使用して初期化されます。</target>
        </trans-unit>
        <trans-unit id="845cdaf2c42f719deb3141928cabec5996b4eedb" translate="yes" xml:space="preserve">
          <source>The usual covariance maximum likelihood estimate can be regularized using shrinkage. Ledoit and Wolf proposed a close formula to compute the asymptotically optimal shrinkage parameter (minimizing a MSE criterion), yielding the Ledoit-Wolf covariance estimate.</source>
          <target state="translated">通常の共分散最尤推定値は、収縮を用いて正則化することができる。Ledoit と Wolf は、漸近的に最適な収縮パラメータ(MSE 基準を最小化する)を計算するための近似式を提案し、これにより Ledoit-Wolf 共分散推定値が得られる。</target>
        </trans-unit>
        <trans-unit id="5d9f8abe9cd1fbb176b951540baae1f3defa7962" translate="yes" xml:space="preserve">
          <source>The usual covariance maximum likelihood estimate is very sensitive to the presence of outliers in the data set. In such a case, it would be better to use a robust estimator of covariance to guarantee that the estimation is resistant to &amp;ldquo;erroneous&amp;rdquo; observations in the data set.</source>
          <target state="translated">通常の共分散最尤推定値は、データセット内の外れ値の存在に非常に敏感です。このような場合は、共分散のロバスト推定量を使用して、推定がデータセット内の「誤った」観測に耐性があることを保証することをお勧めします。</target>
        </trans-unit>
        <trans-unit id="bf61b06542d3e7e313e8463ad6cb36679683eb16" translate="yes" xml:space="preserve">
          <source>The utility function &lt;a href=&quot;generated/sklearn.pipeline.make_pipeline#sklearn.pipeline.make_pipeline&quot;&gt;&lt;code&gt;make_pipeline&lt;/code&gt;&lt;/a&gt; is a shorthand for constructing pipelines; it takes a variable number of estimators and returns a pipeline, filling in the names automatically:</source>
          <target state="translated">ユーティリティ関数&lt;a href=&quot;generated/sklearn.pipeline.make_pipeline#sklearn.pipeline.make_pipeline&quot;&gt; &lt;code&gt;make_pipeline&lt;/code&gt; &lt;/a&gt;は、パイプラインを構築するための省略形です。可変数の推定器を受け取り、パイプラインを返し、名前を自動的に入力します。</target>
        </trans-unit>
        <trans-unit id="e6fa170ec90d321cecf4d3db13ca13c99c71342b" translate="yes" xml:space="preserve">
          <source>The valid distance metrics, and the function they map to, are:</source>
          <target state="translated">有効な距離メトリクスと、それらがマッピングする関数は、次のとおりです。</target>
        </trans-unit>
        <trans-unit id="7aae4519886038a4fe27266a449c263068305fb0" translate="yes" xml:space="preserve">
          <source>The value 2 has the highest score: it appears twice with weights of 1.5 and 2: the sum of these is 3.</source>
          <target state="translated">値2は最も高いスコアを持っています:それは1.5と2の重みで2回現れます:これらの合計は3です。</target>
        </trans-unit>
        <trans-unit id="cf2aaa6a69c13c7a1da598bbc487a099c24264e9" translate="yes" xml:space="preserve">
          <source>The value 4 appears three times: with uniform weights, the result is simply the mode of the distribution.</source>
          <target state="translated">値4は3回出現します:一様な重みでは、結果は単に分布のモードです。</target>
        </trans-unit>
        <trans-unit id="ee4d7bc4aea75382ac7c13c2d3ccdb7c85b05695" translate="yes" xml:space="preserve">
          <source>The value by which &lt;code&gt;|y - X'w - c|&lt;/code&gt; is scaled down.</source>
          <target state="translated">値 &lt;code&gt;|y - X'w - c|&lt;/code&gt; 縮小されます。</target>
        </trans-unit>
        <trans-unit id="208f4b20d150446e32489b5141e41ca0c231e069" translate="yes" xml:space="preserve">
          <source>The value of the inertia criterion associated with the chosen partition (if compute_labels is set to True). The inertia is defined as the sum of square distances of samples to their nearest neighbor.</source>
          <target state="translated">選択されたパーティションに関連付けられた慣性基準の値(compute_labels が True に設定されている場合).慣性は,サンプルの最も近い隣人までの二乗距離の和として定義されます.</target>
        </trans-unit>
        <trans-unit id="605e5e84334ac11a6b1bcf93811952a1f4c93232" translate="yes" xml:space="preserve">
          <source>The value of the information criteria (&amp;lsquo;aic&amp;rsquo;, &amp;lsquo;bic&amp;rsquo;) across all alphas. The alpha which has the smallest information criterion is chosen. This value is larger by a factor of &lt;code&gt;n_samples&lt;/code&gt; compared to Eqns. 2.15 and 2.16 in (Zou et al, 2007).</source>
          <target state="translated">すべてのアルファにわたる情報基準（ 'aic'、 'bic'）の値。最小の情報基準を持つアルファが選択されます。この値は、 &lt;code&gt;n_samples&lt;/code&gt; と比較してn_samples倍大きくなっています。2.15および2.16（Zou et al、2007）。</target>
        </trans-unit>
        <trans-unit id="31a3294fc25a32d76657f5de9f45e5deb67968f8" translate="yes" xml:space="preserve">
          <source>The value of the largest coefficient.</source>
          <target state="translated">最大係数の値。</target>
        </trans-unit>
        <trans-unit id="35a39b75c4cf1051868175c0eb7c4d08a5d8c4c6" translate="yes" xml:space="preserve">
          <source>The value of the smallest coefficient.</source>
          <target state="translated">最小の係数の値。</target>
        </trans-unit>
        <trans-unit id="d764819e9c8551a1ae19a24b5d4cd3ac77d8b2aa" translate="yes" xml:space="preserve">
          <source>The values corresponding the quantiles of reference.</source>
          <target state="translated">基準となるクォンタイルに対応する値。</target>
        </trans-unit>
        <trans-unit id="e64c591fdc6bfea9ce8a9d182cdb9d3354d8a90b" translate="yes" xml:space="preserve">
          <source>The values listed by the ValueError exception correspond to the functions measuring prediction accuracy described in the following sections. The scorer objects for those functions are stored in the dictionary &lt;code&gt;sklearn.metrics.SCORERS&lt;/code&gt;.</source>
          <target state="translated">ValueError例外によってリストされる値は、次のセクションで説明する予測精度を測定する関数に対応しています。これらの関数のスコアラーオブジェクトは、辞書 &lt;code&gt;sklearn.metrics.SCORERS&lt;/code&gt; に格納されています。</target>
        </trans-unit>
        <trans-unit id="ad8b80eff2782593fcb8941c5fc504eacc683633" translate="yes" xml:space="preserve">
          <source>The values of the parameter that will be evaluated.</source>
          <target state="translated">評価されるパラメータの値。</target>
        </trans-unit>
        <trans-unit id="896e61b11bd1cccb5014a25cacfbd49baded2da1" translate="yes" xml:space="preserve">
          <source>The values to be assigned to each cluster of samples</source>
          <target state="translated">サンプルの各クラスタに割り当てられる値</target>
        </trans-unit>
        <trans-unit id="de1379c1aa59c6a0f9d415d70cec6205d70e58b1" translate="yes" xml:space="preserve">
          <source>The variance for each feature in the training set. Used to compute &lt;code&gt;scale_&lt;/code&gt;. Equal to &lt;code&gt;None&lt;/code&gt; when &lt;code&gt;with_std=False&lt;/code&gt;.</source>
          <target state="translated">トレーニングセット内の各特徴の分散。 &lt;code&gt;scale_&lt;/code&gt; の計算に使用されます。 &lt;code&gt;with_std=False&lt;/code&gt; の場合は &lt;code&gt;None&lt;/code&gt; と同じです。</target>
        </trans-unit>
        <trans-unit id="6909f327165fbbe5fe9d7a24773b9839e3b76030" translate="yes" xml:space="preserve">
          <source>The variance of the training samples transformed by a projection to each component.</source>
          <target state="translated">各成分への射影によって変換された訓練サンプルの分散。</target>
        </trans-unit>
        <trans-unit id="73778e0f44de95a7d306fdc5c4bc68ceb4bf8f71" translate="yes" xml:space="preserve">
          <source>The varying values of the coefficients along the path. It is not present if the &lt;code&gt;fit_path&lt;/code&gt; parameter is &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">パスに沿った係数の変動値。 &lt;code&gt;fit_path&lt;/code&gt; パラメータが &lt;code&gt;False&lt;/code&gt; の場合は存在しません。</target>
        </trans-unit>
        <trans-unit id="f193fe45abdd49c251c120544e6bc124439f7f88" translate="yes" xml:space="preserve">
          <source>The vector \(h_i\) is called &amp;ldquo;latent&amp;rdquo; because it is unobserved. \(\epsilon\) is considered a noise term distributed according to a Gaussian with mean 0 and covariance \(\Psi\) (i.e. \(\epsilon \sim \mathcal{N}(0, \Psi)\)), \(\mu\) is some arbitrary offset vector. Such a model is called &amp;ldquo;generative&amp;rdquo; as it describes how \(x_i\) is generated from \(h_i\). If we use all the \(x_i\)&amp;lsquo;s as columns to form a matrix \(\mathbf{X}\) and all the \(h_i\)&amp;lsquo;s as columns of a matrix \(\mathbf{H}\) then we can write (with suitably defined \(\mathbf{M}\) and \(\mathbf{E}\)):</source>
          <target state="translated">ベクトル\（h_i \）は観測されないため、「潜在」と呼ばれます。\（\ epsilon \）は、平均が0、共分散が\（\ Psi \）のガウス分布に従って分布したノイズ項と見なされます（つまり、\（\ epsilon \ sim \ mathcal {N}（0、\ Psi）\））、 \（\ mu \）は任意のオフセットベクトルです。このようなモデルは、\（h_i \）から\（x_i \）がどのように生成されるかを表すため、「生成的」と呼ばれます。すべての\（x_i \）を列として使用して行列\（\ mathbf {X} \）を作成し、すべての\（h_i \）を行列\（\ mathbf {H} \の列として使用する場合）次に、（適切に定義された\（\ mathbf {M} \）および\（\ mathbf {E} \）を使用して）次のように記述できます。</target>
        </trans-unit>
        <trans-unit id="4545cfee48c8d7d2034c72caee40a8d82cfcf4ce" translate="yes" xml:space="preserve">
          <source>The verbose setting on the MiniBatchKMeans enables us to see that some clusters are reassigned during the successive calls to partial-fit. This is because the number of patches that they represent has become too low, and it is better to choose a random new cluster.</source>
          <target state="translated">MiniBatchKMeansの冗長な設定により、部分フィットの連続した呼び出しの間に、いくつかのクラスタが再割り当てされることがわかります。これは、それらが表現するパッチの数が少なくなりすぎたためで、ランダムな新しいクラスタを選択する方が良いからです。</target>
        </trans-unit>
        <trans-unit id="e4c09c360935c392206a8639f0a0b8f4c48b519d" translate="yes" xml:space="preserve">
          <source>The verbosity level</source>
          <target state="translated">冗長性のレベル</target>
        </trans-unit>
        <trans-unit id="4b4be8a44f0a986b95a00a99e1db7cc81cee43d8" translate="yes" xml:space="preserve">
          <source>The verbosity level.</source>
          <target state="translated">冗長性のレベル。</target>
        </trans-unit>
        <trans-unit id="bca220a25d0b85cbc18bcc37bc0c66babd623c39" translate="yes" xml:space="preserve">
          <source>The verbosity level. The default, zero, means silent mode.</source>
          <target state="translated">冗長度のレベル。デフォルトのゼロは、サイレントモードを意味します。</target>
        </trans-unit>
        <trans-unit id="b790263c39903969cb477edb620406690c93cb40" translate="yes" xml:space="preserve">
          <source>The verbosity level: if non zero, progress messages are printed. Above 50, the output is sent to stdout. The frequency of the messages increases with the verbosity level. If it more than 10, all iterations are reported.</source>
          <target state="translated">冗長度レベル:ゼロでない場合、進行状況のメッセージを出力します。50を超えると、出力は標準出力に送られます。メッセージの頻度は冗長度に応じて増加します。10を超えると、すべての繰り返しが報告されます。</target>
        </trans-unit>
        <trans-unit id="ac67f0eccad920e701e068f47d28e090c55e23b1" translate="yes" xml:space="preserve">
          <source>The verbosity mode of the function. By default that of the memory object is used.</source>
          <target state="translated">関数の冗長モード。デフォルトではメモリオブジェクトの冗長モードが使用されます。</target>
        </trans-unit>
        <trans-unit id="31fbaba32a3bec24c49d9ccad5d6e7edcbc904dc" translate="yes" xml:space="preserve">
          <source>The versions of scikit-learn and its dependencies</source>
          <target state="translated">scikit-learn のバージョンとその依存関係</target>
        </trans-unit>
        <trans-unit id="6a1686cc9631522f215a91033f5d185a9b750f85" translate="yes" xml:space="preserve">
          <source>The vocabulary extracted by this vectorizer is hence much bigger and can now resolve ambiguities encoded in local positioning patterns:</source>
          <target state="translated">このベクタライザによって抽出された語彙はより大きくなり、局所的な位置決めパターンで符号化された曖昧さを解決することができるようになりました。</target>
        </trans-unit>
        <trans-unit id="16d6455593e440b1ece6b5d9b609b990b16728ff" translate="yes" xml:space="preserve">
          <source>The weighted average probabilities for a sample would then be calculated as follows:</source>
          <target state="translated">その後、標本の加重平均確率は次のように計算されます。</target>
        </trans-unit>
        <trans-unit id="cf3727918257ef88c9160136d7b98b7188c886ca" translate="yes" xml:space="preserve">
          <source>The weighted impurity decrease equation is the following:</source>
          <target state="translated">加重不純物減少式は以下の通りである。</target>
        </trans-unit>
        <trans-unit id="b71363fa16752021c27a44ccc8c03e740b0054e3" translate="yes" xml:space="preserve">
          <source>The weights \(w\) of the model can be access:</source>
          <target state="translated">モデルの重みにアクセスできるようになりました。</target>
        </trans-unit>
        <trans-unit id="fd37cf293f530a725eccba4fc386300500a89671" translate="yes" xml:space="preserve">
          <source>The weights for each observation in X. If None, all observations are assigned equal weight (default: None)</source>
          <target state="translated">Noneの場合、すべてのオブザベーションに等しい重みが割り当てられる (デフォルト:None)。</target>
        </trans-unit>
        <trans-unit id="0fd9c75eb2401f4a1ca174b965b1602b9d82941e" translate="yes" xml:space="preserve">
          <source>The weights of each feature computed by the &lt;code&gt;fit&lt;/code&gt; method call are stored in a model attribute:</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; メソッド呼び出しによって計算された各特徴の重みは、モデル属性に格納されます。</target>
        </trans-unit>
        <trans-unit id="44235eb4c2d368f2a7e225131bb791a1dc59a457" translate="yes" xml:space="preserve">
          <source>The weights of each mixture components.</source>
          <target state="translated">各混合成分の重さ。</target>
        </trans-unit>
        <trans-unit id="dd426f25816730ede96a98b838186f9fc1553a50" translate="yes" xml:space="preserve">
          <source>The wine dataset is a classic and very easy multi-class classification dataset.</source>
          <target state="translated">ワインデータセットは、古典的で非常に簡単な多クラス分類データセットです。</target>
        </trans-unit>
        <trans-unit id="9f15574c628488edf549f3131c3e1a8bfce37b9c" translate="yes" xml:space="preserve">
          <source>The word &amp;ldquo;article&amp;rdquo; is a significant feature, based on how often people quote previous posts like this: &amp;ldquo;In article [article ID], [name] &amp;lt;[e-mail address]&amp;gt; wrote:&amp;rdquo;</source>
          <target state="translated">「記事」という言葉は重要な特徴であり、「記事[記事ID]、[名前] &amp;lt;[電子メールアドレス]&amp;gt;が次のように書いている：」のように、以前の投稿を引用する頻度に基づいています。</target>
        </trans-unit>
        <trans-unit id="364df8b6c4c1dfcb405abf5ac32289b3f8338a10" translate="yes" xml:space="preserve">
          <source>The word &lt;em&gt;restricted&lt;/em&gt; refers to the bipartite structure of the model, which prohibits direct interaction between hidden units, or between visible units. This means that the following conditional independencies are assumed:</source>
          <target state="translated">&lt;em&gt;制限付き&lt;/em&gt;という用語は、モデルの2部構成を指し、非表示のユニット間または表示されているユニット間の直接的な相互作用を禁止します。つまり、次の条件付き独立性が想定されます。</target>
        </trans-unit>
        <trans-unit id="92b782ef9bfc8a9813d7ea0d8efb9106d8f1896e" translate="yes" xml:space="preserve">
          <source>The word boundaries-aware variant &lt;code&gt;char_wb&lt;/code&gt; is especially interesting for languages that use white-spaces for word separation as it generates significantly less noisy features than the raw &lt;code&gt;char&lt;/code&gt; variant in that case. For such languages it can increase both the predictive accuracy and convergence speed of classifiers trained using such features while retaining the robustness with regards to misspellings and word derivations.</source>
          <target state="translated">単語境界を認識するバリアント &lt;code&gt;char_wb&lt;/code&gt; は、単語の分離に空白を使用する言語では特に興味深いです。この場合、生の &lt;code&gt;char&lt;/code&gt; バリアントよりもノイズの少ない機能が大幅に生成されないためです。このような言語では、スペルミスや単語の派生に関する堅牢性を維持しながら、そのような機能を使用してトレーニングされた分類子の予測精度と収束速度の両方を向上させることができます。</target>
        </trans-unit>
        <trans-unit id="8e8bd120180a9cd30000d6c7bc4b945e5f585fc6" translate="yes" xml:space="preserve">
          <source>The worst case complexity is given by O(n^(k+2/p)) with n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii, &amp;lsquo;How slow is the k-means method?&amp;rsquo; SoCG2006)</source>
          <target state="translated">最悪の場合の複雑性は、n = n_samples、p = n_featuresのO（n ^（k + 2 / p））によって与えられます。（D. ArthurおよびS. Vassilvitskii、「k-meansメソッドはどのくらい遅いのですか？」SoCG2006）</target>
        </trans-unit>
        <trans-unit id="08b4bc51642b52ec53b43a5b2dca7586d296a859" translate="yes" xml:space="preserve">
          <source>Theil-Sen Estimator: robust multivariate regression model.</source>
          <target state="translated">Theil-Sen 推定器:ロバスト多変量回帰モデル。</target>
        </trans-unit>
        <trans-unit id="26357ed7a886288385aec0522bda7ee91031a5a9" translate="yes" xml:space="preserve">
          <source>Theil-Sen Estimators in a Multiple Linear Regression Model, 2009 Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang &lt;a href=&quot;http://home.olemiss.edu/~xdang/papers/MTSE.pdf&quot;&gt;http://home.olemiss.edu/~xdang/papers/MTSE.pdf&lt;/a&gt;</source>
          <target state="translated">多重線形回帰モデルのTheil-Sen推定量、2009 Xin Dang、Hanxiang Peng、Xueqin Wang、およびHeping Zhang &lt;a href=&quot;http://home.olemiss.edu/~xdang/papers/MTSE.pdf&quot;&gt;http://home.olemiss.edu/~xdang/papers/MTSE.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bfbe9912c49db3d4af2258ea17cbacd11611139b" translate="yes" xml:space="preserve">
          <source>Theil-Sen Regression</source>
          <target state="translated">テイルセン回帰</target>
        </trans-unit>
        <trans-unit id="43407a9ed591c227c94e72cd0fbe8ae10a8a282d" translate="yes" xml:space="preserve">
          <source>TheilSen is good for small outliers, both in direction X and y, but has a break point above which it performs worse than OLS.</source>
          <target state="translated">TheilSen は、方向 X と方向 y の両方で、小さな外れ値には適していますが、それ以上のブレークポイントがあり、OLS よりもパフォーマンスが悪くなります。</target>
        </trans-unit>
        <trans-unit id="bae71614b2af83560543a8e9bca47722a78d80c8" translate="yes" xml:space="preserve">
          <source>Their harmonic mean called &lt;strong&gt;V-measure&lt;/strong&gt; is computed by &lt;a href=&quot;generated/sklearn.metrics.v_measure_score#sklearn.metrics.v_measure_score&quot;&gt;&lt;code&gt;v_measure_score&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">&lt;strong&gt;V-measure&lt;/strong&gt;と呼ばれる調和平均は、&lt;a href=&quot;generated/sklearn.metrics.v_measure_score#sklearn.metrics.v_measure_score&quot;&gt; &lt;code&gt;v_measure_score&lt;/code&gt; &lt;/a&gt;によって計算されます。</target>
        </trans-unit>
        <trans-unit id="de0b6ab722b9ec4be95cb7086fea273ea373e1de" translate="yes" xml:space="preserve">
          <source>Then fire an ipython shell and run the work-in-progress script with:</source>
          <target state="translated">次にipythonシェルを起動して、work-in-progressスクリプトを実行します。</target>
        </trans-unit>
        <trans-unit id="8f78bb6cc31961e9507c2d8e418f53fe822a9ecd" translate="yes" xml:space="preserve">
          <source>Then one can show that to classify a data point after scaling is equivalent to finding the estimated class mean \(\mu^*_k\) which is closest to the data point in the Euclidean distance. But this can be done just as well after projecting on the \(K-1\) affine subspace \(H_K\) generated by all the \(\mu^*_k\) for all classes. This shows that, implicit in the LDA classifier, there is a dimensionality reduction by linear projection onto a \(K-1\) dimensional space.</source>
          <target state="translated">そうすると、スケーリング後のデータ点を分類することは、ユークリッド距離で最も近いクラス平均を見つけることと等価であることがわかる。しかし、これは、すべてのクラスのためのすべての\(K-1\)で生成されたアフィネ部分空間に投影した後、同様に行うことができます。これは,LDA 分類器には,次元空間への線形投影による次元削減があることを示している.</target>
        </trans-unit>
        <trans-unit id="37e493a483dcc9fefbfec81c47f8c835f7340e3f" translate="yes" xml:space="preserve">
          <source>Then the Davies-Bouldin index is defined as:</source>
          <target state="translated">すると、Davies-Bouldin指数は次のように定義されます。</target>
        </trans-unit>
        <trans-unit id="54059dffef3b64a1ba00cd6fbe5ba85d85229195" translate="yes" xml:space="preserve">
          <source>Then the metrics are defined as:</source>
          <target state="translated">すると、メトリクスは次のように定義されます。</target>
        </trans-unit>
        <trans-unit id="8da3ae858e8aee4768b456c38c7b9a98b999a912" translate="yes" xml:space="preserve">
          <source>Then the multiclass MCC is defined as:</source>
          <target state="translated">そうすると、マルチクラスMCCは次のように定義されます。</target>
        </trans-unit>
        <trans-unit id="826d143749061228ef1caa51bd344b5a543a3ad8" translate="yes" xml:space="preserve">
          <source>Then the rows of \(Z\) are clustered using &lt;a href=&quot;clustering#k-means&quot;&gt;k-means&lt;/a&gt;. The first &lt;code&gt;n_rows&lt;/code&gt; labels provide the row partitioning, and the remaining &lt;code&gt;n_columns&lt;/code&gt; labels provide the column partitioning.</source>
          <target state="translated">次に、\（Z \）の行が&lt;a href=&quot;clustering#k-means&quot;&gt;k-means&lt;/a&gt;を使用してクラスター化されます。最初の &lt;code&gt;n_rows&lt;/code&gt; ラベルは行パーティションを提供し、残りの &lt;code&gt;n_columns&lt;/code&gt; ラベルは列パーティションを提供します。</target>
        </trans-unit>
        <trans-unit id="a7548beecedd6ae525f17db272cd47fef5592b2e" translate="yes" xml:space="preserve">
          <source>Then, applying the Euclidean (L2) norm, we obtain the following tf-idfs for document 1:</source>
          <target state="translated">そして、ユークリッド(L2)ノルムを適用すると、以下の文書1のtf-idfsが得られる。</target>
        </trans-unit>
        <trans-unit id="743e0b74d42270bf2752b1121cf66d48c177b1df" translate="yes" xml:space="preserve">
          <source>Then, the &lt;code&gt;raw_X&lt;/code&gt; to be fed to &lt;code&gt;FeatureHasher.transform&lt;/code&gt; can be constructed using:</source>
          <target state="translated">次に、 &lt;code&gt;raw_X&lt;/code&gt; にフィードされる &lt;code&gt;FeatureHasher.transform&lt;/code&gt; は、以下を使用して構築できます。</target>
        </trans-unit>
        <trans-unit id="4665e6f004c55fa84e60f1409a5b03c15037dd8c" translate="yes" xml:space="preserve">
          <source>Theoretical bounds</source>
          <target state="translated">理論的な境界</target>
        </trans-unit>
        <trans-unit id="3ebe4a41a0b35192395aac8a80293b4ff462a23b" translate="yes" xml:space="preserve">
          <source>There are 3 different APIs for evaluating the quality of a model&amp;rsquo;s predictions:</source>
          <target state="translated">モデルの予測の品質を評価するための3つの異なるAPIがあります。</target>
        </trans-unit>
        <trans-unit id="aa910de5e10f2b04644e63996efaedb310779ee6" translate="yes" xml:space="preserve">
          <source>There are a number of ways to convert between a distance metric and a similarity measure, such as a kernel. Let &lt;code&gt;D&lt;/code&gt; be the distance, and &lt;code&gt;S&lt;/code&gt; be the kernel:</source>
          <target state="translated">カーネルなど、距離メトリックと類似性メジャーの間で変換する方法はいくつかあります。してみましょう &lt;code&gt;D&lt;/code&gt; は距離であり、 &lt;code&gt;S&lt;/code&gt; は、カーネルのこと：</target>
        </trans-unit>
        <trans-unit id="daf4e822cec5d40489080bfb83cea014bb21271c" translate="yes" xml:space="preserve">
          <source>There are also a couple of cons (vs using a CountVectorizer with an in-memory vocabulary):</source>
          <target state="translated">また、いくつかの欠点もあります(対メモリ内の語彙を持つCountVectorizerの使用)。</target>
        </trans-unit>
        <trans-unit id="17151dd0dcece68b8abecb55a500335bd2f90b73" translate="yes" xml:space="preserve">
          <source>There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.</source>
          <target state="translated">XORやパリティ、マルチプレクサの問題など、決定木では表現しにくいため、学習しにくい概念があります。</target>
        </trans-unit>
        <trans-unit id="fbd19b439fe6ba80531cd23629fc7a9a883a8dcf" translate="yes" xml:space="preserve">
          <source>There are different things to keep in mind when dealing with data corrupted by outliers:</source>
          <target state="translated">外れ値によって破損したデータを扱う際には、さまざまな点に注意が必要です。</target>
        </trans-unit>
        <trans-unit id="0fcc62fa36cf231416a60ac162100095676c743b" translate="yes" xml:space="preserve">
          <source>There are many learning routines which rely on nearest neighbors at their core. One example is &lt;a href=&quot;density#kernel-density&quot;&gt;kernel density estimation&lt;/a&gt;, discussed in the &lt;a href=&quot;density#density-estimation&quot;&gt;density estimation&lt;/a&gt; section.</source>
          <target state="translated">中心にある最近傍に依存する多くの学習ルーチンがあります。1つの例は、&lt;a href=&quot;density#density-estimation&quot;&gt;密度推定&lt;/a&gt;セクションで説明する&lt;a href=&quot;density#kernel-density&quot;&gt;カーネル密度推定&lt;/a&gt;です。</target>
        </trans-unit>
        <trans-unit id="84513801465c2346e62e2a9ecaecce1cb0c3994b" translate="yes" xml:space="preserve">
          <source>There are several known issues in our provided &amp;lsquo;english&amp;rsquo; stop word list. See &lt;a href=&quot;#nqy18&quot; id=&quot;id5&quot;&gt;[NQY18]&lt;/a&gt;.</source>
          <target state="translated">提供されている「英語」のストップワードリストには、いくつかの既知の問題があります。&lt;a href=&quot;#nqy18&quot; id=&quot;id5&quot;&gt;[NQY18]を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="87d60889b216a9f6a8543438325d8902e749b92a" translate="yes" xml:space="preserve">
          <source>There are ten different images of each of 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement).</source>
          <target state="translated">40人の個性的な被写体のそれぞれについて、10種類の画像が用意されています。被験者によっては、照明、顔の表情(目を開いている/閉じている、笑っている/笑っていない)、顔の細部(眼鏡/眼鏡なし)を変えて、異なる時間に撮影されたものもあります。すべての画像は、暗い均質な背景の下で、被験者を正面から見て直立した状態で撮影されました(多少の横移動は許容範囲内)。</target>
        </trans-unit>
        <trans-unit id="840b342f8bab010ed9a33830388b79c022d751bb" translate="yes" xml:space="preserve">
          <source>There are three different implementations of Support Vector Regression: &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt;&lt;code&gt;NuSVR&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;LinearSVR&lt;/code&gt;&lt;/a&gt;. &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;LinearSVR&lt;/code&gt;&lt;/a&gt; provides a faster implementation than &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; but only considers linear kernels, while &lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt;&lt;code&gt;NuSVR&lt;/code&gt;&lt;/a&gt; implements a slightly different formulation than &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;LinearSVR&lt;/code&gt;&lt;/a&gt;. See &lt;a href=&quot;#svm-implementation-details&quot;&gt;Implementation details&lt;/a&gt; for further details.</source>
          <target state="translated">サポートベクター回帰には、&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt; &lt;code&gt;NuSVR&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt; &lt;code&gt;LinearSVR&lt;/code&gt; の&lt;/a&gt; 3つの異なる実装があります。&lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt; &lt;code&gt;LinearSVR&lt;/code&gt; &lt;/a&gt;は&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt;より高速な実装を提供しますが、線形カーネルのみを考慮しますが、&lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt; &lt;code&gt;NuSVR&lt;/code&gt; &lt;/a&gt;は&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt;および&lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt; &lt;code&gt;LinearSVR&lt;/code&gt; &lt;/a&gt;とは少し異なる定式化を実装します。詳細については、&lt;a href=&quot;#svm-implementation-details&quot;&gt;実装の詳細&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="6325cb8ad2d750db2de1f50457999e3ed60c95ad" translate="yes" xml:space="preserve">
          <source>There are three main kinds of dataset interfaces that can be used to get datasets depending on the desired type of dataset.</source>
          <target state="translated">データセットの種類に応じて、データセットを取得するために使用できる主に3種類のデータセットインタフェースがあります。</target>
        </trans-unit>
        <trans-unit id="8d867308d6cfc04930e6d66867b250110233d07e" translate="yes" xml:space="preserve">
          <source>There are two options to assign labels:</source>
          <target state="translated">ラベルを割り当てるには2つのオプションがあります。</target>
        </trans-unit>
        <trans-unit id="60a8f9c96bc7f93958ce08005db84d1ac5e8a2b4" translate="yes" xml:space="preserve">
          <source>There are two ways of evaluating a biclustering result: internal and external. Internal measures, such as cluster stability, rely only on the data and the result themselves. Currently there are no internal bicluster measures in scikit-learn. External measures refer to an external source of information, such as the true solution. When working with real data the true solution is usually unknown, but biclustering artificial data may be useful for evaluating algorithms precisely because the true solution is known.</source>
          <target state="translated">バイクラスタリングの結果を評価するには、内部的なものと外部的なものの2つの方法があります。クラスタの安定性のような内部的な評価は、データと結果そのものにのみ依存します。現在のところ、scikit-learnには内部的なバイクラスタリングの評価方法はありません。外部指標は、真の解のような外部の情報源を参照します。実際のデータを扱う場合、真の解は通常未知ですが、人工的なデータの二重クラスタリングは、真の解が既知であるため、アルゴリズムの評価に役立つかもしれません。</target>
        </trans-unit>
        <trans-unit id="21ade7db23177cbafdee1241d820ab218814e247" translate="yes" xml:space="preserve">
          <source>There are two ways to specify multiple scoring metrics for the &lt;code&gt;scoring&lt;/code&gt; parameter:</source>
          <target state="translated">&lt;code&gt;scoring&lt;/code&gt; パラメータに複数のスコアリングメトリックを指定するには、2つの方法があります。</target>
        </trans-unit>
        <trans-unit id="cdf4947857438b0c51097ba679415b8309e576f2" translate="yes" xml:space="preserve">
          <source>There exists two types of MDS algorithm: metric and non metric. In the scikit-learn, the class &lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt;&lt;code&gt;MDS&lt;/code&gt;&lt;/a&gt; implements both. In Metric MDS, the input similarity matrix arises from a metric (and thus respects the triangular inequality), the distances between output two points are then set to be as close as possible to the similarity or dissimilarity data. In the non-metric version, the algorithms will try to preserve the order of the distances, and hence seek for a monotonic relationship between the distances in the embedded space and the similarities/dissimilarities.</source>
          <target state="translated">MDSアルゴリズムには、メトリックと非メトリックの2つのタイプがあります。scikit-learnでは、&lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt; &lt;code&gt;MDS&lt;/code&gt; &lt;/a&gt;クラスが両方を実装します。メトリックMDSでは、入力類似度行列はメトリックから生じ（したがって、三角形の不等式を尊重します）、出力の2点間の距離は、類似度または非類似度のデータにできるだけ近くなるように設定されます。非メトリックバージョンでは、アルゴリズムは距離の順序を維持しようとするため、埋め込み空間の距離と類似性/非類似性の間の単調な関係を求めます。</target>
        </trans-unit>
        <trans-unit id="9e945ec56932f8495741411aac1ef0391f41ea70" translate="yes" xml:space="preserve">
          <source>There is absolutely no guarantee of recovering a ground truth. First, choosing the right number of clusters is hard. Second, the algorithm is sensitive to initialization, and can fall into local minima, although scikit-learn employs several tricks to mitigate this issue.</source>
          <target state="translated">根拠のある真実を回収できる保証は絶対にありません。第一に、正しいクラスタ数を選択するのは難しいです。第二に、アルゴリズムは初期化に敏感であり、ローカルミニマムに陥る可能性があります。</target>
        </trans-unit>
        <trans-unit id="a5fa0b690cccf03ea1d32deadc1c15c3039ee96d" translate="yes" xml:space="preserve">
          <source>There is built-in support for sparse data given in any matrix in a format supported by &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/sparse.html&quot;&gt;scipy.sparse&lt;/a&gt;. For maximum efficiency, however, use the CSR matrix format as defined in &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html&quot;&gt;scipy.sparse.csr_matrix&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/sparse.html&quot;&gt;scipy.sparseで&lt;/a&gt;サポートされている形式のマトリックスで指定されたスパースデータの組み込みサポートがあります。ただし、最大の効率を得るには、&lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html&quot;&gt;scipy.sparse.csr_matrixで&lt;/a&gt;定義されているCSRマトリックス形式を使用します。</target>
        </trans-unit>
        <trans-unit id="7bf90eed7a42877d9e674dd4cae64f5ef3af07a9" translate="yes" xml:space="preserve">
          <source>There is no general rule to select an alpha parameter for recovery of non-zero coefficients. It can by set by cross-validation (&lt;code&gt;LassoCV&lt;/code&gt; or &lt;code&gt;LassoLarsCV&lt;/code&gt;), though this may lead to under-penalized models: including a small number of non-relevant variables is not detrimental to prediction score. BIC (&lt;code&gt;LassoLarsIC&lt;/code&gt;) tends, on the opposite, to set high values of alpha.</source>
          <target state="translated">ゼロ以外の係数を回復するためのアルファパラメーターを選択する一般的な規則はありません。クロス検証（ &lt;code&gt;LassoCV&lt;/code&gt; または &lt;code&gt;LassoLarsCV&lt;/code&gt; ）によって設定できますが、これはペナルティが不十分なモデルにつながる可能性があります。関連性のない変数を少数含めても、予測スコアに悪影響はありません。 BIC（ &lt;code&gt;LassoLarsIC&lt;/code&gt; ）は、逆にアルファの値を高く設定する傾向があります。</target>
        </trans-unit>
        <trans-unit id="ff0ab54a4c7a8a1cf35484dbae4d9dce95d5c026" translate="yes" xml:space="preserve">
          <source>There might be a difference in the scores obtained between &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;solver=liblinear&lt;/code&gt; or &lt;code&gt;LinearSVC&lt;/code&gt; and the external liblinear library directly, when &lt;code&gt;fit_intercept=False&lt;/code&gt; and the fit &lt;code&gt;coef_&lt;/code&gt; (or) the data to be predicted are zeroes. This is because for the sample(s) with &lt;code&gt;decision_function&lt;/code&gt; zero, &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;LinearSVC&lt;/code&gt; predict the negative class, while liblinear predicts the positive class. Note that a model with &lt;code&gt;fit_intercept=False&lt;/code&gt; and having many samples with &lt;code&gt;decision_function&lt;/code&gt; zero, is likely to be a underfit, bad model and you are advised to set &lt;code&gt;fit_intercept=True&lt;/code&gt; and increase the intercept_scaling.</source>
          <target state="translated">間で得られたスコアに差がある場合があります&lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt;で &lt;code&gt;solver=liblinear&lt;/code&gt; または &lt;code&gt;LinearSVC&lt;/code&gt; とき、直接外部liblinearライブラリ &lt;code&gt;fit_intercept=False&lt;/code&gt; とフィット &lt;code&gt;coef_&lt;/code&gt; （または）データがゼロをしていると予測されます。有する試料（S）のためだからである &lt;code&gt;decision_function&lt;/code&gt; ゼロ、&lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt;および &lt;code&gt;LinearSVC&lt;/code&gt; は liblinearが正のクラスを予測しながら、負のクラスを予測します。モデルのことを注意 &lt;code&gt;fit_intercept=False&lt;/code&gt; として多くのサンプル持つ &lt;code&gt;decision_function&lt;/code&gt; ゼロは、underfit、悪いモデルである可能性が高いですし、セットすることをお勧めします &lt;code&gt;fit_intercept=True&lt;/code&gt; にして、intercept_scalingを増やします。</target>
        </trans-unit>
        <trans-unit id="4a4216d2986ca1c413a45927f7f8dc07ca811204" translate="yes" xml:space="preserve">
          <source>Therefore, a logarithmic (&lt;code&gt;np.log1p&lt;/code&gt;) and an exponential function (&lt;code&gt;np.expm1&lt;/code&gt;) will be used to transform the targets before training a linear regression model and using it for prediction.</source>
          <target state="translated">したがって、線形回帰モデルをトレーニングして予測に使用する前に、対数（ &lt;code&gt;np.log1p&lt;/code&gt; ）と指数関数（ &lt;code&gt;np.expm1&lt;/code&gt; ）を使用してターゲットを変換します。</target>
        </trans-unit>
        <trans-unit id="6912f2dbecf0d873a7ad1021b00fc015a0232427" translate="yes" xml:space="preserve">
          <source>These are transformers that are not intended to be used on features, only on supervised learning targets. See also &lt;a href=&quot;compose#transformed-target-regressor&quot;&gt;Transforming target in regression&lt;/a&gt; if you want to transform the prediction target for learning, but evaluate the model in the original (untransformed) space.</source>
          <target state="translated">これらは、監視対象の学習ターゲットでのみ機能で使用することを目的としたトランスフォーマーではありません。学習のために予測ターゲットを変換したいが、元の（変換されていない）空間でモデルを評価する場合は&lt;a href=&quot;compose#transformed-target-regressor&quot;&gt;、回帰でのターゲットの変換&lt;/a&gt;も参照してください。</target>
        </trans-unit>
        <trans-unit id="9c615abfdf912d61f49e70314e0bacb6d3789d48" translate="yes" xml:space="preserve">
          <source>These classifiers are attractive because they have closed-form solutions that can be easily computed, are inherently multiclass, have proven to work well in practice, and have no hyperparameters to tune.</source>
          <target state="translated">これらの分類器は,簡単に計算できる閉じた形の解を持ち,本質的に多クラスであり,実際にうまく動作することが証明されており,調整するハイパーパラメタがないことが魅力的です.</target>
        </trans-unit>
        <trans-unit id="fde55c65b8197fd0cbaa58300d107768af9ed389" translate="yes" xml:space="preserve">
          <source>These constraint are useful to impose a certain local structure, but they also make the algorithm faster, especially when the number of the samples is high.</source>
          <target state="translated">これらの制約は、特定の局所構造を課すのに便利ですが、特にサンプル数が多い場合にはアルゴリズムの高速化にもつながります。</target>
        </trans-unit>
        <trans-unit id="ebd210a6b7ae21f6cafc3c41203edaaa46e25728" translate="yes" xml:space="preserve">
          <source>These datasets are useful to quickly illustrate the behavior of the various algorithms implemented in scikit-learn. They are however often too small to be representative of real world machine learning tasks.</source>
          <target state="translated">これらのデータセットは、scikit-learnで実装された様々なアルゴリズムの動作を素早く説明するのに便利です。しかし、これらのデータセットは現実の機械学習タスクを代表するには小さすぎます。</target>
        </trans-unit>
        <trans-unit id="8d2ab26191aa60fb366e6a03a6fce9c7d01208ba" translate="yes" xml:space="preserve">
          <source>These environment variables should be set before importing scikit-learn.</source>
          <target state="translated">これらの環境変数は scikit-learn をインポートする前に設定しておく必要があります。</target>
        </trans-unit>
        <trans-unit id="b821932825baa77bb11f8b1f95c8cc38b1d75bf5" translate="yes" xml:space="preserve">
          <source>These estimators are called similarly to their counterparts, with &amp;lsquo;CV&amp;rsquo; appended to their name.</source>
          <target state="translated">これらの推定量は、対応するものと同様に呼び出され、名前に「CV」が追加されます。</target>
        </trans-unit>
        <trans-unit id="8bef6d4f1ba3e716c219b98d63998eb428a7438d" translate="yes" xml:space="preserve">
          <source>These families of algorithms are useful to find linear relations between two multivariate datasets: the &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;Y&lt;/code&gt; arguments of the &lt;code&gt;fit&lt;/code&gt; method are 2D arrays.</source>
          <target state="translated">これらのアルゴリズムファミリーは、2つの多変量データセット間の線形関係を見つけるのに役立ちます &lt;code&gt;fit&lt;/code&gt; メソッドの &lt;code&gt;X&lt;/code&gt; および &lt;code&gt;Y&lt;/code&gt; 引数は2D配列です。</target>
        </trans-unit>
        <trans-unit id="e90c7c20b495d718d6ee1761b52661a07d31f2ac" translate="yes" xml:space="preserve">
          <source>These figures aid in illustrating how a point cloud can be very flat in one direction&amp;ndash;which is where PCA comes in to choose a direction that is not flat.</source>
          <target state="translated">これらの図は、ポイントクラウドを一方向に非常に平坦にする方法を示しています。これは、PCAが平坦でない方向を選択する場所です。</target>
        </trans-unit>
        <trans-unit id="5b3c06102d71e9aa21ce3635f214fb7544bfe93e" translate="yes" xml:space="preserve">
          <source>These functions have an &lt;code&gt;multioutput&lt;/code&gt; keyword argument which specifies the way the scores or losses for each individual target should be averaged. The default is &lt;code&gt;'uniform_average'&lt;/code&gt;, which specifies a uniformly weighted mean over outputs. If an &lt;code&gt;ndarray&lt;/code&gt; of shape &lt;code&gt;(n_outputs,)&lt;/code&gt; is passed, then its entries are interpreted as weights and an according weighted average is returned. If &lt;code&gt;multioutput&lt;/code&gt; is &lt;code&gt;'raw_values'&lt;/code&gt; is specified, then all unaltered individual scores or losses will be returned in an array of shape &lt;code&gt;(n_outputs,)&lt;/code&gt;.</source>
          <target state="translated">これらの関数には複数の &lt;code&gt;multioutput&lt;/code&gt; キーワード引数があり、個々のターゲットのスコアまたは損失を平均化する方法を指定します。デフォルトは &lt;code&gt;'uniform_average'&lt;/code&gt; で、出力の均一な加重平均を指定します。場合 &lt;code&gt;ndarray&lt;/code&gt; 形状の &lt;code&gt;(n_outputs,)&lt;/code&gt; 渡され、そのエントリが重みとして解釈され、係る加重平均が返されます。場合 &lt;code&gt;multioutput&lt;/code&gt; ある &lt;code&gt;'raw_values'&lt;/code&gt; 指定されている、すべての不変の個々のスコアまたは損失は、形状の配列に返される &lt;code&gt;(n_outputs,)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="8f744234c4fef479ca52103694dddbbb39569d67" translate="yes" xml:space="preserve">
          <source>These functions return a tuple &lt;code&gt;(X, y)&lt;/code&gt; consisting of a &lt;code&gt;n_samples&lt;/code&gt; * &lt;code&gt;n_features&lt;/code&gt; numpy array &lt;code&gt;X&lt;/code&gt; and an array of length &lt;code&gt;n_samples&lt;/code&gt; containing the targets &lt;code&gt;y&lt;/code&gt;.</source>
          <target state="translated">これらの関数は、 &lt;code&gt;n_samples&lt;/code&gt; * &lt;code&gt;n_features&lt;/code&gt; numpy配列 &lt;code&gt;X&lt;/code&gt; と、ターゲット &lt;code&gt;y&lt;/code&gt; を含む長さ &lt;code&gt;n_samples&lt;/code&gt; の配列で構成されるタプル &lt;code&gt;(X, y)&lt;/code&gt; 返します。</target>
        </trans-unit>
        <trans-unit id="7ea4b087a47ece8eb67eda1c13d069b3e5f40d70" translate="yes" xml:space="preserve">
          <source>These generators produce a matrix of features and corresponding discrete targets.</source>
          <target state="translated">これらの生成器は、特徴量の行列とそれに対応する離散ターゲットを生成します。</target>
        </trans-unit>
        <trans-unit id="7870342f67a34d74383fe781d3e91593569275ef" translate="yes" xml:space="preserve">
          <source>These images how similar features are merged together using feature agglomeration.</source>
          <target state="translated">これらの画像は、特徴の凝集を利用して類似した特徴をどのように結合しているかを示しています。</target>
        </trans-unit>
        <trans-unit id="2f6550a6d877a222d311c7f7d2e4efbab68b15f3" translate="yes" xml:space="preserve">
          <source>These matrices can be used to impose connectivity in estimators that use connectivity information, such as Ward clustering (&lt;a href=&quot;clustering#hierarchical-clustering&quot;&gt;Hierarchical clustering&lt;/a&gt;), but also to build precomputed kernels, or similarity matrices.</source>
          <target state="translated">これらの行列を使用して、ウォードクラスタリング（&lt;a href=&quot;clustering#hierarchical-clustering&quot;&gt;階層的クラスタリング&lt;/a&gt;）などの接続性情報を使用する推定器に接続性を課すことができますが、事前計算されたカーネルまたは類似性行列を構築することもできます。</target>
        </trans-unit>
        <trans-unit id="f1476efa19922a5a7b34be362fc1e06a3ae81118" translate="yes" xml:space="preserve">
          <source>These metrics &lt;strong&gt;require the knowledge of the ground truth classes&lt;/strong&gt; while almost never available in practice or requires manual assignment by human annotators (as in the supervised learning setting).</source>
          <target state="translated">これらのメトリック&lt;strong&gt;は、グラウンドトゥルースクラスの知識を必要とし&lt;/strong&gt;ます&lt;strong&gt;が、&lt;/strong&gt;実際にはほとんど利用できません。または、教師付き学習設定のように、人間のアノテーターが手動で割り当てる必要があります。</target>
        </trans-unit>
        <trans-unit id="1b78634dcd7b938ef7f64bb3d2756751845f06a1" translate="yes" xml:space="preserve">
          <source>These objects take as input a scoring function that returns univariate scores and p-values (or only scores for &lt;a href=&quot;generated/sklearn.feature_selection.selectkbest#sklearn.feature_selection.SelectKBest&quot;&gt;&lt;code&gt;SelectKBest&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.feature_selection.selectpercentile#sklearn.feature_selection.SelectPercentile&quot;&gt;&lt;code&gt;SelectPercentile&lt;/code&gt;&lt;/a&gt;):</source>
          <target state="translated">これらのオブジェクトは、一変量スコアとp値（または&lt;a href=&quot;generated/sklearn.feature_selection.selectkbest#sklearn.feature_selection.SelectKBest&quot;&gt; &lt;code&gt;SelectKBest&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;generated/sklearn.feature_selection.selectpercentile#sklearn.feature_selection.SelectPercentile&quot;&gt; &lt;code&gt;SelectPercentile&lt;/code&gt; の&lt;/a&gt;スコアのみ）を返すスコアリング関数を入力として受け取ります。</target>
        </trans-unit>
        <trans-unit id="3403571e1d1dd77b054e08b8e749a4d9b5c4d316" translate="yes" xml:space="preserve">
          <source>These parameters can be accessed through the members &lt;code&gt;dual_coef_&lt;/code&gt; which holds the difference \(\alpha_i - \alpha_i^*\), &lt;code&gt;support_vectors_&lt;/code&gt; which holds the support vectors, and &lt;code&gt;intercept_&lt;/code&gt; which holds the independent term \(\rho\)</source>
          <target state="translated">これらのパラメータは、メンバーを介してアクセスすることができ &lt;code&gt;dual_coef_&lt;/code&gt; 、 - （\ alpha_i ^ * \ \ alpha_i）の違いを保持する\ &lt;code&gt;support_vectors_&lt;/code&gt; サポートベクトルを保持しており、 &lt;code&gt;intercept_&lt;/code&gt; 独立した用語\（\ロー\）を保持しています</target>
        </trans-unit>
        <trans-unit id="9254aef96f1c8727db185406da2727e74822dda9" translate="yes" xml:space="preserve">
          <source>These quantities are also related to the (\(F_1\)) score, which is defined as the harmonic mean of precision and recall.</source>
          <target state="translated">これらの量は、精度とリコールの調和平均として定義される(F\(F_1_1))スコアにも関連しています。</target>
        </trans-unit>
        <trans-unit id="8ec6209edcb97e57d934d74900289c4f7467ca16" translate="yes" xml:space="preserve">
          <source>These represent the 14 features measured at each point of the map grid. The latitude/longitude values for the grid are discussed below. Missing data is represented by the value -9999.</source>
          <target state="translated">これらは、地図グリッドの各点で測定された 14 の特徴を表している。グリッドの緯度・経度の値は以下で説明します。不足しているデータは-9999で表されます。</target>
        </trans-unit>
        <trans-unit id="e11ef00d62661e429b4b798a345a9c8d62a348e6" translate="yes" xml:space="preserve">
          <source>These steps are performed either a maximum number of times (&lt;code&gt;max_trials&lt;/code&gt;) or until one of the special stop criteria are met (see &lt;code&gt;stop_n_inliers&lt;/code&gt; and &lt;code&gt;stop_score&lt;/code&gt;). The final model is estimated using all inlier samples (consensus set) of the previously determined best model.</source>
          <target state="translated">これらのステップは、最大回数（ &lt;code&gt;max_trials&lt;/code&gt; ）、または特別な停止基準の1つが満たされるまで実行されます（ &lt;code&gt;stop_n_inliers&lt;/code&gt; および &lt;code&gt;stop_score&lt;/code&gt; を参照）。最終モデルは、以前に決定された最良のモデルのすべてのインライアサンプル（コンセンサスセット）を使用して推定されます。</target>
        </trans-unit>
        <trans-unit id="eff89650d9905b662c6df80228e926f2f144c91b" translate="yes" xml:space="preserve">
          <source>These three distances are special cases of the beta-divergence family, with \(\beta = 2, 1, 0\) respectively &lt;a href=&quot;#id15&quot; id=&quot;id8&quot;&gt;[6]&lt;/a&gt;. The beta-divergence are defined by :</source>
          <target state="translated">これらの3つの距離は、それぞれ\（\ beta = 2、1、0 \）である&amp;beta;分岐ファミリーの特殊なケースです&lt;a href=&quot;#id15&quot; id=&quot;id8&quot;&gt;[6]&lt;/a&gt;。ベータダイバージェンスは次のように定義されます。</target>
        </trans-unit>
        <trans-unit id="100dafc268c3e9d0b628da1715aed2440b14ba32" translate="yes" xml:space="preserve">
          <source>These throughputs are achieved on a single process. An obvious way to increase the throughput of your application is to spawn additional instances (usually processes in Python because of the &lt;a href=&quot;https://wiki.python.org/moin/GlobalInterpreterLock&quot;&gt;GIL&lt;/a&gt;) that share the same model. One might also add machines to spread the load. A detailed explanation on how to achieve this is beyond the scope of this documentation though.</source>
          <target state="translated">これらのスループットは、単一のプロセスで実現されます。アプリケーションのスループットを向上させる明白な方法は、同じモデルを共有する追加のインスタンス（通常は&lt;a href=&quot;https://wiki.python.org/moin/GlobalInterpreterLock&quot;&gt;GILの&lt;/a&gt;ためにPythonでプロセス）を生成することです。負荷を分散するためにマシンを追加することもできます。ただし、これを実現する方法の詳細な説明は、このドキュメントの範囲を超えています。</target>
        </trans-unit>
        <trans-unit id="e63971597c4df5f2dc150f90b566e1219b6a632a" translate="yes" xml:space="preserve">
          <source>They are not sparse, i.e., they use the whole samples/features information to perform the prediction.</source>
          <target state="translated">彼らはスパースではない、すなわち、予測を実行するためにサンプル/特徴量全体の情報を使用しています。</target>
        </trans-unit>
        <trans-unit id="26f84bd6fe103542912ebb1fb588d29515a2fd6d" translate="yes" xml:space="preserve">
          <source>They can be loaded using the following functions:</source>
          <target state="translated">これらは、以下の関数を使用してロードすることができます。</target>
        </trans-unit>
        <trans-unit id="f7ddbd9f45ee3f137b5eb656135dce7584351da0" translate="yes" xml:space="preserve">
          <source>They expose a &lt;code&gt;split&lt;/code&gt; method which accepts the input dataset to be split and yields the train/test set indices for each iteration of the chosen cross-validation strategy.</source>
          <target state="translated">これらは、 &lt;code&gt;split&lt;/code&gt; する入力データセットを受け入れ、選択された交差検証戦略の各反復のトレーニング/テストセットインデックスを生成する分割メソッドを公開します。</target>
        </trans-unit>
        <trans-unit id="03bbcc98e1d567dd0afc112fe378a99851c98226" translate="yes" xml:space="preserve">
          <source>They lose efficiency in high dimensional spaces &amp;ndash; namely when the number of features exceeds a few dozens.</source>
          <target state="translated">それらは高次元空間で効率を失います&amp;ndash;つまり、特徴の数が数十を超えるとき。</target>
        </trans-unit>
        <trans-unit id="a673690dbc33eee17ee6f3995f817097918876ff" translate="yes" xml:space="preserve">
          <source>This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile).</source>
          <target state="translated">このスケーラーは、中央値を削除し、分位範囲(デフォルトはIQR:Interquartile Range)に応じてデータをスケーリングします。IQRは、第1分位(25分位)と第3分位(75分位)の間の範囲です。</target>
        </trans-unit>
        <trans-unit id="d9994b645c162ae9e80a2c6bc1c81390f2e92325" translate="yes" xml:space="preserve">
          <source>This Warning is used in meta estimators GridSearchCV and RandomizedSearchCV and the cross-validation helper function cross_val_score to warn when there is an error while fitting the estimator.</source>
          <target state="translated">この警告は、メタ推定器GridSearchCVおよびRandomizedSearchCV、およびクロスバリデーションヘルパー関数 cross_val_scoreで使用され、推定器のフィッティング中にエラーが発生した場合に警告を発します。</target>
        </trans-unit>
        <trans-unit id="7f5e3d23312509826c13f1663d34b7e7912ac4af" translate="yes" xml:space="preserve">
          <source>This algorithm can be viewed as an instance or data reduction method, since it reduces the input data to a set of subclusters which are obtained directly from the leaves of the CFT. This reduced data can be further processed by feeding it into a global clusterer. This global clusterer can be set by &lt;code&gt;n_clusters&lt;/code&gt;. If &lt;code&gt;n_clusters&lt;/code&gt; is set to None, the subclusters from the leaves are directly read off, otherwise a global clustering step labels these subclusters into global clusters (labels) and the samples are mapped to the global label of the nearest subcluster.</source>
          <target state="translated">このアルゴリズムは、入力データをCFTのリーフから直接取得されるサブクラスターのセットに削減するため、インスタンスまたはデータ削減方法と見なすことができます。この削減されたデータは、グローバルクラスターにフィードすることでさらに処理できます。このグローバルクラスタは &lt;code&gt;n_clusters&lt;/code&gt; で設定できます。 &lt;code&gt;n_clusters&lt;/code&gt; がNoneに設定されている場合、リーフからのサブクラスターが直接読み取られます。それ以外の場合、グローバルクラスタリングステップはこれらのサブクラスターをグローバルクラスター（ラベル）にラベル付けし、サンプルは最も近いサブクラスターのグローバルラベルにマッピングされます。</target>
        </trans-unit>
        <trans-unit id="895ec6bb1e64f58ea59b9ae781fa620e703125a9" translate="yes" xml:space="preserve">
          <source>This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting &lt;a href=&quot;#r4d113ba76fc0-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;. If samples are drawn with replacement, then the method is known as Bagging &lt;a href=&quot;#r4d113ba76fc0-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt;. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces &lt;a href=&quot;#r4d113ba76fc0-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt;. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches &lt;a href=&quot;#r4d113ba76fc0-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt;.</source>
          <target state="translated">このアルゴリズムには、文献のいくつかの研究が含まれています。データセットのランダムなサブセットがサンプルのランダムなサブセットとして描画される場合、このアルゴリズムは貼り付けとして知られています&lt;a href=&quot;#r4d113ba76fc0-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;。サンプルを交換して採取する場合、その方法はバギングとして知られています&lt;a href=&quot;#r4d113ba76fc0-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt;。データセットのランダムサブセットがフィーチャのランダムサブセットとして描画される場合、この方法はランダムサブスペースとして知られています&lt;a href=&quot;#r4d113ba76fc0-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt;。最後に、ベースエスティメータがサンプルと機能の両方のサブセットに基づいて構築される場合、この方法はランダムパッチとして知られています&lt;a href=&quot;#r4d113ba76fc0-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="870122c945d57891e89c2ba931542331f1b4afdb" translate="yes" xml:space="preserve">
          <source>This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting &lt;a href=&quot;#rb1846455d0e5-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;. If samples are drawn with replacement, then the method is known as Bagging &lt;a href=&quot;#rb1846455d0e5-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt;. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces &lt;a href=&quot;#rb1846455d0e5-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt;. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches &lt;a href=&quot;#rb1846455d0e5-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt;.</source>
          <target state="translated">このアルゴリズムには、文献のいくつかの研究が含まれています。データセットのランダムなサブセットがサンプルのランダムなサブセットとして描画される場合、このアルゴリズムは貼り付けとして知られています&lt;a href=&quot;#rb1846455d0e5-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;。サンプルを交換して採取する場合、その方法はバギングとして知られています&lt;a href=&quot;#rb1846455d0e5-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt;。データセットのランダムサブセットがフィーチャのランダムサブセットとして描画される場合、この方法はランダムサブスペースとして知られています&lt;a href=&quot;#rb1846455d0e5-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt;。最後に、ベースエスティメータがサンプルと機能の両方のサブセットに基づいて構築される場合、この方法はランダムパッチとして知られています&lt;a href=&quot;#rb1846455d0e5-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="866e774dfaaba96a720c3090067c49c13cac935f" translate="yes" xml:space="preserve">
          <source>This algorithm finds a (usually very good) approximate truncated singular value decomposition using randomization to speed up the computations. It is particularly fast on large matrices on which you wish to extract only a small number of components. In order to obtain further speed up, &lt;code&gt;n_iter&lt;/code&gt; can be set &amp;lt;=2 (at the cost of loss of precision).</source>
          <target state="translated">このアルゴリズムは、計算を高速化するためにランダム化を使用して、（通常は非常に良い）切り捨てられた特異値分解を見つけます。少数のコンポーネントのみを抽出したい大きな行列では特に高速です。さらに高速化するには、 &lt;code&gt;n_iter&lt;/code&gt; を&amp;lt;= 2に設定できます（精度が低下します）。</target>
        </trans-unit>
        <trans-unit id="77977d31f5357c834112cbb27bbc98b5c64c10d7" translate="yes" xml:space="preserve">
          <source>This algorithm has constant memory complexity, on the order of &lt;code&gt;batch_size&lt;/code&gt;, enabling use of np.memmap files without loading the entire file into memory.</source>
          <target state="translated">このアルゴリズムは、 &lt;code&gt;batch_size&lt;/code&gt; のオーダーで一定のメモリ複雑度を持ち、ファイル全体をメモリにロードせずにnp.memmapファイルを使用できるようにします。</target>
        </trans-unit>
        <trans-unit id="9586aed3b1a5b6a2c44b32af5cc0558b6ad496a6" translate="yes" xml:space="preserve">
          <source>This algorithm solves the normalized cut for k=2: it is a normalized spectral clustering.</source>
          <target state="translated">このアルゴリズムは、k=2のための正規化カットを解く:それは正規化されたスペクトルクラスタリングである。</target>
        </trans-unit>
        <trans-unit id="01c65a021c885e4e00baaaa5c6652a314a97daa3" translate="yes" xml:space="preserve">
          <source>This algorithm will always use all the components it has access to, needing held-out data or information theoretical criteria to decide how many components to use in the absence of external cues.</source>
          <target state="translated">このアルゴリズムは、外部からの手がかりがない場合に使用するコンポーネントの数を決定するために、ホールドアウトされたデータや情報の理論的な基準を必要とし、常にアクセス可能なすべてのコンポーネントを使用します。</target>
        </trans-unit>
        <trans-unit id="ddf04fe856314e7dd4ddddf49f4086029e04d842" translate="yes" xml:space="preserve">
          <source>This allows better model selection than probabilistic PCA in the presence of heteroscedastic noise:</source>
          <target state="translated">これにより、異種ノイズの存在下では、確率的PCAよりも優れたモデル選択が可能になります。</target>
        </trans-unit>
        <trans-unit id="3307a2458ebbefda8ea7fe8b898077ec4cadcf2c" translate="yes" xml:space="preserve">
          <source>This also works where final estimator is &lt;code&gt;None&lt;/code&gt;: all prior transformations are applied.</source>
          <target state="translated">これは、最終的な推定量が &lt;code&gt;None&lt;/code&gt; の場合にも機能します。以前のすべての変換が適用されます。</target>
        </trans-unit>
        <trans-unit id="c63b80512d8853cd76b24210ee07543da5c60fc4" translate="yes" xml:space="preserve">
          <source>This assumption is the base of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Vector_Space_Model&quot;&gt;Vector Space Model&lt;/a&gt; often used in text classification and clustering contexts.</source>
          <target state="translated">この仮定は、テキスト分類およびクラスタリングのコンテキストでよく使用される&lt;a href=&quot;https://en.wikipedia.org/wiki/Vector_Space_Model&quot;&gt;ベクトル空間モデルの&lt;/a&gt;ベースです。</target>
        </trans-unit>
        <trans-unit id="8142b653ecd3322ad782e8a8807635d6c4c0325e" translate="yes" xml:space="preserve">
          <source>This calibration results in a lower log-loss. Note that an alternative would have been to increase the number of base estimators which would have resulted in a similar decrease in log-loss.</source>
          <target state="translated">このキャリブレーションの結果、ログロスが減少する。これに代わる方法として、基底推定量の数を増やすことも考えられるが、これはログロスを同様に減少させる結果になるだろう。</target>
        </trans-unit>
        <trans-unit id="3ba422e075fa10216e381bf46530abda4e719fab" translate="yes" xml:space="preserve">
          <source>This call requires the estimation of a p x q matrix, which may be an issue in high dimensional space.</source>
          <target state="translated">この呼び出しは,高次元空間では問題となるかもしれないp x q行列の推定を必要とします.</target>
        </trans-unit>
        <trans-unit id="345a3bd30c8553d3251f728b344d1bd100958670" translate="yes" xml:space="preserve">
          <source>This can be confirmed on a independent testing set with similar remarks:</source>
          <target state="translated">これは、似たような発言をした独立したテストセットで確認することができます。</target>
        </trans-unit>
        <trans-unit id="e60927eda9d0f07135f56fa39eaa1a8f1f9c2813" translate="yes" xml:space="preserve">
          <source>This can be done by introducing &lt;a href=&quot;https://en.wikipedia.org/wiki/Non-informative_prior#Uninformative_priors&quot;&gt;uninformative priors&lt;/a&gt; over the hyper parameters of the model. The \(\ell_{2}\) regularization used in &lt;a href=&quot;#id2&quot;&gt;Ridge Regression&lt;/a&gt; is equivalent to finding a maximum a posteriori estimation under a Gaussian prior over the parameters \(w\) with precision \(\lambda^{-1}\). Instead of setting &lt;code&gt;lambda&lt;/code&gt; manually, it is possible to treat it as a random variable to be estimated from the data.</source>
          <target state="translated">これは、モデルのハイパーパラメータに対して&lt;a href=&quot;https://en.wikipedia.org/wiki/Non-informative_prior#Uninformative_priors&quot;&gt;情報のない事前分布&lt;/a&gt;を導入することで実行できます。&lt;a href=&quot;#id2&quot;&gt;リッジ回帰で&lt;/a&gt;使用される\（\ ell_ {2} \）正則化は、ガウス分布の下で最大の事後推定を、精度\（\ lambda ^ {-1} \）でパラメーター\（w \）よりも前に見つけることと同等です。 &lt;code&gt;lambda&lt;/code&gt; 手動で設定する代わりに、データから推定されるランダム変数として扱うことができます。</target>
        </trans-unit>
        <trans-unit id="aa181018cfeb3219e1e67073f9c58ca90a0c4faa" translate="yes" xml:space="preserve">
          <source>This can be done by using the &lt;a href=&quot;generated/sklearn.model_selection.train_test_split#sklearn.model_selection.train_test_split&quot;&gt;&lt;code&gt;train_test_split&lt;/code&gt;&lt;/a&gt; utility function.</source>
          <target state="translated">これは、&lt;a href=&quot;generated/sklearn.model_selection.train_test_split#sklearn.model_selection.train_test_split&quot;&gt; &lt;code&gt;train_test_split&lt;/code&gt; &lt;/a&gt;ユーティリティ関数を使用して実行できます。</target>
        </trans-unit>
        <trans-unit id="821e5435b62e56a883478da64d6731f6479103d5" translate="yes" xml:space="preserve">
          <source>This can be set to a higher value than the actual number of features in any of the input files, but setting it to a lower value will cause an exception to be raised.</source>
          <target state="translated">これは、どの入力ファイルに含まれる実際の特徴量よりも高い値に設定することができますが、低い値に設定すると例外が発生します。</target>
        </trans-unit>
        <trans-unit id="4a0bd36c1ccd6d51b38a900236d58695657d5aff" translate="yes" xml:space="preserve">
          <source>This class allows to infer an approximate posterior distribution over the parameters of a Gaussian mixture distribution. The effective number of components can be inferred from the data.</source>
          <target state="translated">このクラスでは,ガウス混合分布のパラメータに対する近似事後分布を推論することができます.データから有効成分の数を推測することができます。</target>
        </trans-unit>
        <trans-unit id="6130cf2c7564234b715447525f16ff596ebe842e" translate="yes" xml:space="preserve">
          <source>This class can be used to cross-validate time series data samples that are observed at fixed time intervals.</source>
          <target state="translated">このクラスは、一定の時間間隔で観測された時系列データサンプルのクロスバリデーションに使用することができます。</target>
        </trans-unit>
        <trans-unit id="88afb4091eb2a25b2e2017ee8508b1ff5c5ae125" translate="yes" xml:space="preserve">
          <source>This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.</source>
          <target state="translated">このクラスは、データセットのさまざまなサブサンプルにランダム化された多数の決定木(別名エクストラツリー)を適合させ、平均化を使用して予測精度を向上させ、オーバーフィットを制御するメタ推定器を実装します。</target>
        </trans-unit>
        <trans-unit id="dd60f6580be8e1908408c4fbfd8d3915f46e55b1" translate="yes" xml:space="preserve">
          <source>This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.</source>
          <target state="translated">このクラスは liblinear,newton-cg,sag,lbfgs オプティマイザを用いたロジスティック回帰を実装しています。newton-cg,sag,lbfgsソルバーは、L2正則化とプライマル定式化のみをサポートします。liblinearソルバーは、L1とL2の両方の正則化をサポートし、L2のペナルティに対してのみ二重定式化を行います。</target>
        </trans-unit>
        <trans-unit id="350693d0493245dcbd676a8f10e001d5020f745e" translate="yes" xml:space="preserve">
          <source>This class implements regularized logistic regression using the &amp;lsquo;liblinear&amp;rsquo; library, &amp;lsquo;newton-cg&amp;rsquo;, &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;lbfgs&amp;rsquo; solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied).</source>
          <target state="translated">このクラスは、「liblinear」ライブラリー、「newton-cg」、「sag」、および「lbfgs」ソルバーを使用して、正規化されたロジスティック回帰を実装します。密な入力と疎な入力の両方を処理できます。最適なパフォーマンスを得るには、Cビット配列または64ビットの浮動小数点を含むCSR行列を使用してください。他の入力フォーマットはすべて変換（およびコピー）されます。</target>
        </trans-unit>
        <trans-unit id="2c107aea4a3526193efe1f31d97ccab92891d87f" translate="yes" xml:space="preserve">
          <source>This class implements the Graphical Lasso algorithm.</source>
          <target state="translated">このクラスはGraphical Lassoアルゴリズムを実装しています。</target>
        </trans-unit>
        <trans-unit id="456e7e6f7be68de425401d6f66fe28d8a1090538" translate="yes" xml:space="preserve">
          <source>This class implements the algorithm known as AdaBoost-SAMME [2].</source>
          <target state="translated">このクラスは AdaBoost-SAMME として知られているアルゴリズムを実装しています [2]。</target>
        </trans-unit>
        <trans-unit id="003b6bf538c58eeda3c6fd28b21967bfd8b6c40e" translate="yes" xml:space="preserve">
          <source>This class implements the algorithm known as AdaBoost.R2 [2].</source>
          <target state="translated">このクラスは,AdaBoost.R2として知られているアルゴリズムを実装しています[2].</target>
        </trans-unit>
        <trans-unit id="63c4b52b10df12140782204ea7cf58fe0edab9de" translate="yes" xml:space="preserve">
          <source>This class implements two types of prior for the weights distribution: a finite mixture model with Dirichlet distribution and an infinite mixture model with the Dirichlet Process. In practice Dirichlet Process inference algorithm is approximated and uses a truncated distribution with a fixed maximum number of components (called the Stick-breaking representation). The number of components actually used almost always depends on the data.</source>
          <target state="translated">このクラスは、ディリクレ分布を用いた有限混合モデルとディリクレ過程を用いた無限混合モデルの2種類の優先順位を実装しています。実際には、ディリクレ過程の推論アルゴリズムは近似的であり、固定の最大成分数を持つ切り詰められた分布を使用します(棒切れ表現と呼ばれます)。実際に使用される成分の数は、ほとんどの場合、データに依存します。</target>
        </trans-unit>
        <trans-unit id="e15e7e7d8d04d41fdd2a4f34183baa0366e86dea" translate="yes" xml:space="preserve">
          <source>This class inherits from PLS with mode=&amp;rdquo;A&amp;rdquo; and deflation_mode=&amp;rdquo;canonical&amp;rdquo;, norm_y_weights=True and algorithm=&amp;rdquo;nipals&amp;rdquo;, but svd should provide similar results up to numerical errors.</source>
          <target state="translated">このクラスは、mode =&amp;rdquo; A&amp;rdquo;およびdeflation_mode =&amp;rdquo; canonical&amp;rdquo;、norm_y_weights = True、algorithm =&amp;rdquo; nipals&amp;rdquo;を使用してPLSから継承しますが、svdは数値エラーまで同様の結果を提供します。</target>
        </trans-unit>
        <trans-unit id="a0a1d1daa83e6ad727cb13fd589f28f37b44df20" translate="yes" xml:space="preserve">
          <source>This class inherits from both ValueError and AttributeError to help with exception handling and backward compatibility.</source>
          <target state="translated">このクラスは、例外処理と下位互換性を助けるために、ValueError と AttributeError の両方を継承します。</target>
        </trans-unit>
        <trans-unit id="d34b4e0a14d3a494edeba399329a47fb49b3ee6c" translate="yes" xml:space="preserve">
          <source>This class is a low-memory alternative to DictVectorizer and CountVectorizer, intended for large-scale (online) learning and situations where memory is tight, e.g. when running prediction code on embedded devices.</source>
          <target state="translated">このクラスは,DictVectorizerやCountVectorizerに代わる低メモリの代替クラスであり,大規模な(オンラインの)学習や,組み込み機器で予測コードを実行する場合など,メモリが逼迫している状況を想定しています.</target>
        </trans-unit>
        <trans-unit id="8e34d865883b535f68990f01a240eaab9f87f080" translate="yes" xml:space="preserve">
          <source>This class is hence suitable for use in the early steps of a &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">したがって、このクラスは&lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt; の&lt;/a&gt;初期段階での使用に適しています。</target>
        </trans-unit>
        <trans-unit id="eef0760dd6d25fd731b9abce61656453bb690cee" translate="yes" xml:space="preserve">
          <source>This class is useful when the behavior of &lt;a href=&quot;generated/sklearn.model_selection.leavepgroupsout#sklearn.model_selection.LeavePGroupsOut&quot;&gt;&lt;code&gt;LeavePGroupsOut&lt;/code&gt;&lt;/a&gt; is desired, but the number of groups is large enough that generating all possible partitions with \(P\) groups withheld would be prohibitively expensive. In such a scenario, &lt;a href=&quot;generated/sklearn.model_selection.groupshufflesplit#sklearn.model_selection.GroupShuffleSplit&quot;&gt;&lt;code&gt;GroupShuffleSplit&lt;/code&gt;&lt;/a&gt; provides a random sample (with replacement) of the train / test splits generated by &lt;a href=&quot;generated/sklearn.model_selection.leavepgroupsout#sklearn.model_selection.LeavePGroupsOut&quot;&gt;&lt;code&gt;LeavePGroupsOut&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">このクラスは、&lt;a href=&quot;generated/sklearn.model_selection.leavepgroupsout#sklearn.model_selection.LeavePGroupsOut&quot;&gt; &lt;code&gt;LeavePGroupsOut&lt;/code&gt; &lt;/a&gt;の動作が必要な場合に役立ちますが、グループの数が多すぎるため、\（P \）グループを保留にしてすべての可能なパーティションを生成すると、コストが非常に高くなります。そのようなシナリオでは、&lt;a href=&quot;generated/sklearn.model_selection.groupshufflesplit#sklearn.model_selection.GroupShuffleSplit&quot;&gt; &lt;code&gt;GroupShuffleSplit&lt;/code&gt; は&lt;/a&gt;によって生成された列車/試験分割の（置換を有する）のランダムサンプルを提供&lt;a href=&quot;generated/sklearn.model_selection.leavepgroupsout#sklearn.model_selection.LeavePGroupsOut&quot;&gt; &lt;code&gt;LeavePGroupsOut&lt;/code&gt; を&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="edf4c29bfa2afe43016dc0b6660ad50132bd51ec" translate="yes" xml:space="preserve">
          <source>This class provides a uniform interface to fast distance metric functions. The various metrics can be accessed via the &lt;code&gt;get_metric&lt;/code&gt; class method and the metric string identifier (see below). For example, to use the Euclidean distance:</source>
          <target state="translated">このクラスは、高速距離メトリック関数への統一されたインターフェースを提供します。さまざまなメトリックには、 &lt;code&gt;get_metric&lt;/code&gt; クラスメソッドとメトリック文字列識別子を介してアクセスできます（以下を参照）。たとえば、ユークリッド距離を使用するには：</target>
        </trans-unit>
        <trans-unit id="336f533fd7cf96bc18f597ff7211d31f0cd62386" translate="yes" xml:space="preserve">
          <source>This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.</source>
          <target state="translated">このクラスは,密な入力と疎な入力の両方をサポートし,マルチクラスのサポートは,onevs the-rest スキームに従って処理されます.</target>
        </trans-unit>
        <trans-unit id="aebcf6792861578bf4b4a69a0be71898d4023b6a" translate="yes" xml:space="preserve">
          <source>This class supports both dense and sparse input.</source>
          <target state="translated">このクラスは,密な入力と疎な入力の両方をサポートします.</target>
        </trans-unit>
        <trans-unit id="4042c6697e9df3310aa51f4f2bbe289c3d222c6e" translate="yes" xml:space="preserve">
          <source>This class turns sequences of symbolic feature names (strings) into scipy.sparse matrices, using a hash function to compute the matrix column corresponding to a name. The hash function employed is the signed 32-bit version of Murmurhash3.</source>
          <target state="translated">このクラスは,記号的特徴名(文字列)のシーケンスを scipy.sparse 行列に変換し,ハッシュ関数を用いて名前に対応する行列の列を計算します.採用されるハッシュ関数は,Murmurhash3の符号付き32ビット版です.</target>
        </trans-unit>
        <trans-unit id="afb2ca6e635ea8a6abf9d5cec38428c8ecdc147c" translate="yes" xml:space="preserve">
          <source>This classification dataset is constructed by taking a multi-dimensional standard normal distribution and defining classes separated by nested concentric multi-dimensional spheres such that roughly equal numbers of samples are in each class (quantiles of the \(\chi^2\) distribution).</source>
          <target state="translated">この分類データセットは、多次元の標準正規分布を用いて、同心円状の多次元球で区切られたクラスを定義し、各クラスのサンプル数がほぼ等しくなるようにしたものです(「\chi^2」分布の数列)。</target>
        </trans-unit>
        <trans-unit id="a35e593acabc67ebdf8fae8d0484d5f85056d4a7" translate="yes" xml:space="preserve">
          <source>This classifier is useful as a simple baseline to compare with other (real) classifiers. Do not use it for real problems.</source>
          <target state="translated">この分類器は,他の(実際の)分類器と比較するための単純なベースラインとして有用です.実際の問題には使用しないでください。</target>
        </trans-unit>
        <trans-unit id="4ceac36d1efc9bb429dd84350330a84101bb5c8c" translate="yes" xml:space="preserve">
          <source>This classifier lost over a lot of its F-score, just because we removed metadata that has little to do with topic classification. It loses even more if we also strip this metadata from the training data:</source>
          <target state="translated">この分類器は、トピックの分類とはほとんど関係のないメタデータを削除しただけで、そのFスコアの多くを失ってしまいました。訓練データからこのメタデータを削除した場合、この分類器はさらに多くのFスコアを失います。</target>
        </trans-unit>
        <trans-unit id="064e5da463cfecd3ca166c4023a79d0a6500160d" translate="yes" xml:space="preserve">
          <source>This combination is implementing in &lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt;&lt;code&gt;HashingVectorizer&lt;/code&gt;&lt;/a&gt;, a transformer class that is mostly API compatible with &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt;. &lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt;&lt;code&gt;HashingVectorizer&lt;/code&gt;&lt;/a&gt; is stateless, meaning that you don&amp;rsquo;t have to call &lt;code&gt;fit&lt;/code&gt; on it:</source>
          <target state="translated">この組み合わせは、ほとんどが&lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt;とAPI互換性のある変換クラスである&lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt; &lt;code&gt;HashingVectorizer&lt;/code&gt; &lt;/a&gt;で実装されています。&lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt; &lt;code&gt;HashingVectorizer&lt;/code&gt; &lt;/a&gt;はステートレスです。つまり、それに &lt;code&gt;fit&lt;/code&gt; させる必要はありません。</target>
        </trans-unit>
        <trans-unit id="1d56591f4aa7f0ebb864c2d8835af7dfb7f8f26b" translate="yes" xml:space="preserve">
          <source>This combines the values of agglomerated features into a single value, and should accept an array of shape [M, N] and the keyword argument &lt;code&gt;axis=1&lt;/code&gt;, and reduce it to an array of size [M].</source>
          <target state="translated">これは、凝集した特徴の値を1つの値に結合し、形状[M、N]の配列とキーワード引数 &lt;code&gt;axis=1&lt;/code&gt; を受け入れ、サイズ[M]の配列に縮小します。</target>
        </trans-unit>
        <trans-unit id="26c788fe31e79bd1bbf24fbba8a1b8342ff15ce1" translate="yes" xml:space="preserve">
          <source>This consumes less memory than shuffling the data directly.</source>
          <target state="translated">これは、データを直接シャッフルするよりも少ないメモリ消費量です。</target>
        </trans-unit>
        <trans-unit id="9c8cf431c4f1299ae41612f84a50a597aa4a1059" translate="yes" xml:space="preserve">
          <source>This creates binary hashes of input data points by getting the dot product of input points and hash_function then transforming the projection into a binary string array based on the sign (positive/negative) of the projection. A sorted array of binary hashes is created.</source>
          <target state="translated">これは,入力点と hash_function のドット積を取得し,その射影を射影の符号(正/負)に基づいてバイナリ文字列配列に変換することで,入力データ点のバイナリハッシュを作成します.ソートされたバイナリハッシュの配列が作成されます。</target>
        </trans-unit>
        <trans-unit id="75f340063df2a6996986c297d7d4423dc4417e05" translate="yes" xml:space="preserve">
          <source>This cross-validation object is a merge of StratifiedKFold and ShuffleSplit, which returns stratified randomized folds. The folds are made by preserving the percentage of samples for each class.</source>
          <target state="translated">このクロスバリデーションオブジェクトは、StratifiedKFoldとShuffleSplitをマージしたもので、層化されたランダム化されたフォールドを返します。フォールドは、各クラスのサンプルのパーセンテージを保存して作られます。</target>
        </trans-unit>
        <trans-unit id="ab90d891a9b7f61040a0bd2fc78eae2488d53a3a" translate="yes" xml:space="preserve">
          <source>This cross-validation object is a variation of &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt;. In the kth split, it returns first k folds as train set and the (k+1)th fold as test set.</source>
          <target state="translated">この交差検証オブジェクトは&lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; の&lt;/a&gt;バリエーションです。k番目の分割では、最初のkフォールドを学習セットとして、（k + 1）フォールドをテストセットとして返します。</target>
        </trans-unit>
        <trans-unit id="c3d6a6171342c06e134b7087a7e83e3f80ba8a79" translate="yes" xml:space="preserve">
          <source>This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class.</source>
          <target state="translated">このクロスバリデーションオブジェクトは、層化されたフォールドを返すKFoldのバリエーションです。各クラスのサンプルのパーセンテージを保持することでフォールドを作成します。</target>
        </trans-unit>
        <trans-unit id="2c34ca372157ddad0d3d18ffb66a8717775276f6" translate="yes" xml:space="preserve">
          <source>This data sets consists of 3 different types of irises&amp;rsquo; (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray</source>
          <target state="translated">このデータセットは、3つの異なるタイプのアイリス（Setosa、Versicolour、およびVirginica）の花びらとがく片の長さで構成され、150x4 numpy.ndarrayに保存されます</target>
        </trans-unit>
        <trans-unit id="158d23b76a72fb850a277110200a4b319b51d7f5" translate="yes" xml:space="preserve">
          <source>This database is also available through the UW CS ftp server:</source>
          <target state="translated">このデータベースは、UW CSのftpサーバーからも利用可能です。</target>
        </trans-unit>
        <trans-unit id="8f0ed5f875aa21e65ca9d6116dc0e918ff34c0ff" translate="yes" xml:space="preserve">
          <source>This dataset consists of 20,640 samples and 9 features.</source>
          <target state="translated">このデータセットは、20,640個のサンプルと9個の特徴量で構成されています。</target>
        </trans-unit>
        <trans-unit id="2e15d3adbea56bdf31e76cb4ee55fcf6dfb7c05f" translate="yes" xml:space="preserve">
          <source>This dataset is a collection of JPEG pictures of famous people collected over the internet, all details are available on the official website:</source>
          <target state="translated">このデータセットは、インターネット上で収集された有名人のJPEG画像を集めたもので、すべての詳細は公式サイトに掲載されています。</target>
        </trans-unit>
        <trans-unit id="1c0b9129c637e05601004735cb7bfdbdba431796" translate="yes" xml:space="preserve">
          <source>This dataset is described in Celeux et al [1]. as:</source>
          <target state="translated">このデータセットは、Celeuxら[1]として記載されている。</target>
        </trans-unit>
        <trans-unit id="e92704f97c6e77c413a0405e7cf7dd2e32191660" translate="yes" xml:space="preserve">
          <source>This dataset is described in Friedman [1] and Breiman [2].</source>
          <target state="translated">このデータセットはFriedman [1]とBreiman [2]に記述されている.</target>
        </trans-unit>
        <trans-unit id="473d3b557a1c23b381f633c2c2acf0c38c2a328f" translate="yes" xml:space="preserve">
          <source>This dataset is made up of 1797 8x8 images. Each image, like the one shown below, is of a hand-written digit. In order to utilize an 8x8 figure like this, we&amp;rsquo;d have to first transform it into a feature vector with length 64.</source>
          <target state="translated">このデータセットは、1797個の8x8画像で構成されています。以下に示すような各画像は、手書きの数字です。このような8x8の図を使用するには、まず長さ64の特徴ベクトルに変換する必要があります。</target>
        </trans-unit>
        <trans-unit id="73cdbbbbdb25af126933f658f4b065e86b9c1a55" translate="yes" xml:space="preserve">
          <source>This dataset represents the geographic distribution of species. The dataset is provided by Phillips et. al. (2006).</source>
          <target state="translated">このデータセットは種の地理的分布を表している。このデータセットは Phillips et al.(2006).</target>
        </trans-unit>
        <trans-unit id="e72397d5f7691c5c60df49442bb007cee4717786" translate="yes" xml:space="preserve">
          <source>This dataset was derived from the 1990 U.S. census, using one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people).</source>
          <target state="translated">このデータセットは、1990 年の米国国勢調査から得られたもので、国勢調査のブロックグループごとに 1 行ずつのデータを用いている。ブロックグループとは、米国国勢調査局がサンプルデータを公表している最小の地理的単位である(ブロックグループの人口は通常600人から3,000人である)。</target>
        </trans-unit>
        <trans-unit id="47bf559cf1abc9fb33a96a120f583ad0bcd0f9f8" translate="yes" xml:space="preserve">
          <source>This dataset was obtained from the StatLib repository. &lt;a href=&quot;http://lib.stat.cmu.edu/datasets/&quot;&gt;http://lib.stat.cmu.edu/datasets/&lt;/a&gt;</source>
          <target state="translated">このデータセットは、StatLibリポジトリから取得されました。&lt;a href=&quot;http://lib.stat.cmu.edu/datasets/&quot;&gt;http://lib.stat.cmu.edu/datasets/&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4668c093fae272aec3196fc6d39e4e5a4eede980" translate="yes" xml:space="preserve">
          <source>This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.</source>
          <target state="translated">このデータセットは、カーネギーメロン大学で管理されているStatLibライブラリから取得したものです。</target>
        </trans-unit>
        <trans-unit id="9b1b90be23e0adb200134bcc2570822a79ae6e88" translate="yes" xml:space="preserve">
          <source>This demonstrates Label Propagation learning a good boundary even with a small amount of labeled data.</source>
          <target state="translated">これは、少量のラベル付きデータでもラベル伝搬が良好な境界を学習することを実証しています。</target>
        </trans-unit>
        <trans-unit id="33d36bf521beb70b7b2f4b7e5d24d58f96f46c86" translate="yes" xml:space="preserve">
          <source>This description can be vectorized into a sparse two-dimensional matrix suitable for feeding into a classifier (maybe after being piped into a &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt;&lt;code&gt;text.TfidfTransformer&lt;/code&gt;&lt;/a&gt; for normalization):</source>
          <target state="translated">この説明は、分類器へのフィードに適したスパースな2次元マトリックスにベクトル化できます（おそらく正規化のために&lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt; &lt;code&gt;text.TfidfTransformer&lt;/code&gt; &lt;/a&gt;にパイプされた後）。</target>
        </trans-unit>
        <trans-unit id="12f7e332bc936dbb460a17349dda07780cab7782" translate="yes" xml:space="preserve">
          <source>This determines which warnings will be made in the case that this function is being used to return only one of its metrics.</source>
          <target state="translated">これは、この関数がそのメトリクスのうちの 1 つだけを返すために使用されている場合に、どの警告を出すかを決定します。</target>
        </trans-unit>
        <trans-unit id="a1a66d0ad9255f63c11b93170b95da4e6eeaea4f" translate="yes" xml:space="preserve">
          <source>This downscaling is called &lt;a href=&quot;https://en.wikipedia.org/wiki/Tf-idf&quot;&gt;tf&amp;ndash;idf&lt;/a&gt; for &amp;ldquo;Term Frequency times Inverse Document Frequency&amp;rdquo;.</source>
          <target state="translated">このダウンスケーリングは、「用語の頻度とドキュメントの逆数の頻度」の&lt;a href=&quot;https://en.wikipedia.org/wiki/Tf-idf&quot;&gt;tf-idf&lt;/a&gt;と呼ばれます。</target>
        </trans-unit>
        <trans-unit id="edeebc499fdf886cf2b1fe82f9cc25a148384f70" translate="yes" xml:space="preserve">
          <source>This early stopping strategy is activated if &lt;code&gt;early_stopping=True&lt;/code&gt;; otherwise the stopping criterion only uses the training loss on the entire input data. To better control the early stopping strategy, we can specify a parameter &lt;code&gt;validation_fraction&lt;/code&gt; which set the fraction of the input dataset that we keep aside to compute the validation score. The optimization will continue until the validation score did not improve by at least &lt;code&gt;tol&lt;/code&gt; during the last &lt;code&gt;n_iter_no_change&lt;/code&gt; iterations. The actual number of iterations is available at the attribute &lt;code&gt;n_iter_&lt;/code&gt;.</source>
          <target state="translated">この早期停止戦略は、 &lt;code&gt;early_stopping=True&lt;/code&gt; の場合にアクティブになります。それ以外の場合、停止基準は入力データ全体のトレーニング損失のみを使用します。早期停止戦略をより適切に制御するために、検証スコアを計算するために確保しておく入力データセットの割合を設定するパラメーター &lt;code&gt;validation_fraction&lt;/code&gt; を指定できます。最適化は、最後の &lt;code&gt;n_iter_no_change&lt;/code&gt; の反復中に検証スコアが少なくとも &lt;code&gt;tol&lt;/code&gt; まで改善されなくなるまで続きます。実際の反復回数は、属性 &lt;code&gt;n_iter_&lt;/code&gt; で取得できます。</target>
        </trans-unit>
        <trans-unit id="6528dcf2523991bd357ca56474b42d537ada09b8" translate="yes" xml:space="preserve">
          <source>This embedding can also &amp;lsquo;work&amp;rsquo; even if the &lt;code&gt;adjacency&lt;/code&gt; variable is not strictly the adjacency matrix of a graph but more generally an affinity or similarity matrix between samples (for instance the heat kernel of a euclidean distance matrix or a k-NN matrix).</source>
          <target state="translated">この埋め込みは、 &lt;code&gt;adjacency&lt;/code&gt; 変数が厳密にはグラフの隣接行列ではなく、より一般的にはサンプル間の親和性または類似性行列（たとえば、ユークリッド距離行列またはk-NN行列の熱カーネル）であっても、「機能」します。</target>
        </trans-unit>
        <trans-unit id="662188aeeffeee289aab2f0d97150266d90c022d" translate="yes" xml:space="preserve">
          <source>This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.</source>
          <target state="translated">このエンコーディングは、多くの scikit-learn 推定器、特に標準カーネルを持つ線形モデルや SVM にカテゴリデータを供給するために必要です。</target>
        </trans-unit>
        <trans-unit id="752036d9bd5ae374e975c046f504e0b38de39538" translate="yes" xml:space="preserve">
          <source>This estimator</source>
          <target state="translated">この推定子</target>
        </trans-unit>
        <trans-unit id="36ceeb9f387752a577aeb048b3f21cd319ecfb48" translate="yes" xml:space="preserve">
          <source>This estimator allows different columns or column subsets of the input to be transformed separately and the results combined into a single feature space. This is useful for heterogeneous or columnar data, to combine several feature extraction mechanisms or transformations into a single transformer.</source>
          <target state="translated">この推定器を使用すると、入力の異なる列または列のサブセットを別々に変換し、その結果を1つの特徴空間に結合することができます。これは、複数の特徴抽出メカニズムまたは変換を1つの変換器に結合するために、異種データまたは柱状データに有用である。</target>
        </trans-unit>
        <trans-unit id="02199e2b9b2bd941c7464261eedf68a6fd2d82e2" translate="yes" xml:space="preserve">
          <source>This estimator applies a list of transformer objects in parallel to the input data, then concatenates the results. This is useful to combine several feature extraction mechanisms into a single transformer.</source>
          <target state="translated">この推定器は、入力データに並列にトランスフォーマーオブジェクトのリストを適用し、その結果を連結します。これは、複数の特徴抽出メカニズムを1つのトランスフォーマーに結合するのに便利です。</target>
        </trans-unit>
        <trans-unit id="a93ab3d6ae360c030a56bd4fabca46c42abdaf54" translate="yes" xml:space="preserve">
          <source>This estimator approximates a slightly different version of the additive chi squared kernel then &lt;code&gt;metric.additive_chi2&lt;/code&gt; computes.</source>
          <target state="translated">この推定器は、わずかに異なるバージョンの加法カイ二乗カーネルを近似し、次に &lt;code&gt;metric.additive_chi2&lt;/code&gt; が計算します。</target>
        </trans-unit>
        <trans-unit id="57f1dab8dd3e838e06f9128461ff865667eb2891" translate="yes" xml:space="preserve">
          <source>This estimator has built-in support for multi-variate regression (i.e., when y is a 2d-array of shape [n_samples, n_targets]).</source>
          <target state="translated">この推定器は、多変量回帰のサポートを内蔵している(すなわち、yが形状[n_samples,n_targets]の2次元配列の場合)。</target>
        </trans-unit>
        <trans-unit id="ab45bcef3fd31ebffe9c4724334c2d64921dae44" translate="yes" xml:space="preserve">
          <source>This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance.</source>
          <target state="translated">損失の勾配はサンプルごとに一度に推定され、モデルは途中で強度が低下するスケジュール(別名学習率)で更新されます。SGD ではミニバッチ (オンライン/アウトオブコア)学習が可能で、パーシャルフィット法を参照してください。デフォルトの学習率スケジュールを使用して最良の結果を得るには、データの平均値と単位分散がゼロである必要があります。</target>
        </trans-unit>
        <trans-unit id="5fdb17bfdb14f498d9377f8ca9b7cc2199a41d7f" translate="yes" xml:space="preserve">
          <source>This estimator is stateless (besides constructor parameters), the fit method does nothing but is useful when used in a pipeline.</source>
          <target state="translated">この推定器はステートレスであり(コンストラクタのパラメータ以外に)、フィットメソッドは何もしませんが、パイプラインで使用する場合には便利です。</target>
        </trans-unit>
        <trans-unit id="42d08f109b32ce107e9f058e7449521b0b6eab28" translate="yes" xml:space="preserve">
          <source>This estimator scales and translates each feature individually such that it is in the given range on the training set, i.e. between zero and one.</source>
          <target state="translated">この推定器は、各特徴が訓練セット上の所定の範囲内にあるように、つまり0から1の間にあるように、各特徴を個別にスケーリングして変換します。</target>
        </trans-unit>
        <trans-unit id="ce7850baf5a7a3e7ef75716db2d2e4af71c92137" translate="yes" xml:space="preserve">
          <source>This estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity.</source>
          <target state="translated">この推定器は、訓練セットの各特徴の最大絶対値が1.0になるように、各特徴を個別にスケーリングして変換します。データのシフトやセンタリングを行わないため、疎分散を破壊しない。</target>
        </trans-unit>
        <trans-unit id="0ff92b3f8e56701f386ccbc5279ec5fdb2974bc0" translate="yes" xml:space="preserve">
          <source>This estimator scales each feature individually such that the maximal absolute value of each feature in the training set will be 1.0.</source>
          <target state="translated">この推定器は、訓練セットの各特徴の最大絶対値が1.0になるように、各特徴を個別にスケーリングする。</target>
        </trans-unit>
        <trans-unit id="354214ed410106228bbb593cd82a49f32a2508f8" translate="yes" xml:space="preserve">
          <source>This estimator supports two algorithms: a fast randomized SVD solver, and a &amp;ldquo;naive&amp;rdquo; algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.</source>
          <target state="translated">この推定器は、2つのアルゴリズムをサポートします。高速ランダム化SVDソルバーと、（X * XT）または（XT * X）の固有ソルバーとしてARPACKを使用する「単純な」アルゴリズムのうち、どちらか効率的です。</target>
        </trans-unit>
        <trans-unit id="3ed5861f671a7b7a1c102cd9c37c8bf2e273de13" translate="yes" xml:space="preserve">
          <source>This estimator will run an extensive test-suite for input validation, shapes, etc. Additional tests for classifiers, regressors, clustering or transformers will be run if the Estimator class inherits from the corresponding mixin from sklearn.base.</source>
          <target state="translated">この推定器は、入力の検証や形状などの広範なテストスイートを実行します。Estimatorクラスがsklearn.baseの対応するmixinを継承している場合、分類器、回帰器、クラスタリング、変換器のための追加テストが実行されます。</target>
        </trans-unit>
        <trans-unit id="7011f5e2b484f266188acd0aba4f3d3175f11ed0" translate="yes" xml:space="preserve">
          <source>This example also shows the usefulness of applying Ridge regression to highly ill-conditioned matrices. For such matrices, a slight change in the target variable can cause huge variances in the calculated weights. In such cases, it is useful to set a certain regularization (alpha) to reduce this variation (noise).</source>
          <target state="translated">この事例は,高度に条件の悪い行列にリッジ回帰を適用することの有用性も示している.このような行列では、ターゲット変数のわずかな変化が、計算された重みに大きな分散を引き起こすことがあります。このような場合,この変動(ノイズ)を減らすために,ある種の正則化(アルファ)を設定することが有用である.</target>
        </trans-unit>
        <trans-unit id="7f82721c4674480adff4241dbcac8e543f16c868" translate="yes" xml:space="preserve">
          <source>This example applies to olivetti_faces different unsupervised matrix decomposition (dimension reduction) methods from the module &lt;a href=&quot;../../modules/classes#module-sklearn.decomposition&quot;&gt;&lt;code&gt;sklearn.decomposition&lt;/code&gt;&lt;/a&gt; (see the documentation chapter &lt;a href=&quot;../../modules/decomposition#decompositions&quot;&gt;Decomposing signals in components (matrix factorization problems)&lt;/a&gt;) .</source>
          <target state="translated">この例は、モジュール&lt;a href=&quot;../../modules/classes#module-sklearn.decomposition&quot;&gt; &lt;code&gt;sklearn.decomposition&lt;/code&gt; &lt;/a&gt;からのolivetti_facesのさまざまな教師なし行列分解（次元削減）メソッドに適用されます（ドキュメントの「&lt;a href=&quot;../../modules/decomposition#decompositions&quot;&gt;コンポーネント内の信号の分解（行列分解の問題）&lt;/a&gt;」の章を参照してください）。</target>
        </trans-unit>
        <trans-unit id="33924f5409489cd3edd1b22f28ee011b17a585da" translate="yes" xml:space="preserve">
          <source>This example compares 2 dimensionality reduction strategies:</source>
          <target state="translated">この例では、2 次元削減戦略を比較しています。</target>
        </trans-unit>
        <trans-unit id="1ba8c26b14d0dc5555ed6b18d75cbed15c385668" translate="yes" xml:space="preserve">
          <source>This example compares non-nested and nested cross-validation strategies on a classifier of the iris data set. Nested cross-validation (CV) is often used to train a model in which hyperparameters also need to be optimized. Nested CV estimates the generalization error of the underlying model and its (hyper)parameter search. Choosing the parameters that maximize non-nested CV biases the model to the dataset, yielding an overly-optimistic score.</source>
          <target state="translated">この例は、虹彩データセットの分類器での非入れ子と入れ子のクロスバリデーション戦略を比較する。ネストされたクロスバリデーション(CV)は,ハイパーパラメータも最適化する必要があるモデルを訓練するためによく使われる.入れ子になったCVは,基礎となるモデルとその(ハイパー)パラメータ探索の一般化誤差を推定する.非入れ子CVを最大化するパラメータを選択すると,モデルがデータセットに偏り,過度に最適化されたスコアが得られる.</target>
        </trans-unit>
        <trans-unit id="7ef1cb506f6769f9ea8f57cc850c6090d62898eb" translate="yes" xml:space="preserve">
          <source>This example compares the timing of Birch (with and without the global clustering step) and MiniBatchKMeans on a synthetic dataset having 100,000 samples and 2 features generated using make_blobs.</source>
          <target state="translated">この例では、10万サンプルとmake_blobsを使用して生成された2つの特徴量を持つ合成データセットについて、Birch(グローバルクラスタリングステップの有無に関わらず)とMiniBatchKMeansのタイミングを比較しています。</target>
        </trans-unit>
        <trans-unit id="8901e1f5225dc1b7e06d2fabb8f06f19a3753c45" translate="yes" xml:space="preserve">
          <source>This example constructs a pipeline that does dimensionality reduction followed by prediction with a support vector classifier. It demonstrates the use of &lt;code&gt;GridSearchCV&lt;/code&gt; and &lt;code&gt;Pipeline&lt;/code&gt; to optimize over different classes of estimators in a single CV run &amp;ndash; unsupervised &lt;code&gt;PCA&lt;/code&gt; and &lt;code&gt;NMF&lt;/code&gt; dimensionality reductions are compared to univariate feature selection during the grid search.</source>
          <target state="translated">この例では、次元削減とそれに続くサポートベクター分類器による予測を行うパイプラインを構築します。これは、 &lt;code&gt;GridSearchCV&lt;/code&gt; と &lt;code&gt;Pipeline&lt;/code&gt; を使用して、1回のCV実行でさまざまなクラスの推定量を最適化する方法を示しています。教師なし &lt;code&gt;PCA&lt;/code&gt; および &lt;code&gt;NMF&lt;/code&gt; の次元削減は、グリッド検索中に単変量特徴選択と比較されます。</target>
        </trans-unit>
        <trans-unit id="b084d11db0bc218128b35a7afe55ac9fa7c4daf1" translate="yes" xml:space="preserve">
          <source>This example demonstrates how to approximate a function with a polynomial of degree n_degree by using ridge regression. Concretely, from n_samples 1d points, it suffices to build the Vandermonde matrix, which is n_samples x n_degree+1 and has the following form:</source>
          <target state="translated">この例では、リッジ回帰を用いて次数n_degreeの多項式を持つ関数を近似する方法を示します。具体的には、n_samples 1d点から、n_samples x n_degree+1で、以下の形式を持つVandermonde行列を構築すればよい。</target>
        </trans-unit>
        <trans-unit id="8509d7895b98e9b3d2d07ed03eae90baa68f1c72" translate="yes" xml:space="preserve">
          <source>This example demonstrates how to generate a checkerboard dataset and bicluster it using the Spectral Biclustering algorithm.</source>
          <target state="translated">この例では、Spectral Biclustering アルゴリズムを使用して、チェッカーボードデータセットを生成し、それを二重クラスタ化する方法を示しています。</target>
        </trans-unit>
        <trans-unit id="fb9a20a0b6ffdb624c38c357324f211d180af27f" translate="yes" xml:space="preserve">
          <source>This example demonstrates how to generate a dataset and bicluster it using the Spectral Co-Clustering algorithm.</source>
          <target state="translated">この例では、Spectral Co-Clustering アルゴリズムを使用してデータセットを生成し、それを二重クラスタリングする方法を示します。</target>
        </trans-unit>
        <trans-unit id="4e024e96a6e8109f327c2d890a3a85ee374e03bd" translate="yes" xml:space="preserve">
          <source>This example demonstrates how to use &lt;a href=&quot;../../modules/generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;sklearn.compose.ColumnTransformer&lt;/code&gt;&lt;/a&gt; on a dataset containing different types of features. We use the 20-newsgroups dataset and compute standard bag-of-words features for the subject line and body in separate pipelines as well as ad hoc features on the body. We combine them (with weights) using a ColumnTransformer and finally train a classifier on the combined set of features.</source>
          <target state="translated">この例は、さまざまなタイプの特徴を含むデータセットで&lt;a href=&quot;../../modules/generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt; &lt;code&gt;sklearn.compose.ColumnTransformer&lt;/code&gt; &lt;/a&gt;を使用する方法を示しています。20ニュースグループのデータセットを使用して、件名行と本文の標準的なBag-of-Words機能を別のパイプラインで計算し、本文のアドホック機能を計算します。ColumnTransformerを使用してそれらを（重み付きで）組み合わせ、最後に、組み合わせた機能のセットで分類子をトレーニングします。</target>
        </trans-unit>
        <trans-unit id="76d491fec0fed042e6d7927f2dc124b698f9bded" translate="yes" xml:space="preserve">
          <source>This example demonstrates the Spectral Co-clustering algorithm on the twenty newsgroups dataset. The &amp;lsquo;comp.os.ms-windows.misc&amp;rsquo; category is excluded because it contains many posts containing nothing but data.</source>
          <target state="translated">この例は、20のニュースグループデータセットのスペクトル共クラスタリングアルゴリズムを示しています。'comp.os.ms-windows.misc'カテゴリは、データのみを含む多くの投稿が含まれているため除外されます。</target>
        </trans-unit>
        <trans-unit id="6186f51b55d8cba756254a15fe651e5e8504791a" translate="yes" xml:space="preserve">
          <source>This example demonstrates the behavior of Gaussian mixture models fit on data that was not sampled from a mixture of Gaussian random variables. The dataset is formed by 100 points loosely spaced following a noisy sine curve. There is therefore no ground truth value for the number of Gaussian components.</source>
          <target state="translated">この例は、ガウシアン混合モデルが、ガウシアンランダム変数の混合物からサンプリングされなかったデータに適合したときの挙動を示しています。このデータセットは、ノイズの多いサイン曲線に沿って疎間隔に配置された100点によって形成されています。したがって、ガウス成分の数には基底真理値はありません。</target>
        </trans-unit>
        <trans-unit id="a1949f51dde2d60d7d4d1e707d145c1301537434" translate="yes" xml:space="preserve">
          <source>This example demonstrates the power of semisupervised learning by training a Label Spreading model to classify handwritten digits with sets of very few labels.</source>
          <target state="translated">この例では、非常に少ないラベルのセットで手書きの数字を分類するためにラベル拡散モデルを訓練することで、半教師付き学習の威力を実証しています。</target>
        </trans-unit>
        <trans-unit id="c6020c6a7334e89ced3b2c5f4b02f4ed9989e37f" translate="yes" xml:space="preserve">
          <source>This example demonstrates the problems of underfitting and overfitting and how we can use linear regression with polynomial features to approximate nonlinear functions. The plot shows the function that we want to approximate, which is a part of the cosine function. In addition, the samples from the real function and the approximations of different models are displayed. The models have polynomial features of different degrees. We can see that a linear function (polynomial with degree 1) is not sufficient to fit the training samples. This is called &lt;strong&gt;underfitting&lt;/strong&gt;. A polynomial of degree 4 approximates the true function almost perfectly. However, for higher degrees the model will &lt;strong&gt;overfit&lt;/strong&gt; the training data, i.e. it learns the noise of the training data. We evaluate quantitatively &lt;strong&gt;overfitting&lt;/strong&gt; / &lt;strong&gt;underfitting&lt;/strong&gt; by using cross-validation. We calculate the mean squared error (MSE) on the validation set, the higher, the less likely the model generalizes correctly from the training data.</source>
          <target state="translated">この例では、アンダーフィットとオーバーフィットの問題と、多項式機能を使用して線形回帰を使用して非線形関数を近似する方法を示します。プロットは、コサイン関数の一部である、近似したい関数を示しています。さらに、実際の関数のサンプルとさまざまなモデルの近似値が表示されます。モデルには、次数の異なる多項式の特徴があります。線形関数（次数1の多項式）ではトレーニングサンプルを近似するのに十分ではないことがわかります。これは&lt;strong&gt;アンダーフィッティング&lt;/strong&gt;と呼ばれ&lt;strong&gt;ます&lt;/strong&gt;。次数4の多項式は、真の関数をほぼ完全に近似します。ただし、より高い次数の場合、モデルはトレーニングデータを&lt;strong&gt;オーバーフィット&lt;/strong&gt;します。つまり、トレーニングデータのノイズを学習します。定量的に評価&lt;strong&gt;&lt;/strong&gt;交差&lt;strong&gt;検定&lt;/strong&gt;を使用した&lt;strong&gt;過剰適合&lt;/strong&gt; / &lt;strong&gt;過適合&lt;/strong&gt;。検証セットの平均二乗誤差（MSE）を計算するほど、モデルがトレーニングデータから正しく一般化される可能性が低くなります。</target>
        </trans-unit>
        <trans-unit id="ab8d51c9ac9762c2930c84a5a03c1c12345a6627" translate="yes" xml:space="preserve">
          <source>This example demonstrates the use of the Box-Cox and Yeo-Johnson transforms through &lt;code&gt;preprocessing.PowerTransformer&lt;/code&gt; to map data from various distributions to a normal distribution.</source>
          <target state="translated">この例は、 &lt;code&gt;preprocessing.PowerTransformer&lt;/code&gt; によるBox-CoxおよびYeo-Johnson変換の使用法を示し、さまざまな分布から正規分布にデータをマッピングします。</target>
        </trans-unit>
        <trans-unit id="9b62ef0be0bf7ce49acab00a23e04164c0becf9d" translate="yes" xml:space="preserve">
          <source>This example does not perform any learning over the data (see &lt;a href=&quot;../applications/plot_species_distribution_modeling#sphx-glr-auto-examples-applications-plot-species-distribution-modeling-py&quot;&gt;Species distribution modeling&lt;/a&gt; for an example of classification based on the attributes in this dataset). It simply shows the kernel density estimate of observed data points in geospatial coordinates.</source>
          <target state="translated">この例では、データの学習は行われません（このデータセットの属性に基づく分類の例については、&lt;a href=&quot;../applications/plot_species_distribution_modeling#sphx-glr-auto-examples-applications-plot-species-distribution-modeling-py&quot;&gt;種分布モデリング&lt;/a&gt;を参照してください）。それは単に、地理空間座標で観測されたデータポイントのカーネル密度推定を示します。</target>
        </trans-unit>
        <trans-unit id="03eea75ae69c05ca0f9dc1a13d89bbd210d48145" translate="yes" xml:space="preserve">
          <source>This example doesn&amp;rsquo;t show it, as we&amp;rsquo;re in a low-dimensional space, but another advantage of the Dirichlet process model is that it can fit full covariance matrices effectively even when there are less examples per cluster than there are dimensions in the data, due to regularization properties of the inference algorithm.</source>
          <target state="translated">この例では、低次元空間にいるため、それを示していませんが、ディリクレプロセスモデルのもう1つの利点は、クラスターごとの例の数が推論アルゴリズムの正則化特性によるデータ。</target>
        </trans-unit>
        <trans-unit id="7b398c0b1dbb0f3edb9cc17097a9c9f8696a24cc" translate="yes" xml:space="preserve">
          <source>This example employs several unsupervised learning techniques to extract the stock market structure from variations in historical quotes.</source>
          <target state="translated">この例では、過去の相場の変動から株式市場の構造を抽出するために、いくつかの教師なし学習技術を採用しています。</target>
        </trans-unit>
        <trans-unit id="41937f256baaea1198c4d043d4bb81b61df0d50b" translate="yes" xml:space="preserve">
          <source>This example fits a Gradient Boosting model with least squares loss and 500 regression trees of depth 4.</source>
          <target state="translated">この例は、最小二乗損失と深さ4の500本の回帰木を持つ勾配ブーストモデルに適合します。</target>
        </trans-unit>
        <trans-unit id="e8121408498cf6fb8c886d50eef2580465ff3307" translate="yes" xml:space="preserve">
          <source>This example fits an AdaBoosted decision stump on a non-linearly separable classification dataset composed of two &amp;ldquo;Gaussian quantiles&amp;rdquo; clusters (see &lt;a href=&quot;../../modules/generated/sklearn.datasets.make_gaussian_quantiles#sklearn.datasets.make_gaussian_quantiles&quot;&gt;&lt;code&gt;sklearn.datasets.make_gaussian_quantiles&lt;/code&gt;&lt;/a&gt;) and plots the decision boundary and decision scores. The distributions of decision scores are shown separately for samples of class A and B. The predicted class label for each sample is determined by the sign of the decision score. Samples with decision scores greater than zero are classified as B, and are otherwise classified as A. The magnitude of a decision score determines the degree of likeness with the predicted class label. Additionally, a new dataset could be constructed containing a desired purity of class B, for example, by only selecting samples with a decision score above some value.</source>
          <target state="translated">この例では、2つの「ガウス分位点」クラスター（&lt;a href=&quot;../../modules/generated/sklearn.datasets.make_gaussian_quantiles#sklearn.datasets.make_gaussian_quantiles&quot;&gt; &lt;code&gt;sklearn.datasets.make_gaussian_quantiles&lt;/code&gt; を&lt;/a&gt;参照）で構成される非線形分離可能な分類データセットにAdaBoosted決定スタンプを適合させ、決定境界と決定スコアをプロットします。決定スコアの分布は、クラスAとクラスBのサンプルに対して別々に表示されます。各サンプルの予測クラスラベルは、決定スコアの符号によって決定されます。ゼロより大きい決定スコアを持つサンプルはBとして分類され、そうでない場合はAとして分類されます。決定スコアの大きさは、予測されたクラスラベルとの類似度を決定します。さらに、たとえば、ある値以上の決定スコアを持つサンプルのみを選択することにより、クラスBの望ましい純度を含む新しいデータセットを構築できます。</target>
        </trans-unit>
        <trans-unit id="02c230a18f98a72777c5f2652062014b16511fe8" translate="yes" xml:space="preserve">
          <source>This example has a fair amount of visualization-related code, as visualization is crucial here to display the graph. One of the challenge is to position the labels minimizing overlap. For this we use an heuristic based on the direction of the nearest neighbor along each axis.</source>
          <target state="translated">グラフを表示するには可視化が重要なので、この例にはかなりの量の可視化関連のコードが含まれています。課題の一つは、ラベルの重なりを最小限に抑えて配置することです。このために、各軸に沿った最も近い隣人の方向に基づくヒューリスティックを使用しています。</target>
        </trans-unit>
        <trans-unit id="259139974bd9ca7304e763dff02af979bb7908e6" translate="yes" xml:space="preserve">
          <source>This example illustrates GPC on XOR data. Compared are a stationary, isotropic kernel (&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt;) and a non-stationary kernel (&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt;&lt;code&gt;DotProduct&lt;/code&gt;&lt;/a&gt;). On this particular dataset, the &lt;code&gt;DotProduct&lt;/code&gt; kernel obtains considerably better results because the class-boundaries are linear and coincide with the coordinate axes. In practice, however, stationary kernels such as &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt; often obtain better results.</source>
          <target state="translated">この例は、XORデータのGPCを示しています。静止した等方性カーネル（&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt; &lt;code&gt;RBF&lt;/code&gt; &lt;/a&gt;）と非定常カーネル（&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt; &lt;code&gt;DotProduct&lt;/code&gt; &lt;/a&gt;）を比較します。この特定のデータセットでは、クラス境界が線形で座標軸と一致しているため、 &lt;code&gt;DotProduct&lt;/code&gt; カーネルはかなり優れた結果を取得します。ただし、実際には、&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt; &lt;code&gt;RBF&lt;/code&gt; &lt;/a&gt;などの固定カーネルがより良い結果を得ることがよくあります。</target>
        </trans-unit>
        <trans-unit id="a467b781e30c272f4f5e4645f1261bd726b14e8d" translate="yes" xml:space="preserve">
          <source>This example illustrates GPC on XOR data. Compared are a stationary, isotropic kernel (RBF) and a non-stationary kernel (DotProduct). On this particular dataset, the DotProduct kernel obtains considerably better results because the class-boundaries are linear and coincide with the coordinate axes. In general, stationary kernels often obtain better results.</source>
          <target state="translated">この例は、XORデータ上でのGPCを説明しています。比較されているのは、静止した等方性カーネル(RBF)と非静止カーネル(DotProduct)です。この特定のデータセットでは、クラス境界が線形で座標軸と一致しているため、DotProduct カーネルの方がかなり良い結果が得られます。一般的に,定常カーネルの方が良い結果が得られることが多いです.</target>
        </trans-unit>
        <trans-unit id="b43f5231cb55a1a95a64bd8afe5472e7955fc98b" translate="yes" xml:space="preserve">
          <source>This example illustrates and compares the bias-variance decomposition of the expected mean squared error of a single estimator against a bagging ensemble.</source>
          <target state="translated">この例では、単一の推定器の期待平均2乗誤差のバイアス-分散分解と袋詰めアンサンブルの比較を説明しています。</target>
        </trans-unit>
        <trans-unit id="0a3450a632d656139c95a4f138b569cec22ba6ef" translate="yes" xml:space="preserve">
          <source>This example illustrates both methods on an artificial dataset, which consists of a sinusoidal target function and strong noise added to every fifth datapoint. The first figure compares the learned model of KRR and SVR when both complexity/regularization and bandwidth of the RBF kernel are optimized using grid-search. The learned functions are very similar; however, fitting KRR is approx. seven times faster than fitting SVR (both with grid-search). However, prediction of 100000 target values is more than tree times faster with SVR since it has learned a sparse model using only approx. 1/3 of the 100 training datapoints as support vectors.</source>
          <target state="translated">この例では、正弦波のターゲット関数と5つ目のデータポイントごとに強いノイズが追加された人工的なデータセットを用いて、両手法を説明しています。最初の図は、グリッドサーチを用いて複雑化/正則化とRBFカーネルの帯域幅の両方を最適化した場合のKRRとSVRの学習モデルを比較したものです。学習された関数は非常によく似ているが,KRRの方がSVRの方が約7倍速い(いずれもグリッドサーチを用いた場合).しかし,100個の学習データポイントのうち約1/3のデータポイントのみを支持ベクトルとして用いて疎なモデルを学習しているため,100000個の目標値の予測はSVRの方が木数倍以上高速である.</target>
        </trans-unit>
        <trans-unit id="bdbaaa805869c3c13d157f267aeffaf332ff1284" translate="yes" xml:space="preserve">
          <source>This example illustrates both methods on an artificial dataset, which consists of a sinusoidal target function and strong noise. The figure compares the learned model of KRR and GPR based on a ExpSineSquared kernel, which is suited for learning periodic functions. The kernel&amp;rsquo;s hyperparameters control the smoothness (l) and periodicity of the kernel (p). Moreover, the noise level of the data is learned explicitly by GPR by an additional WhiteKernel component in the kernel and by the regularization parameter alpha of KRR.</source>
          <target state="translated">この例は、正弦波のターゲット関数と強いノイズで構成される人工データセットの両方の方法を示しています。この図は、周期関数の学習に適したExpSineSquaredカーネルに基づいて、KRRとGPRの学習モデルを比較します。カーネルのハイパーパラメータは、カーネルの滑らかさ（l）と周期性（p）を制御します。さらに、データのノイズレベルは、カーネル内の追加のWhiteKernelコンポーネントとKRRの正則化パラメーターalphaによってGPRによって明示的に学習されます。</target>
        </trans-unit>
        <trans-unit id="745a420beb7d4cfe3dcb516bc28a36f2576e5395" translate="yes" xml:space="preserve">
          <source>This example illustrates how sigmoid calibration changes predicted probabilities for a 3-class classification problem. Illustrated is the standard 2-simplex, where the three corners correspond to the three classes. Arrows point from the probability vectors predicted by an uncalibrated classifier to the probability vectors predicted by the same classifier after sigmoid calibration on a hold-out validation set. Colors indicate the true class of an instance (red: class 1, green: class 2, blue: class 3).</source>
          <target state="translated">この例は、シグモイド校正が、3クラスの分類問題の予測確率をどのように変化させるかを説明しています。図示されているのは、3つの角が3つのクラスに対応する標準の2-複素です。矢印は、校正されていない分類器によって予測された確率ベクトルから、ホールドアウト検証セットでのシグモイド校正後の同じ分類器によって予測された確率ベクトルを指しています。色は、インスタンスの真のクラスを示します(赤:クラス1、緑:クラス2、青:クラス3)。</target>
        </trans-unit>
        <trans-unit id="12c733d8527ccd2c1862324a52d9d453fa3717b9" translate="yes" xml:space="preserve">
          <source>This example illustrates how the Mahalanobis distances are affected by outlying data: observations drawn from a contaminating distribution are not distinguishable from the observations coming from the real, Gaussian distribution that one may want to work with. Using MCD-based Mahalanobis distances, the two populations become distinguishable. Associated applications are outliers detection, observations ranking, clustering, &amp;hellip; For visualization purpose, the cubic root of the Mahalanobis distances are represented in the boxplot, as Wilson and Hilferty suggest [2]</source>
          <target state="translated">この例は、マハラノビス距離が外れ値のデータによってどのように影響されるかを示しています。汚染分布から引き出された観測は、操作したい実際のガウス分布からの観測と区別できません。 MCDベースのマハラノビス距離を使用すると、2つの集団が区別できるようになります。関連するアプリケーションは、外れ値の検出、観測のランキング、クラスタリングなどです&amp;hellip;視覚化の目的で、マハラノビス距離の3次根は箱ひげ図で表されます。</target>
        </trans-unit>
        <trans-unit id="e6c2656adbcd3c5e59b375bc18300061f72c931d" translate="yes" xml:space="preserve">
          <source>This example illustrates how the early stopping can used in the &lt;a href=&quot;../../modules/generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;sklearn.ensemble.GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; model to achieve almost the same accuracy as compared to a model built without early stopping using many fewer estimators. This can significantly reduce training time, memory usage and prediction latency.</source>
          <target state="translated">この例は、&lt;a href=&quot;../../modules/generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt; &lt;code&gt;sklearn.ensemble.GradientBoostingClassifier&lt;/code&gt; &lt;/a&gt;モデルで早期停止を使用して、より少ない推定量を使用して早期停止なしで構築されたモデルと比較してほぼ同じ精度を達成する方法を示しています。これにより、トレーニング時間、メモリ使用量、予測待ち時間を大幅に削減できます。</target>
        </trans-unit>
        <trans-unit id="c9534999032b13d85edbba90f8420279af14435d" translate="yes" xml:space="preserve">
          <source>This example illustrates how the early stopping can used in the &lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;sklearn.linear_model.SGDClassifier&lt;/code&gt;&lt;/a&gt; model to achieve almost the same accuracy as compared to a model built without early stopping. This can significantly reduce training time. Note that scores differ between the stopping criteria even from early iterations because some of the training data is held out with the validation stopping criterion.</source>
          <target state="translated">この例は、早期停止を使用せずに構築されたモデルと比較して、ほぼ同じ精度を達成するために、&lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;sklearn.linear_model.SGDClassifier&lt;/code&gt; &lt;/a&gt;モデルで早期停止を使用する方法を示しています。これにより、トレーニング時間が大幅に短縮されます。一部のトレーニングデータは検証の停止基準で保持されているため、初期の反復からでもスコアは停止基準間で異なることに注意してください。</target>
        </trans-unit>
        <trans-unit id="c861b8fa50d5165e1e9cdc8044114c0ba76be7f1" translate="yes" xml:space="preserve">
          <source>This example illustrates how to apply different preprocessing and feature extraction pipelines to different subsets of features, using &lt;a href=&quot;../../modules/generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;sklearn.compose.ColumnTransformer&lt;/code&gt;&lt;/a&gt;. This is particularly handy for the case of datasets that contain heterogeneous data types, since we may want to scale the numeric features and one-hot encode the categorical ones.</source>
          <target state="translated">この例では、&lt;a href=&quot;../../modules/generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt; &lt;code&gt;sklearn.compose.ColumnTransformer&lt;/code&gt; &lt;/a&gt;を使用して、さまざまな前処理パイプラインと特徴抽出パイプラインを特徴の異なるサブセットに適用する方法を示します。これは、異種のデータ型を含むデータセットの場合に特に便利です。これは、数値の特徴をスケーリングし、カテゴリの特徴をワンホットエンコードしたい場合があるためです。</target>
        </trans-unit>
        <trans-unit id="7688b615fac5133028d275260a50b4a9e7d6a213" translate="yes" xml:space="preserve">
          <source>This example illustrates that GPR with a sum-kernel including a WhiteKernel can estimate the noise level of data. An illustration of the log-marginal-likelihood (LML) landscape shows that there exist two local maxima of LML.</source>
          <target state="translated">この例では、WhiteKernelを含む和カーネルを用いたGPRがデータのノイズレベルを推定できることを示しています。log-marginal-likelihood (LML)の図解は、LMLの局所的な最大値が2つ存在することを示しています。</target>
        </trans-unit>
        <trans-unit id="aab3b4258aabcd6bfe55c7c5adf04622c59229dc" translate="yes" xml:space="preserve">
          <source>This example illustrates that GPR with a sum-kernel including a WhiteKernel can estimate the noise level of data. An illustration of the log-marginal-likelihood (LML) landscape shows that there exist two local maxima of LML. The first corresponds to a model with a high noise level and a large length scale, which explains all variations in the data by noise. The second one has a smaller noise level and shorter length scale, which explains most of the variation by the noise-free functional relationship. The second model has a higher likelihood; however, depending on the initial value for the hyperparameters, the gradient-based optimization might also converge to the high-noise solution. It is thus important to repeat the optimization several times for different initializations.</source>
          <target state="translated">この例では、WhiteKernelを含む和カーネルを用いたGPRがデータのノイズレベルを推定できることを示しています。log-marginal-likelihood (LML)の図解では、LMLの局所的な最大値が2つ存在することが示されています。1つ目は、ノイズレベルが高く、長さスケールが大きいモデルに対応し、ノイズによるデータのすべての変動を説明します。2つ目は、ノイズレベルが小さく、長さスケールが短いモデルに対応し、ノイズのない関数関係によってほとんどの変動を説明します。2番目のモデルの方が尤度が高いですが、ハイパーパラメータの初期値によっては、勾配ベースの最適化も高ノイズ解に収束する可能性があります。そのため、異なる初期化に対して数回の最適化を繰り返すことが重要です。</target>
        </trans-unit>
        <trans-unit id="d19a1528c92e37a37fb4fd9cad2cad1ac3d91123" translate="yes" xml:space="preserve">
          <source>This example illustrates the differences between univariate F-test statistics and mutual information.</source>
          <target state="translated">この例は、一変量F-検定統計量と相互情報の違いを示しています。</target>
        </trans-unit>
        <trans-unit id="b411e019157b9b0953b37eb36e27bc6a8ffb3f5f" translate="yes" xml:space="preserve">
          <source>This example illustrates the effect of the parameters &lt;code&gt;gamma&lt;/code&gt; and &lt;code&gt;C&lt;/code&gt; of the Radial Basis Function (RBF) kernel SVM.</source>
          <target state="translated">この例は、放射基底関数（RBF）カーネルSVM のパラメーター &lt;code&gt;gamma&lt;/code&gt; と &lt;code&gt;C&lt;/code&gt; の効果を示しています。</target>
        </trans-unit>
        <trans-unit id="0f49634dcf1598fde3c403bd7ddd702e3816c634" translate="yes" xml:space="preserve">
          <source>This example illustrates the need for robust covariance estimation on a real data set. It is useful both for outlier detection and for a better understanding of the data structure.</source>
          <target state="translated">この例は、実データセットでのロバストな共分散推定の必要性を示しています。これは、外れ値の検出にも、データ構造の理解を深めるのにも役立ちます。</target>
        </trans-unit>
        <trans-unit id="dc09ff23a3cae32be328d47d0a0a7e64185cd75a" translate="yes" xml:space="preserve">
          <source>This example illustrates the predicted probability of GPC for an RBF kernel with different choices of the hyperparameters. The first figure shows the predicted probability of GPC with arbitrarily chosen hyperparameters and with the hyperparameters corresponding to the maximum log-marginal-likelihood (LML).</source>
          <target state="translated">この例は、ハイパーパラメータの選択が異なる場合の RBF カーネルの GPC の予測確率を示しています。最初の図は、任意に選択されたハイパーパラメータと最大対数限界尤度(LML)に対応するハイパーパラメータを用いた場合のGPCの予測確率を示しています。</target>
        </trans-unit>
        <trans-unit id="d49b62c58d1a1ad4b52cfdb4df97043fcb25dfc7" translate="yes" xml:space="preserve">
          <source>This example illustrates the predicted probability of GPC for an isotropic and anisotropic RBF kernel on a two-dimensional version for the iris-dataset. The anisotropic RBF kernel obtains slightly higher log-marginal-likelihood by assigning different length-scales to the two feature dimensions.</source>
          <target state="translated">この例は、虹彩データセットの 2 次元バージョンにおける等方性および異方性 RBF カーネルの GPC の予測確率を示している。異方性RBFカーネルは、2つの特徴次元に異なる長さのスケールを割り当てることで、わずかに高い対数限界尤度を得ることができます。</target>
        </trans-unit>
        <trans-unit id="f8e8b4dfa6bc8bbc237c34d26328609c3561c0d3" translate="yes" xml:space="preserve">
          <source>This example illustrates the predicted probability of GPC for an isotropic and anisotropic RBF kernel on a two-dimensional version for the iris-dataset. This illustrates the applicability of GPC to non-binary classification. The anisotropic RBF kernel obtains slightly higher log-marginal-likelihood by assigning different length-scales to the two feature dimensions.</source>
          <target state="translated">この例は、虹彩データセットの2次元版における等方性と異方性のRBFカーネルに対するGPCの予測確率を示している。これは、非二元分類へのGPCの適用可能性を示している。異方性RBFカーネルは、2つの特徴次元に異なる長さスケールを割り当てることで、わずかに高い対数限界尤度を得ることができる。</target>
        </trans-unit>
        <trans-unit id="d60d503f722a9b87495f756d071794c2e2c52164" translate="yes" xml:space="preserve">
          <source>This example illustrates the prior and posterior of a GPR with different kernels. Mean, standard deviation, and 10 samples are shown for both prior and posterior.</source>
          <target state="translated">この例は、異なるカーネルを持つGPRの事前および事後を示しています。平均、標準偏差、および10サンプルが、先行と事後の両方で示されています。</target>
        </trans-unit>
        <trans-unit id="7e4f09a45ae58596e6f6f82a5f372f88442c0679" translate="yes" xml:space="preserve">
          <source>This example illustrates the use of the &lt;a href=&quot;../../modules/multiclass#multiclass&quot;&gt;multioutput.MultiOutputRegressor&lt;/a&gt; meta-estimator to perform multi-output regression. A random forest regressor is used, which supports multi-output regression natively, so the results can be compared.</source>
          <target state="translated">この例は、&lt;a href=&quot;../../modules/multiclass#multiclass&quot;&gt;multioutput.MultiOutputRegressor&lt;/a&gt;メタ推定器を使用してマルチ出力回帰を実行する方法を示しています。ランダムフォレストリグレッサーが使用されます。これは、ネイティブにマルチ出力回帰をサポートするため、結果を比較できます。</target>
        </trans-unit>
        <trans-unit id="8a4413b8994df2fb3a9be31d12e5c3eff453a657" translate="yes" xml:space="preserve">
          <source>This example illustrates visually in the feature space a comparison by results using two different component analysis techniques.</source>
          <target state="translated">この例では、2つの異なる成分分析手法を用いた結果による比較を特徴空間で視覚的に説明しています。</target>
        </trans-unit>
        <trans-unit id="2cd2616385e5968100e838cea35843639b5d4649" translate="yes" xml:space="preserve">
          <source>This example is based on Figure 10.2 from Hastie et al 2009 &lt;a href=&quot;#id3&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; and illustrates the difference in performance between the discrete SAMME &lt;a href=&quot;#id4&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt; boosting algorithm and real SAMME.R boosting algorithm. Both algorithms are evaluated on a binary classification task where the target Y is a non-linear function of 10 input features.</source>
          <target state="translated">この例は、Hastie et al 2009 &lt;a href=&quot;#id3&quot; id=&quot;id1&quot;&gt;[1]の&lt;/a&gt;図10.2に基づいており、離散SAMME &lt;a href=&quot;#id4&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt;ブースティングアルゴリズムと実際のSAMME.Rブースティングアルゴリズムのパフォーマンスの違いを示しています。両方のアルゴリズムは、ターゲットYが10個の入力フィーチャの非線形関数であるバイナリ分類タスクで評価されます。</target>
        </trans-unit>
        <trans-unit id="f74f72b73b7f5fc12f50525ee3a073668f796ed6" translate="yes" xml:space="preserve">
          <source>This example is based on Section 5.4.3 of &amp;ldquo;Gaussian Processes for Machine Learning&amp;rdquo; [RW2006]. It illustrates an example of complex kernel engineering and hyperparameter optimization using gradient ascent on the log-marginal-likelihood. The data consists of the monthly average atmospheric CO2 concentrations (in parts per million by volume (ppmv)) collected at the Mauna Loa Observatory in Hawaii, between 1958 and 2001. The objective is to model the CO2 concentration as a function of the time t.</source>
          <target state="translated">この例は、「機械学習のためのガウス過程」[RW2006]のセクション5.4.3に基づいています。対数マージナル尤度での勾配上昇を使用した複雑なカーネルエンジニアリングとハイパーパラメーター最適化の例を示しています。データは、1958年から2001年の間にハワイのマウナロア天文台で収集された月間平均大気中CO2濃度（体積百万分の1（ppmv））で構成されています。CO2濃度を時間tの関数としてモデル化することが目的です。</target>
        </trans-unit>
        <trans-unit id="b3b5741f90375b259727a574f2a0488e7637e5bc" translate="yes" xml:space="preserve">
          <source>This example is based on Section 5.4.3 of &lt;a href=&quot;#rw2006&quot; id=&quot;id2&quot;&gt;[RW2006]&lt;/a&gt;. It illustrates an example of complex kernel engineering and hyperparameter optimization using gradient ascent on the log-marginal-likelihood. The data consists of the monthly average atmospheric CO2 concentrations (in parts per million by volume (ppmv)) collected at the Mauna Loa Observatory in Hawaii, between 1958 and 1997. The objective is to model the CO2 concentration as a function of the time t.</source>
          <target state="translated">この例は、&lt;a href=&quot;#rw2006&quot; id=&quot;id2&quot;&gt;[RW2006]の&lt;/a&gt;セクション5.4.3に基づいています。対数マージナル尤度で勾配上昇を使用した複雑なカーネルエンジニアリングとハイパーパラメーター最適化の例を示しています。データは、1958年から1997年の間にハワイのマウナロア天文台で収集された月間平均大気CO2濃度（体積百万分率（ppmv））で構成されています。CO2濃度を時間tの関数としてモデル化することが目的です。</target>
        </trans-unit>
        <trans-unit id="9ecc1c02a68f38d70271669af08df8c1497b1d98" translate="yes" xml:space="preserve">
          <source>This example is commented in the &lt;a href=&quot;../../tutorial/basic/tutorial#introduction&quot;&gt;tutorial section of the user manual&lt;/a&gt;.</source>
          <target state="translated">この例は&lt;a href=&quot;../../tutorial/basic/tutorial#introduction&quot;&gt;、ユーザーマニュアルのチュートリアルセクションで&lt;/a&gt;コメントされています。</target>
        </trans-unit>
        <trans-unit id="9143eb314f05280f157b4081755ef4be5d0d1857" translate="yes" xml:space="preserve">
          <source>This example is meant to illustrate situations where k-means will produce unintuitive and possibly unexpected clusters. In the first three plots, the input data does not conform to some implicit assumption that k-means makes and undesirable clusters are produced as a result. In the last plot, k-means returns intuitive clusters despite unevenly sized blobs.</source>
          <target state="translated">この例は、k-meansが直感的でない、おそらく予想外のクラスタを生成する状況を説明することを意図しています。最初の3つのプロットでは、入力データがk-meansの暗黙の仮定に適合しておらず、結果として望ましくないクラスタが生成されています。最後のプロットでは、k-meansは不均一なサイズのブロブにもかかわらず、直感的なクラスタを返します。</target>
        </trans-unit>
        <trans-unit id="fb84e7488212d58818869cb1a848aafb46dc6573" translate="yes" xml:space="preserve">
          <source>This example plots the covariance ellipsoids of each class and decision boundary learned by LDA and QDA. The ellipsoids display the double standard deviation for each class. With LDA, the standard deviation is the same for all the classes, while each class has its own standard deviation with QDA.</source>
          <target state="translated">この例では、LDAとQDAで学習した各クラスと決定境界の共分散楕円体をプロットしています。楕円体は、各クラスの標準偏差を2倍にして表示しています。LDAでは、標準偏差はすべてのクラスで同じですが、QDAでは各クラスごとに標準偏差を持っています。</target>
        </trans-unit>
        <trans-unit id="61cf8846c08926de131cab630666b0d7a1ff4033" translate="yes" xml:space="preserve">
          <source>This example plots the ellipsoids obtained from a toy dataset (mixture of three Gaussians) fitted by the &lt;code&gt;BayesianGaussianMixture&lt;/code&gt; class models with a Dirichlet distribution prior (&lt;code&gt;weight_concentration_prior_type='dirichlet_distribution'&lt;/code&gt;) and a Dirichlet process prior (&lt;code&gt;weight_concentration_prior_type='dirichlet_process'&lt;/code&gt;). On each figure, we plot the results for three different values of the weight concentration prior.</source>
          <target state="translated">この例では、 &lt;code&gt;BayesianGaussianMixture&lt;/code&gt; ガウス混合モデルモデルによって近似されたおもちゃのデータセット（3つのガウスの混合）から得られた楕円を、事前のディリクレ分布（ &lt;code&gt;weight_concentration_prior_type='dirichlet_distribution'&lt;/code&gt; ）と事前のディリクレプロセス（ &lt;code&gt;weight_concentration_prior_type='dirichlet_process'&lt;/code&gt; ）でプロットします。各図で、事前の重量濃度の3つの異なる値の結果をプロットします。</target>
        </trans-unit>
        <trans-unit id="b0d46d4faf4d4d45c7ba844c05b8ff931d890d6c" translate="yes" xml:space="preserve">
          <source>This example presents the different strategies implemented in KBinsDiscretizer:</source>
          <target state="translated">この例では、KBinsDiscretizer で実装されているさまざまな戦略を紹介します。</target>
        </trans-unit>
        <trans-unit id="aff95617011fc0f39a1d4573107679df90fa3c83" translate="yes" xml:space="preserve">
          <source>This example reproduces Figure 1 of Zhu et al &lt;a href=&quot;#id3&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; and shows how boosting can improve prediction accuracy on a multi-class problem. The classification dataset is constructed by taking a ten-dimensional standard normal distribution and defining three classes separated by nested concentric ten-dimensional spheres such that roughly equal numbers of samples are in each class (quantiles of the \(\chi^2\) distribution).</source>
          <target state="translated">この例では、Zhu et al &lt;a href=&quot;#id3&quot; id=&quot;id1&quot;&gt;[1]の&lt;/a&gt;図1を再現し、ブースティングによってマルチクラス問題の予測精度を向上させる方法を示します。分類データセットは、10次元の標準正規分布を取り、入れ子になった同心の10次元の球で区切られた3つのクラスを定義することによって構築され、各クラス（\（\ chi ^ 2 \）分布の分位数）にサンプルの数がほぼ等しくなるようにします。 ）。</target>
        </trans-unit>
        <trans-unit id="f0193d5eae04ea4c45d1272eb8f24ceb76514e1e" translate="yes" xml:space="preserve">
          <source>This example serves as a visual check that IPCA is able to find a similar projection of the data to PCA (to a sign flip), while only processing a few samples at a time. This can be considered a &amp;ldquo;toy example&amp;rdquo;, as IPCA is intended for large datasets which do not fit in main memory, requiring incremental approaches.</source>
          <target state="translated">この例は、IPCAがPCAへの（サインフリップへの）データの同様の投影を検出できる一方で、一度にいくつかのサンプルのみを処理できることを視覚的に確認する役割を果たします。これは「おもちゃの例」と考えることができます。IPCAは、メインメモリに収まらず、インクリメンタルアプローチが必要な大規模なデータセットを対象としているためです。</target>
        </trans-unit>
        <trans-unit id="3dfa9a56451c107730b94ff094ad060fe6660b05" translate="yes" xml:space="preserve">
          <source>This example should be taken with a grain of salt, as the intuition conveyed does not necessarily carry over to real datasets. Particularly in high-dimensional spaces, data can more easily be separated linearly. Moreover, using feature discretization and one-hot encoding increases the number of features, which easily lead to overfitting when the number of samples is small.</source>
          <target state="translated">この例は、伝えられた直感が必ずしも実際のデータセットに反映されるとは限らないので、大目に見てください。特に高次元空間では、データはより簡単に線形に分離することができます。また、特徴量の離散化やワンホット符号化を用いると、特徴量が増えてしまい、サンプル数が少ない場合にはオーバーフィットを起こしやすくなります。</target>
        </trans-unit>
        <trans-unit id="ad21e470ff2bffe875ca12e50c6f8a883eaa0515" translate="yes" xml:space="preserve">
          <source>This example shows an example usage of the &lt;code&gt;split&lt;/code&gt; method.</source>
          <target state="translated">この例は、 &lt;code&gt;split&lt;/code&gt; メソッドの使用例を示しています。</target>
        </trans-unit>
        <trans-unit id="05321a1b8964aa5eaada463be8f8b6ab44686817" translate="yes" xml:space="preserve">
          <source>This example shows characteristics of different anomaly detection algorithms on 2D datasets. Datasets contain one or two modes (regions of high density) to illustrate the ability of algorithms to cope with multimodal data.</source>
          <target state="translated">この例は、2Dデータセット上の異なる異常検出アルゴリズムの特徴を示しています。データセットには1つまたは2つのモード(高密度の領域)が含まれており、マルチモーダルデータに対処するアルゴリズムの能力を示しています。</target>
        </trans-unit>
        <trans-unit id="9671740bdc9e0f010272719df08d61d30b070724" translate="yes" xml:space="preserve">
          <source>This example shows characteristics of different clustering algorithms on datasets that are &amp;ldquo;interesting&amp;rdquo; but still in 2D. With the exception of the last dataset, the parameters of each of these dataset-algorithm pairs has been tuned to produce good clustering results. Some algorithms are more sensitive to parameter values than others.</source>
          <target state="translated">この例は、「興味深い」が2Dであるデータセットのさまざまなクラスタリングアルゴリズムの特性を示しています。最後のデータセットを除いて、これらのデータセットとアルゴリズムの各ペアのパラメーターは、適切なクラスタリング結果を生成するように調整されています。一部のアルゴリズムは、他のアルゴリズムよりもパラメーター値に敏感です。</target>
        </trans-unit>
        <trans-unit id="408c25df8162bc85c75adf89aefb6c4283aab313" translate="yes" xml:space="preserve">
          <source>This example shows characteristics of different linkage methods for hierarchical clustering on datasets that are &amp;ldquo;interesting&amp;rdquo; but still in 2D.</source>
          <target state="translated">この例は、「興味深い」が2Dのままであるデータセットの階層的クラスタリングのさまざまなリンケージメソッドの特性を示しています。</target>
        </trans-unit>
        <trans-unit id="ee904b77cbf769dbe7d1093eb852f864329f4bb5" translate="yes" xml:space="preserve">
          <source>This example shows how kernel density estimation (KDE), a powerful non-parametric density estimation technique, can be used to learn a generative model for a dataset. With this generative model in place, new samples can be drawn. These new samples reflect the underlying model of the data.</source>
          <target state="translated">この例では、強力なノンパラメトリック密度推定技術であるカーネル密度推定(KDE)を使用して、データセットの生成モデルを学習する方法を示します。この生成モデルがあると、新しいサンプルを描くことができます。これらの新しいサンプルは、データの基礎となるモデルを反映しています。</target>
        </trans-unit>
        <trans-unit id="54102d8f78c42d496181e5bcdf5a40bdaee3e42d" translate="yes" xml:space="preserve">
          <source>This example shows how quantile regression can be used to create prediction intervals.</source>
          <target state="translated">この例では、クォンタイル回帰がどのようにして予測区間を作成するために使用できるかを示しています。</target>
        </trans-unit>
        <trans-unit id="682ea376dc5fd413204f119c7903367cc1b71149" translate="yes" xml:space="preserve">
          <source>This example shows how to build a classification pipeline with a BernoulliRBM feature extractor and a &lt;a href=&quot;../../modules/generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; classifier. The hyperparameters of the entire model (learning rate, hidden layer size, regularization) were optimized by grid search, but the search is not reproduced here because of runtime constraints.</source>
          <target state="translated">この例は、BernoulliRBM特徴抽出器と&lt;a href=&quot;../../modules/generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt;分類器を使用して分類パイプラインを構築する方法を示しています。モデル全体のハイパーパラメータ（学習率、隠れ層サイズ、正則化）はグリッド検索によって最適化されましたが、実行時の制約のため、検索はここでは再現されません。</target>
        </trans-unit>
        <trans-unit id="e6287a37f5ab2f7e58b3aa65bfc9b371f8d2e434" translate="yes" xml:space="preserve">
          <source>This example shows how to obtain partial dependence plots from a &lt;a href=&quot;../../modules/generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; trained on the California housing dataset. The example is taken from &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="translated">この例は、カリフォルニアの住宅データセットでトレーニングされた&lt;a href=&quot;../../modules/generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; &lt;/a&gt;から部分依存プロットを取得する方法を示しています。例は&lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt;から取られています。</target>
        </trans-unit>
        <trans-unit id="dd4b844c3488b502b915a6b11d22b13911beaa74" translate="yes" xml:space="preserve">
          <source>This example shows how to perform univariate feature selection before running a SVC (support vector classifier) to improve the classification scores.</source>
          <target state="translated">この例では、SVC(サポートベクター分類器)を実行する前に一変量の特徴選択を行い、分類スコアを向上させる方法を示しています。</target>
        </trans-unit>
        <trans-unit id="47a26e628df959c3e5ed3ffe4f4f3490e8927a8d" translate="yes" xml:space="preserve">
          <source>This example shows how to plot some of the first layer weights in a MLPClassifier trained on the MNIST dataset.</source>
          <target state="translated">この例では,MNISTデータセット上で学習されたMLPClassifierの第1層の重みをプロットする方法を示しています.</target>
        </trans-unit>
        <trans-unit id="dd07bd7e7afed03f3c2a3c74458c42dc14028dfe" translate="yes" xml:space="preserve">
          <source>This example shows how to plot the decision surface for four SVM classifiers with different kernels.</source>
          <target state="translated">この例では、異なるカーネルを持つ4つのSVM分類器の決定面をプロットする方法を示しています。</target>
        </trans-unit>
        <trans-unit id="52dcf2b8bf47a153b3ba3beca30c9af85b850fad" translate="yes" xml:space="preserve">
          <source>This example shows how to use &lt;code&gt;cross_val_predict&lt;/code&gt; to visualize prediction errors.</source>
          <target state="translated">この例は、 &lt;code&gt;cross_val_predict&lt;/code&gt; を使用して予測エラーを視覚化する方法を示しています。</target>
        </trans-unit>
        <trans-unit id="351c02b1031f149a08df70cbeed39cd2a6bb9ec7" translate="yes" xml:space="preserve">
          <source>This example shows that Kernel PCA is able to find a projection of the data that makes data linearly separable.</source>
          <target state="translated">この例では、カーネルPCAがデータを直線的に分離可能にするデータの射影を見つけることができることを示している。</target>
        </trans-unit>
        <trans-unit id="607fdb6fda0285694fd1dd982f83082f2f9a6687" translate="yes" xml:space="preserve">
          <source>This example shows that imputing the missing values can give better results than discarding the samples containing any missing value. Imputing does not always improve the predictions, so please check via cross-validation. Sometimes dropping rows or using marker values is more effective.</source>
          <target state="translated">この例では、欠損値を入力することで、欠損値を含むサンプルを破棄するよりも良い結果が得られることを示しています。インプットが必ずしも予測値を改善するとは限らないので、クロス・バリデーションで確認してください。行を削除したり、マーカー値を使用したりすると、より効果的な場合があります。</target>
        </trans-unit>
        <trans-unit id="b6045a3110197ccfd64402c3c879acac73bdaadb" translate="yes" xml:space="preserve">
          <source>This example shows that model selection can be performed with Gaussian Mixture Models using information-theoretic criteria (BIC). Model selection concerns both the covariance type and the number of components in the model. In that case, AIC also provides the right result (not shown to save time), but BIC is better suited if the problem is to identify the right model. Unlike Bayesian procedures, such inferences are prior-free.</source>
          <target state="translated">この例は、情報理論的基準(BIC)を使用してガウス混合モデルでモデル選択が実行できることを示しています。モデル選択は、共分散型とモデル内の成分数の両方に関係します。その場合、AICも正しい結果を提供しますが(時間を節約するために示されていません)、問題が正しいモデルを特定することであれば、BICの方が適しています。ベイズ的手続きとは異なり、そのような推論は事前に自由である。</target>
        </trans-unit>
        <trans-unit id="7ed4db944a196b1cb5ab23d8834c95cd5421c757" translate="yes" xml:space="preserve">
          <source>This example shows that you can do non-linear regression with a linear model, using a pipeline to add non-linear features. Kernel methods extend this idea and can induce very high (even infinite) dimensional feature spaces.</source>
          <target state="translated">この例は、非線形特徴量を追加するパイプラインを使用して、線形モデルで非線形回帰を行うことができることを示しています。カーネル法はこのアイデアを拡張し、非常に高い(無限であっても)次元の特徴空間を誘導することができます。</target>
        </trans-unit>
        <trans-unit id="48dcc848c2d6e1561f6c336d60b0fb518f9ab59a" translate="yes" xml:space="preserve">
          <source>This example shows the ROC response of different datasets, created from K-fold cross-validation. Taking all of these curves, it is possible to calculate the mean area under curve, and see the variance of the curve when the training set is split into different subsets. This roughly shows how the classifier output is affected by changes in the training data, and how different the splits generated by K-fold cross-validation are from one another.</source>
          <target state="translated">この例は、K-フォールド交差検証から作成された異なるデータセットのROC応答を示します。これらの曲線をすべて取ると、曲線下の平均面積を計算でき、訓練集合が異なるサブセットに分割されたときの曲線の分散を見ることができます。これは、分類器の出力が訓練データの変化によってどのように影響を受けるか、また、K-フォールド・クロス・バリデーションによって生成された分割が互いにどのように異なるかを大まかに示します。</target>
        </trans-unit>
        <trans-unit id="7b92e840bf44fca60c7edc394c5ccf3da0546857" translate="yes" xml:space="preserve">
          <source>This example shows the effect of imposing a connectivity graph to capture local structure in the data. The graph is simply the graph of 20 nearest neighbors.</source>
          <target state="translated">この例では、データの局所的な構造を捉えるために接続性グラフを課すことの効果を示しています。このグラフは、単純に20個の最も近い隣人のグラフです。</target>
        </trans-unit>
        <trans-unit id="a122bd5b47879a72d10715b2e2741901d74ebd5f" translate="yes" xml:space="preserve">
          <source>This example shows the reconstruction of an image from a set of parallel projections, acquired along different angles. Such a dataset is acquired in &lt;strong&gt;computed tomography&lt;/strong&gt; (CT).</source>
          <target state="translated">この例は、異なる角度に沿って取得された一連の平行投影からの画像の再構成を示しています。このようなデータセットは、&lt;strong&gt;コンピューター断層撮影&lt;/strong&gt;（CT）で取得されます。</target>
        </trans-unit>
        <trans-unit id="277c7e399c7f521a9367c3279ba6605ffa32bb5b" translate="yes" xml:space="preserve">
          <source>This example shows the use of forests of trees to evaluate the importance of the pixels in an image classification task (faces). The hotter the pixel, the more important.</source>
          <target state="translated">この例は、画像分類タスク(顔)のピクセルの重要度を評価するための木の森の使用を示しています。画素の温度が高いほど重要度が高いことを示しています。</target>
        </trans-unit>
        <trans-unit id="181df9c721e88b620fbcd3038af372d2bc17958d" translate="yes" xml:space="preserve">
          <source>This example shows the use of multi-output estimator to complete images. The goal is to predict the lower half of a face given its upper half.</source>
          <target state="translated">この例は、画像を完成させるためのマルチ出力推定器の使用を示しています。目標は、顔の上半分を与えられた顔の下半分を予測することです。</target>
        </trans-unit>
        <trans-unit id="c194d9ad3fd820ff97a5b60a54a77ad5e096ac46" translate="yes" xml:space="preserve">
          <source>This example simulates a multi-label document classification problem. The dataset is generated randomly based on the following process:</source>
          <target state="translated">この例では、マルチラベル文書分類問題をシミュレートしています。データセットは、以下の処理に基づいてランダムに生成される。</target>
        </trans-unit>
        <trans-unit id="beca62f6aeebe1ac71c16bdf04b277689fc51da6" translate="yes" xml:space="preserve">
          <source>This example uses &lt;a href=&quot;../../modules/clustering#spectral-clustering&quot;&gt;Spectral clustering&lt;/a&gt; on a graph created from voxel-to-voxel difference on an image to break this image into multiple partly-homogeneous regions.</source>
          <target state="translated">この例では、画像のボクセルごとの差から作成されたグラフで&lt;a href=&quot;../../modules/clustering#spectral-clustering&quot;&gt;スペクトルクラスタリング&lt;/a&gt;を使用して、この画像を複数の部分的に均一な領域に分割します。</target>
        </trans-unit>
        <trans-unit id="b1b3f16a0bb367262a72b24da0008ca16f86a096" translate="yes" xml:space="preserve">
          <source>This example uses a large dataset of faces to learn a set of 20 x 20 images patches that constitute faces.</source>
          <target state="translated">この例では、顔を構成する20×20の画像パッチのセットを学習するために、顔の大規模なデータセットを使用しています。</target>
        </trans-unit>
        <trans-unit id="0ad2a4281006cf2ce3833b3619a37abf58d678e0" translate="yes" xml:space="preserve">
          <source>This example uses different scalers, transformers, and normalizers to bring the data within a pre-defined range.</source>
          <target state="translated">この例では、さまざまなスケーラ、トランスフォーマ、およびノーマライザを使用して、データを事前に定義された範囲内に収めるようにしています。</target>
        </trans-unit>
        <trans-unit id="e1bfbae38c6acd2b8b362eacb6ca0227cf12cdc6" translate="yes" xml:space="preserve">
          <source>This example uses the &lt;a href=&quot;../../modules/generated/sklearn.neighbors.kerneldensity#sklearn.neighbors.KernelDensity&quot;&gt;&lt;code&gt;sklearn.neighbors.KernelDensity&lt;/code&gt;&lt;/a&gt; class to demonstrate the principles of Kernel Density Estimation in one dimension.</source>
          <target state="translated">この例では、&lt;a href=&quot;../../modules/generated/sklearn.neighbors.kerneldensity#sklearn.neighbors.KernelDensity&quot;&gt; &lt;code&gt;sklearn.neighbors.KernelDensity&lt;/code&gt; &lt;/a&gt;クラスを使用して、カーネル密度推定の原理を1次元で示します。</target>
        </trans-unit>
        <trans-unit id="1c91a9c343e9d52fecff06520d2432c55d78e2a0" translate="yes" xml:space="preserve">
          <source>This example uses the &lt;code&gt;scipy.stats&lt;/code&gt; module, which contains many useful distributions for sampling parameters, such as &lt;code&gt;expon&lt;/code&gt;, &lt;code&gt;gamma&lt;/code&gt;, &lt;code&gt;uniform&lt;/code&gt; or &lt;code&gt;randint&lt;/code&gt;. In principle, any function can be passed that provides a &lt;code&gt;rvs&lt;/code&gt; (random variate sample) method to sample a value. A call to the &lt;code&gt;rvs&lt;/code&gt; function should provide independent random samples from possible parameter values on consecutive calls.</source>
          <target state="translated">この例では、 &lt;code&gt;scipy.stats&lt;/code&gt; モジュールを使用しています。このモジュールには、 &lt;code&gt;expon&lt;/code&gt; 、 &lt;code&gt;gamma&lt;/code&gt; 、 &lt;code&gt;uniform&lt;/code&gt; 、 &lt;code&gt;randint&lt;/code&gt; などのサンプリングパラメーターに役立つ多くの分布が含まれています。原則として、値をサンプリングする &lt;code&gt;rvs&lt;/code&gt; （ランダム変量サンプル）メソッドを提供する任意の関数を渡すことができます。 &lt;code&gt;rvs&lt;/code&gt; 関数の呼び出しは、連続する呼び出しで可能なパラメーター値から独立したランダムサンプルを提供する必要があります。</target>
        </trans-unit>
        <trans-unit id="d9381762b80079a0275cf5384c50652951b2c3b8" translate="yes" xml:space="preserve">
          <source>This example uses the only the first feature of the &lt;code&gt;diabetes&lt;/code&gt; dataset, in order to illustrate a two-dimensional plot of this regression technique. The straight line can be seen in the plot, showing how linear regression attempts to draw a straight line that will best minimize the residual sum of squares between the observed responses in the dataset, and the responses predicted by the linear approximation.</source>
          <target state="translated">この例では、この回帰手法の2次元プロットを示すために、 &lt;code&gt;diabetes&lt;/code&gt; データセットの最初の機能のみを使用しています。プロットに直線が表示され、線形回帰が、データセットで観測された応答と線形近似によって予測された応答との間の残差二乗和を最適化する直線を描画しようとする様子を示しています。</target>
        </trans-unit>
        <trans-unit id="c6bb5d76743a81844f0fc5afc16345d399cae103" translate="yes" xml:space="preserve">
          <source>This example visualizes some training loss curves for different stochastic learning strategies, including SGD and Adam. Because of time-constraints, we use several small datasets, for which L-BFGS might be more suitable. The general trend shown in these examples seems to carry over to larger datasets, however.</source>
          <target state="translated">この例は、SGDとAdamを含むさまざまな確率的学習戦略のトレーニング損失曲線を可視化したものです。時間的制約があるため、ここではいくつかの小さなデータセットを使用していますが、L-BFGS の方が適しているかもしれません。しかし、これらの例に示されている一般的な傾向は、より大きなデータセットにも当てはまるようです。</target>
        </trans-unit>
        <trans-unit id="65646a35859e04e16667e59a0e282463725f9c9e" translate="yes" xml:space="preserve">
          <source>This example visualizes the behavior of several common scikit-learn objects for comparison.</source>
          <target state="translated">この例では、比較のためにいくつかの一般的な scikit-learn オブジェクトの動作を視覚化しています。</target>
        </trans-unit>
        <trans-unit id="49dcb9492cd2c3de6ca468ca869fddbd4adf1109" translate="yes" xml:space="preserve">
          <source>This example visualizes the partitions given by several trees and shows how the transformation can also be used for non-linear dimensionality reduction or non-linear classification.</source>
          <target state="translated">この例は、いくつかの木によって与えられた分割を可視化し、変換が非線形次元削減や非線形分類にも使用できることを示しています。</target>
        </trans-unit>
        <trans-unit id="6888e16176d5ba984d30e5b2aecac9e36e3202eb" translate="yes" xml:space="preserve">
          <source>This example will also work by replacing &lt;code&gt;SVC(kernel=&quot;linear&quot;)&lt;/code&gt; with &lt;code&gt;SGDClassifier(loss=&quot;hinge&quot;)&lt;/code&gt;. Setting the &lt;code&gt;loss&lt;/code&gt; parameter of the &lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; equal to &lt;code&gt;hinge&lt;/code&gt; will yield behaviour such as that of a SVC with a linear kernel.</source>
          <target state="translated">この例は、 &lt;code&gt;SVC(kernel=&quot;linear&quot;)&lt;/code&gt; を &lt;code&gt;SGDClassifier(loss=&quot;hinge&quot;)&lt;/code&gt; に置き換えることでも機能します。設定 &lt;code&gt;loss&lt;/code&gt; のパラメータ&lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; こと&lt;/a&gt;に等しい &lt;code&gt;hinge&lt;/code&gt; そのような線形カーネルを持つSVCのような挙動をもたらします。</target>
        </trans-unit>
        <trans-unit id="08633b59361c5b4332dffd09b9ac681bbe920080" translate="yes" xml:space="preserve">
          <source>This example, inspired from Chen&amp;rsquo;s publication [1], shows a comparison of the estimated MSE of the LW and OAS methods, using Gaussian distributed data.</source>
          <target state="translated">この例は、Chenの出版物[1]からヒントを得て、ガウス分布データを使用して、LWおよびOASメソッドの推定MSEの比較を示しています。</target>
        </trans-unit>
        <trans-unit id="f9260a90e6c35e520398765702d07497fe04f1a8" translate="yes" xml:space="preserve">
          <source>This examples shows how a classifier is optimized by cross-validation, which is done using the &lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;sklearn.model_selection.GridSearchCV&lt;/code&gt;&lt;/a&gt; object on a development set that comprises only half of the available labeled data.</source>
          <target state="translated">この例は、利用可能なラベル付きデータの半分のみを含む開発セットで&lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;sklearn.model_selection.GridSearchCV&lt;/code&gt; &lt;/a&gt;オブジェクトを使用して行われる交差検証によって分類子が最適化される方法を示しています。</target>
        </trans-unit>
        <trans-unit id="e16048c7f7cf75798d53fe7e675cc81cd1d6af8b" translate="yes" xml:space="preserve">
          <source>This examples shows the use of forests of trees to evaluate the importance of features on an artificial classification task. The red bars are the feature importances of the forest, along with their inter-trees variability.</source>
          <target state="translated">この例は、人工的な分類タスクにおける特徴の重要性を評価するために、木の森を使用していることを示しています。赤い棒は、木の森の特徴の重要度と、その木間変動を示しています。</target>
        </trans-unit>
        <trans-unit id="4fb4f14900902539137295018be0a0c7a07b1094" translate="yes" xml:space="preserve">
          <source>This exercise is used in the &lt;a href=&quot;../../tutorial/statistical_inference/model_selection#cv-estimators-tut&quot;&gt;Cross-validated estimators&lt;/a&gt; part of the &lt;a href=&quot;../../tutorial/statistical_inference/model_selection#model-selection-tut&quot;&gt;Model selection: choosing estimators and their parameters&lt;/a&gt; section of the &lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;A tutorial on statistical-learning for scientific data processing&lt;/a&gt;.</source>
          <target state="translated">この演習は、&lt;a href=&quot;../../tutorial/statistical_inference/model_selection#model-selection-tut&quot;&gt;モデル選択の&lt;/a&gt;「&lt;a href=&quot;../../tutorial/statistical_inference/model_selection#cv-estimators-tut&quot;&gt;相互検証&lt;/a&gt;された推定量」の部分で使用されます。「&lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;科学データ処理のための統計学習に関するチュートリアル」&lt;/a&gt;の「推定量とそのパラメーターの選択」セクション。</target>
        </trans-unit>
        <trans-unit id="621b0c8349abb129c7bc150bd9745f46f49af3ab" translate="yes" xml:space="preserve">
          <source>This exercise is used in the &lt;a href=&quot;../../tutorial/statistical_inference/model_selection#cv-generators-tut&quot;&gt;Cross-validation generators&lt;/a&gt; part of the &lt;a href=&quot;../../tutorial/statistical_inference/model_selection#model-selection-tut&quot;&gt;Model selection: choosing estimators and their parameters&lt;/a&gt; section of the &lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;A tutorial on statistical-learning for scientific data processing&lt;/a&gt;.</source>
          <target state="translated">この演習は、&lt;a href=&quot;../../tutorial/statistical_inference/model_selection#model-selection-tut&quot;&gt;モデル選択&lt;/a&gt;の&lt;a href=&quot;../../tutorial/statistical_inference/model_selection#cv-generators-tut&quot;&gt;交差検証ジェネレーター&lt;/a&gt;部分で使用されます。「&lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;科学データ処理のための統計学習に関するチュートリアル」&lt;/a&gt;の「推定量とそのパラメーターの選択」セクション。</target>
        </trans-unit>
        <trans-unit id="6915ba6643fea2f9e387885428a6e6189c7df3bf" translate="yes" xml:space="preserve">
          <source>This exercise is used in the &lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#clf-tut&quot;&gt;Classification&lt;/a&gt; part of the &lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#supervised-learning-tut&quot;&gt;Supervised learning: predicting an output variable from high-dimensional observations&lt;/a&gt; section of the &lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;A tutorial on statistical-learning for scientific data processing&lt;/a&gt;.</source>
          <target state="translated">この演習は、&lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#supervised-learning-tut&quot;&gt;「教師あり学習：&lt;/a&gt;&lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;科学的データ処理のための統計学習に関するチュートリアル」&lt;/a&gt;の「高次元の観測からの出力変数の予測」の&lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#clf-tut&quot;&gt;分類の&lt;/a&gt;部分で使用されます。</target>
        </trans-unit>
        <trans-unit id="421684adc1a996556d28fe46ff4c101c2f3063ef" translate="yes" xml:space="preserve">
          <source>This exercise is used in the &lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#using-kernels-tut&quot;&gt;Using kernels&lt;/a&gt; part of the &lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#supervised-learning-tut&quot;&gt;Supervised learning: predicting an output variable from high-dimensional observations&lt;/a&gt; section of the &lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;A tutorial on statistical-learning for scientific data processing&lt;/a&gt;.</source>
          <target state="translated">この演習は、&lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#supervised-learning-tut&quot;&gt;「教師あり学習：&lt;/a&gt;&lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;科学的データ処理のための統計的学習に関するチュートリアル」&lt;/a&gt;の「高次元観測からの出力変数の予測」の&lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#using-kernels-tut&quot;&gt;カーネル&lt;/a&gt;の使用の部分で使用されます。</target>
        </trans-unit>
        <trans-unit id="dadb46eeaf7a2bf2f8f61dc107ba2d3f5d55d33a" translate="yes" xml:space="preserve">
          <source>This extends to the multiclass case as follows. Let the true labels for a set of samples be encoded as a 1-of-K binary indicator matrix \(Y\), i.e., \(y_{i,k} = 1\) if sample \(i\) has label \(k\) taken from a set of \(K\) labels. Let \(P\) be a matrix of probability estimates, with \(p_{i,k} = \operatorname{Pr}(t_{i,k} = 1)\). Then the log loss of the whole set is</source>
          <target state="translated">これは、マルチクラスの場合には、次のように拡張される。Let the true labels for a set of a samples is encoded as a 1-of-K binary indicator matrix ﾞ (Y\(Y)),i.e.e.if sample \(i))has label ﾞ (k)taken from a set of \(K)labels.\(P\)を確率推定行列とし、\(p_{i,k}=\operatorname{Pr}(t_{i,k}=1)とする。)とすると、集合全体の対数損失は</target>
        </trans-unit>
        <trans-unit id="826a67cf49f96f56e23921af61e52712fab61d33" translate="yes" xml:space="preserve">
          <source>This factory function wraps scoring functions for use in GridSearchCV and cross_val_score. It takes a score function, such as &lt;code&gt;accuracy_score&lt;/code&gt;, &lt;code&gt;mean_squared_error&lt;/code&gt;, &lt;code&gt;adjusted_rand_index&lt;/code&gt; or &lt;code&gt;average_precision&lt;/code&gt; and returns a callable that scores an estimator&amp;rsquo;s output.</source>
          <target state="translated">このファクトリー関数は、GridSearchCVおよびcross_val_scoreで使用するスコアリング関数をラップします。それは、次のような、スコア関数を取る &lt;code&gt;accuracy_score&lt;/code&gt; 、 &lt;code&gt;mean_squared_error&lt;/code&gt; 、 &lt;code&gt;adjusted_rand_index&lt;/code&gt; または &lt;code&gt;average_precision&lt;/code&gt; とそのスコア推定の出力呼び出し可能なを返します。</target>
        </trans-unit>
        <trans-unit id="5eb7a4eb6e2161d3d9f2a7b4c7010506ccf35ad1" translate="yes" xml:space="preserve">
          <source>This feature corresponds to the sepal length in cm. Once the quantile transformation applied, those landmarks approach closely the percentiles previously defined:</source>
          <target state="translated">この特徴は、cm単位のセパルの長さに対応しています。分位変換が適用されると、これらのランドマークは以前に定義されたパーセンタイルに近づきます。</target>
        </trans-unit>
        <trans-unit id="18a1d2c5a41fd4d57af7a6bb802060cade230322" translate="yes" xml:space="preserve">
          <source>This feature selection algorithm looks only at the features (X), not the desired outputs (y), and can thus be used for unsupervised learning.</source>
          <target state="translated">この特徴選択アルゴリズムは、目的の出力(y)ではなく、特徴(X)のみを見ているので、教師なし学習に使用することができます。</target>
        </trans-unit>
        <trans-unit id="677c582ff4a458e9dc8e636909bbbb985fe5cce6" translate="yes" xml:space="preserve">
          <source>This figure is created using the &lt;a href=&quot;generated/sklearn.preprocessing.polynomialfeatures#sklearn.preprocessing.PolynomialFeatures&quot;&gt;&lt;code&gt;PolynomialFeatures&lt;/code&gt;&lt;/a&gt; preprocessor. This preprocessor transforms an input data matrix into a new data matrix of a given degree. It can be used as follows:</source>
          <target state="translated">この図は、&lt;a href=&quot;generated/sklearn.preprocessing.polynomialfeatures#sklearn.preprocessing.PolynomialFeatures&quot;&gt; &lt;code&gt;PolynomialFeatures&lt;/code&gt; &lt;/a&gt;プリプロセッサを使用して作成されます。このプリプロセッサは、入力データマトリックスを指定された次数の新しいデータマトリックスに変換します。次のように使用できます。</target>
        </trans-unit>
        <trans-unit id="adbd1df9acbf84e51fe5dc83e34aca6a9423eabf" translate="yes" xml:space="preserve">
          <source>This figure shows an example of such an ROC curve:</source>
          <target state="translated">この図は、このようなROC曲線の一例を示しています。</target>
        </trans-unit>
        <trans-unit id="e1207da0df5038f5f29891db83b7d5022ead8471" translate="yes" xml:space="preserve">
          <source>This folder is used by some large dataset loaders to avoid downloading the data several times.</source>
          <target state="translated">このフォルダは、いくつかの大規模なデータセットローダーが、データを何度もダウンロードしないようにするために使用します。</target>
        </trans-unit>
        <trans-unit id="697a12fdadac01e298b3e16a8634659c2b054014" translate="yes" xml:space="preserve">
          <source>This format is a text-based format, with one sample per line. It does not store zero valued features hence is suitable for sparse dataset.</source>
          <target state="translated">このフォーマットはテキストベースのフォーマットで、1行に1サンプルです。ゼロ値特徴量を格納しないので、疎なデータセットに適しています。</target>
        </trans-unit>
        <trans-unit id="a2fe6f0ee6734c2a60bcbb1d0d95dc9dfd886002" translate="yes" xml:space="preserve">
          <source>This format is used as the default format for both svmlight and the libsvm command line programs.</source>
          <target state="translated">このフォーマットは、svmlight と libsvm コマンドラインプログラムのデフォルトフォーマットとして使用されます。</target>
        </trans-unit>
        <trans-unit id="a2e505f490185afab8e1242d5832b6872eb9a667" translate="yes" xml:space="preserve">
          <source>This formulation has two advantages over other ways of computing distances. First, it is computationally efficient when dealing with sparse data. Second, if one argument varies but the other remains unchanged, then &lt;code&gt;dot(x, x)&lt;/code&gt; and/or &lt;code&gt;dot(y, y)&lt;/code&gt; can be pre-computed.</source>
          <target state="translated">この公式には、距離を計算する他の方法に比べて2つの利点があります。1つ目は、スパースデータを処理する場合の計算効率です。第2に、1つの引数が変化しても、他の引数は変化しない場合、 &lt;code&gt;dot(x, x)&lt;/code&gt; や &lt;code&gt;dot(y, y)&lt;/code&gt; を事前に計算できます。</target>
        </trans-unit>
        <trans-unit id="fd76e45c139161a6c2340aa524dcf0429afb583e" translate="yes" xml:space="preserve">
          <source>This function computes Cohen&amp;rsquo;s kappa &lt;a href=&quot;#r219a3b9132e1-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;, a score that expresses the level of agreement between two annotators on a classification problem. It is defined as</source>
          <target state="translated">この関数は、コーエンのカッパ&lt;a href=&quot;#r219a3b9132e1-1&quot; id=&quot;id1&quot;&gt;[1]を&lt;/a&gt;計算します。これは、分類問題に関する2つのアノテーター間の一致のレベルを表すスコアです。次のように定義されます</target>
        </trans-unit>
        <trans-unit id="1a26f30c64bc915159ba37349551edb033f8db68" translate="yes" xml:space="preserve">
          <source>This function computes for each row in X, the index of the row of Y which is closest (according to the specified distance).</source>
          <target state="translated">この関数は,X の各行について,(指定された距離に応じて)最も近い Y の行のインデックスを計算します.</target>
        </trans-unit>
        <trans-unit id="40ca8ec3788866f2480b1619115207e644d05cac" translate="yes" xml:space="preserve">
          <source>This function computes for each row in X, the index of the row of Y which is closest (according to the specified distance). The minimal distances are also returned.</source>
          <target state="translated">この関数は,X の各行について,(指定された距離に応じて)最も近い Y の行のインデックスを求めます.最小距離も返されます.</target>
        </trans-unit>
        <trans-unit id="6e14f241e1c03d6b67a6c0f22515d375ae432487" translate="yes" xml:space="preserve">
          <source>This function crawls the module and gets all classes that inherit from BaseEstimator. Classes that are defined in test-modules are not included. By default meta_estimators such as GridSearchCV are also not included.</source>
          <target state="translated">この関数はモジュールをクロールし、BaseEstimatorを継承するすべてのクラスを取得します。テストモジュールで定義されているクラスは含まれません.デフォルトでは,GridSearchCVのようなmeta_estimatorも含まれません.</target>
        </trans-unit>
        <trans-unit id="311d27372cf5315019acfac7480657777c623446" translate="yes" xml:space="preserve">
          <source>This function does not try to extract features into a numpy array or scipy sparse matrix. In addition, if load_content is false it does not try to load the files in memory.</source>
          <target state="translated">この関数は,numpy配列やscipy疎行列に特徴を抽出しようとはしません.また,load_contentがfalseの場合は,メモリ上のファイルをロードしようとはしません.</target>
        </trans-unit>
        <trans-unit id="28042729acf4bf75d343c82f013b112dad91f4c8" translate="yes" xml:space="preserve">
          <source>This function generates a GraphViz representation of the decision tree, which is then written into &lt;code&gt;out_file&lt;/code&gt;. Once exported, graphical renderings can be generated using, for example:</source>
          <target state="translated">この関数は、決定木のGraphViz表現を生成し、それを &lt;code&gt;out_file&lt;/code&gt; に書き込みます。いったんエクスポートされると、グラフィックレンダリングは、たとえば以下を使用して生成できます。</target>
        </trans-unit>
        <trans-unit id="90af5dc07a0d6b1b987fbe6284966c35b8f7dbed" translate="yes" xml:space="preserve">
          <source>This function implements Test 1 in:</source>
          <target state="translated">この関数は、テスト1を実装しています。</target>
        </trans-unit>
        <trans-unit id="51510777ab8c25d02b45243629e172250d646e40" translate="yes" xml:space="preserve">
          <source>This function is called with the estimated model and the randomly selected data: &lt;code&gt;is_model_valid(model, X, y)&lt;/code&gt;. If its return value is False the current randomly chosen sub-sample is skipped. Rejecting samples with this function is computationally costlier than with &lt;code&gt;is_data_valid&lt;/code&gt;. &lt;code&gt;is_model_valid&lt;/code&gt; should therefore only be used if the estimated model is needed for making the rejection decision.</source>
          <target state="translated">この関数は、推定モデルとランダムに選択されたデータで &lt;code&gt;is_model_valid(model, X, y)&lt;/code&gt; れます：is_model_valid（model、X、y）。戻り値がFalseの場合、ランダムに選択された現在のサブサンプルはスキップされます。この関数を使用したサンプルの拒否は、 &lt;code&gt;is_data_valid&lt;/code&gt; を使用する場合よりも計算コストがかかります。したがって、 &lt;code&gt;is_model_valid&lt;/code&gt; は、拒否の決定を行うために推定モデルが必要な場合にのみ使用する必要があります。</target>
        </trans-unit>
        <trans-unit id="a4283f593950d3e9c84617d07a78fd011e78bfa4" translate="yes" xml:space="preserve">
          <source>This function is called with the randomly selected data before the model is fitted to it: &lt;code&gt;is_data_valid(X, y)&lt;/code&gt;. If its return value is False the current randomly chosen sub-sample is skipped.</source>
          <target state="translated">この関数は、モデルがフィットされる前にランダムに選択されたデータで呼び出されます： &lt;code&gt;is_data_valid(X, y)&lt;/code&gt; 。戻り値がFalseの場合、ランダムに選択された現在のサブサンプルはスキップされます。</target>
        </trans-unit>
        <trans-unit id="7289fd594a0de96a89a572bf0a7bd6e9501fda52" translate="yes" xml:space="preserve">
          <source>This function is equivalent to mapping load_svmlight_file over a list of files, except that the results are concatenated into a single, flat list and the samples vectors are constrained to all have the same number of features.</source>
          <target state="translated">この関数は,結果が単一のフラットリストに結合され,サンプルベクトルがすべて同じ数の特徴を持つように制約される点を除いては,ファイルのリスト上にload_svmlight_fileをマッピングするのと同等です.</target>
        </trans-unit>
        <trans-unit id="828fa7414b6e6676bd49f6624bf5ec1232d13777" translate="yes" xml:space="preserve">
          <source>This function makes it possible to compute this transformation for a fixed set of class labels known ahead of time.</source>
          <target state="translated">この関数は,事前に既知のクラスラベルの固定集合に対して,この変換を計算することを可能にします.</target>
        </trans-unit>
        <trans-unit id="910452d7cd5e91358a13a185cadee882cea17632" translate="yes" xml:space="preserve">
          <source>This function modifies the estimator in-place.</source>
          <target state="translated">この関数は、インプレースの推定器を変更します。</target>
        </trans-unit>
        <trans-unit id="3e0bd9f3948e27380d0112f277598d80d17269d2" translate="yes" xml:space="preserve">
          <source>This function requires the true binary value and the target scores, which can either be probability estimates of the positive class, confidence values, or binary decisions. Here is a small example of how to use the &lt;a href=&quot;generated/sklearn.metrics.roc_curve#sklearn.metrics.roc_curve&quot;&gt;&lt;code&gt;roc_curve&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">この関数には、真のバイナリ値とターゲットスコアが必要です。ターゲットスコアは、陽性クラスの確率推定、信頼値、またはバイナリ判定のいずれかです。&lt;a href=&quot;generated/sklearn.metrics.roc_curve#sklearn.metrics.roc_curve&quot;&gt; &lt;code&gt;roc_curve&lt;/code&gt; &lt;/a&gt;関数の使用例を次に示します。</target>
        </trans-unit>
        <trans-unit id="2ddc8c678c75b432a0f0fafa6490a9a69a784bf8" translate="yes" xml:space="preserve">
          <source>This function returns a score of the mean square difference between the actual outcome and the predicted probability of the possible outcome. The actual outcome has to be 1 or 0 (true or false), while the predicted probability of the actual outcome can be a value between 0 and 1.</source>
          <target state="translated">この関数は、実際の結果と予測される結果の確率との平均二乗差のスコアを返します。実際の結果は1か0(真か偽)でなければならず,実際の結果の予測確率は0から1の間の値をとることができます.</target>
        </trans-unit>
        <trans-unit id="fbdeef434a34fee928d2d9974f81cfc54768558d" translate="yes" xml:space="preserve">
          <source>This function returns posterior probabilities of classification according to each class on an array of test vectors X.</source>
          <target state="translated">この関数は,テストベクトルXの配列に対して,各クラスに応じた分類の事後確率を返します.</target>
        </trans-unit>
        <trans-unit id="f7adc46ef6325367984cfe5d6cabca879706d70d" translate="yes" xml:space="preserve">
          <source>This function returns the Silhouette Coefficient for each sample.</source>
          <target state="translated">この関数は、各サンプルのシルエット係数を返します。</target>
        </trans-unit>
        <trans-unit id="30221178098fdd3682a8c91454092d6226b25e4f" translate="yes" xml:space="preserve">
          <source>This function returns the mean Silhouette Coefficient over all samples. To obtain the values for each sample, use &lt;a href=&quot;sklearn.metrics.silhouette_samples#sklearn.metrics.silhouette_samples&quot;&gt;&lt;code&gt;silhouette_samples&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">この関数は、すべてのサンプルの平均シルエット係数を返します。各サンプルの値を取得するには、&lt;a href=&quot;sklearn.metrics.silhouette_samples#sklearn.metrics.silhouette_samples&quot;&gt; &lt;code&gt;silhouette_samples&lt;/code&gt; を&lt;/a&gt;使用します。</target>
        </trans-unit>
        <trans-unit id="bd812dc35d867d7152375e5009e2698c8db08fc0" translate="yes" xml:space="preserve">
          <source>This function simply returns the valid pairwise distance metrics. It exists to allow for a description of the mapping for each of the valid strings.</source>
          <target state="translated">この関数は,単に有効なペアワイズ距離メトリクスを返します.これは,有効な文字列のそれぞれに対するマッピングの記述を可能にするために存在します.</target>
        </trans-unit>
        <trans-unit id="fa4705a70e55596dcf2ace89a6d2a8d09a9fcccf" translate="yes" xml:space="preserve">
          <source>This function simply returns the valid pairwise distance metrics. It exists, however, to allow for a verbose description of the mapping for each of the valid strings.</source>
          <target state="translated">この関数は,単に有効なペアワイズ距離メトリクスを返します.しかし、有効な文字列ごとにマッピングの詳細な説明ができるようにするために存在します。</target>
        </trans-unit>
        <trans-unit id="d1a7a45215b31f0644d6e686c87b329e59419299" translate="yes" xml:space="preserve">
          <source>This function won&amp;rsquo;t compute the intercept.</source>
          <target state="translated">この関数は切片を計算しません。</target>
        </trans-unit>
        <trans-unit id="a808622a520f852134a2d8734b9e29ce0a669efe" translate="yes" xml:space="preserve">
          <source>This function works with dense 2D arrays only.</source>
          <target state="translated">この関数は,密な2次元配列のみで動作します.</target>
        </trans-unit>
        <trans-unit id="7541ac358f5bb3bc5dcdfc1193ff72d6ac233a66" translate="yes" xml:space="preserve">
          <source>This generator method yields the ensemble predicted class probabilities after each iteration of boosting and therefore allows monitoring, such as to determine the predicted class probabilities on a test set after each boost.</source>
          <target state="translated">この生成方法では、ブーストの各反復後にアンサンブル予測クラス確率が得られるため、各ブースト後にテストセットの予測クラス確率を決定するなどのモニタリングが可能になります。</target>
        </trans-unit>
        <trans-unit id="7c1ad29f5d19940cf714626cd821b0934b5bc400" translate="yes" xml:space="preserve">
          <source>This generator method yields the ensemble prediction after each iteration of boosting and therefore allows monitoring, such as to determine the prediction on a test set after each boost.</source>
          <target state="translated">この生成方法では、ブーストの各イテレーション後にアンサンブル予測が得られるため、ブースト後にテストセットの予測値を決定するなどのモニタリングが可能となる。</target>
        </trans-unit>
        <trans-unit id="acc74c06c673308a3e484c230b5ff2c8348cfe79" translate="yes" xml:space="preserve">
          <source>This generator method yields the ensemble score after each iteration of boosting and therefore allows monitoring, such as to determine the score on a test set after each boost.</source>
          <target state="translated">この生成方法は、ブーストの各反復後にアンサンブルスコアを生成するため、ブースト後にテストセットのスコアを決定するなどのモニタリングが可能です。</target>
        </trans-unit>
        <trans-unit id="cb7462acd1f1e763247c87d170997bea5c436272" translate="yes" xml:space="preserve">
          <source>This illustrates the &lt;code&gt;datasets.make_multilabel_classification&lt;/code&gt; dataset generator. Each sample consists of counts of two features (up to 50 in total), which are differently distributed in each of two classes.</source>
          <target state="translated">これは、 &lt;code&gt;datasets.make_multilabel_classification&lt;/code&gt; データセットジェネレーターを示しています。各サンプルは、2つの機能のカウント（合計50まで）で構成され、2つのクラスのそれぞれに異なる方法で分散されます。</target>
        </trans-unit>
        <trans-unit id="b80113eed9b4b9668cc4e8b638bede5d1f2cf638" translate="yes" xml:space="preserve">
          <source>This implementation bulk-computes all neighborhood queries, which increases the memory complexity to O(n.d) where d is the average number of neighbors, while original DBSCAN had memory complexity O(n). It may attract a higher memory complexity when querying these nearest neighborhoods, depending on the &lt;code&gt;algorithm&lt;/code&gt;.</source>
          <target state="translated">この実装はすべての近傍クエリを一括計算します。これにより、メモリの複雑度はO（nd）に増加します。ここで、dはネイバーの平均数ですが、元のDBSCANはメモリの複雑度O（n）を持ちました。 &lt;code&gt;algorithm&lt;/code&gt; によっては、これらの最近傍を照会するときに、メモリの複雑さが高くなる可能性があります。</target>
        </trans-unit>
        <trans-unit id="92eb61902d4dfd6571a464d87feff72fe7b32901" translate="yes" xml:space="preserve">
          <source>This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad, M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal Matching Pursuit Technical Report - CS Technion, April 2008. &lt;a href=&quot;http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf&quot;&gt;http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf&lt;/a&gt;</source>
          <target state="translated">この実装は、Rubinstein、R.、Zibulevsky、M.とElad、M &lt;a href=&quot;http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf&quot;&gt;.に基づいています。&lt;/a&gt;バッチ直交マッチング追跡技術レポートを使用したK-SVDアルゴリズムの効率的な実装-CS Technion、2008年4月。http：//www.cs。 technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf</target>
        </trans-unit>
        <trans-unit id="6fb112845601277f8931b295b857e73c1428c8fb" translate="yes" xml:space="preserve">
          <source>This implementation is by default not memory efficient because it constructs a full pairwise similarity matrix in the case where kd-trees or ball-trees cannot be used (e.g. with sparse matrices). This matrix will consume n^2 floats. A couple of mechanisms for getting around this are:</source>
          <target state="translated">この実装は,kd-木やボールツリーが使用できない場合(例えば,疎な行列の場合)に完全なペアワイズ類似度行列を構築するため,デフォルトではメモリ効率が良くありません.この行列は,n^2個のフロートを消費します.これを回避するには,いくつかの方法があります.</target>
        </trans-unit>
        <trans-unit id="cc51c30dcd01f51cada4be15777f17eb95eb7cbd" translate="yes" xml:space="preserve">
          <source>This implementation is not intended for large-scale applications. In particular, scikit-learn offers no GPU support. For much faster, GPU-based implementations, as well as frameworks offering much more flexibility to build deep learning architectures, see &lt;a href=&quot;http://scikit-learn.org/stable/related_projects.html#related-projects&quot;&gt;Related Projects&lt;/a&gt;.</source>
          <target state="translated">この実装は、大規模なアプリケーションを対象としたものではありません。特に、scikit-learnはGPUサポートを提供しません。より高速なGPUベースの実装と、ディープラーニングアーキテクチャを構築するためのはるかに高い柔軟性を提供するフレームワークについては、&lt;a href=&quot;http://scikit-learn.org/stable/related_projects.html#related-projects&quot;&gt;関連プロジェクトを&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="0e30a130e6449e6025aca5fcf59ecb737e97cb91" translate="yes" xml:space="preserve">
          <source>This implementation is written in Cython and is reasonably fast. However, a faster API-compatible loader is also available at:</source>
          <target state="translated">この実装はCythonで書かれており、それなりに高速です。しかし、より高速な API 互換のローダも用意されています。</target>
        </trans-unit>
        <trans-unit id="b488dd9d3cb1238d47d93805595214963db6dd0c" translate="yes" xml:space="preserve">
          <source>This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.</source>
          <target state="translated">この実装では,scipy.sparse.csr_matrixを用いてカウントの疎な表現を生成します.</target>
        </trans-unit>
        <trans-unit id="42bc7bbc3f5bd0df8efbfbad62976f6ec6db583b" translate="yes" xml:space="preserve">
          <source>This implementation provides the same results that 3 PLS packages provided in the R language (R-project):</source>
          <target state="translated">この実装は、R言語(R-project)で提供されている3つのPLSパッケージと同じ結果を提供します。</target>
        </trans-unit>
        <trans-unit id="09c013dbb84b7e3d406ea0732bc3a8236ed37cbd" translate="yes" xml:space="preserve">
          <source>This implementation provides the same results that the &amp;ldquo;plspm&amp;rdquo; package provided in the R language (R-project), using the function plsca(X, Y). Results are equal or collinear with the function &lt;code&gt;pls(..., mode = &quot;canonical&quot;)&lt;/code&gt; of the &amp;ldquo;mixOmics&amp;rdquo; package. The difference relies in the fact that mixOmics implementation does not exactly implement the Wold algorithm since it does not normalize y_weights to one.</source>
          <target state="translated">この実装は、関数plsca（X、Y）を使用して、R言語（Rプロジェクト）で提供される「plspm」パッケージと同じ結果を提供します。結果は、「mixOmics」パッケージの関数 &lt;code&gt;pls(..., mode = &quot;canonical&quot;)&lt;/code&gt; と等しいか、同一線上にあります。違いは、mixOmics実装がy_weightsを1に正規化しないため、Woldアルゴリズムを正確に実装していないという事実に依存しています。</target>
        </trans-unit>
        <trans-unit id="36e4a374c505873717456a086d5c9ed44e5157f6" translate="yes" xml:space="preserve">
          <source>This implementation will refuse to center scipy.sparse matrices since it would make them non-sparse and would potentially crash the program with memory exhaustion problems.</source>
          <target state="translated">この実装では、scipy.sparse行列のセンタリングを拒否します。これは、行列を非疎にしてしまい、メモリ枯渇の問題でプログラムがクラッシュする可能性があるからです。</target>
        </trans-unit>
        <trans-unit id="6684c1532df5f751b6b61c242ea952621dc3f4e8" translate="yes" xml:space="preserve">
          <source>This implementation works with data represented as dense and sparse numpy arrays of floating point values.</source>
          <target state="translated">この実装は,浮動小数点値の密で疎なnumpy配列として表現されたデータで動作します.</target>
        </trans-unit>
        <trans-unit id="b0df3cb22108e4cd0ed0fcd534ed03e427412c64" translate="yes" xml:space="preserve">
          <source>This implementation works with data represented as dense numpy arrays of floating point values for the features.</source>
          <target state="translated">この実装では,特徴量の浮動小数点値の密なnumpy配列として表されるデータで動作します.</target>
        </trans-unit>
        <trans-unit id="4abb3ee00da8e0ef45c7b10f884f8d272712ca81" translate="yes" xml:space="preserve">
          <source>This implementation works with data represented as dense numpy arrays or sparse scipy arrays of floating point values.</source>
          <target state="translated">この実装は、浮動小数点値の密なnumpy配列または疎なscipy配列で表現されたデータで動作します。</target>
        </trans-unit>
        <trans-unit id="c3c22c958df17cff584f8c572beeb762a9a0290e" translate="yes" xml:space="preserve">
          <source>This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM).</source>
          <target state="translated">この実装は,特徴量の浮動小数点値の密または疎な配列として表現されたデータで動作します.フィットするモデルは,損失パラメータで制御することができます.デフォルトでは,線形サポートベクターマシン(SVM)にフィットします.</target>
        </trans-unit>
        <trans-unit id="c43a7d8bb7931a79100804db2f074a29d45e4b6b" translate="yes" xml:space="preserve">
          <source>This improvement is not visible in the Silhouette Coefficient which is small for both as this measure seem to suffer from the phenomenon called &amp;ldquo;Concentration of Measure&amp;rdquo; or &amp;ldquo;Curse of Dimensionality&amp;rdquo; for high dimensional datasets such as text data. Other measures such as V-measure and Adjusted Rand Index are information theoretic based evaluation scores: as they are only based on cluster assignments rather than distances, hence not affected by the curse of dimensionality.</source>
          <target state="translated">テキストデータなどの高次元のデータセットの場合、このメジャーは「メジャーの集中」または「次元のカース」と呼ばれる現象に悩まされているように見えるため、この改善は両方に対して小さいシルエット係数では表示されません。V-measureやAdjusted Rand Indexなどの他のメジャーは、情報理論に基づく評価スコアです。これらは距離ではなくクラスター割り当てにのみ基づいているため、次元の呪いの影響を受けません。</target>
        </trans-unit>
        <trans-unit id="a0d4ffe805942e66e32866a4eb458e728074d78e" translate="yes" xml:space="preserve">
          <source>This initially creates clusters of points normally distributed (std=1) about vertices of an &lt;code&gt;n_informative&lt;/code&gt;-dimensional hypercube with sides of length &lt;code&gt;2*class_sep&lt;/code&gt; and assigns an equal number of clusters to each class. It introduces interdependence between these features and adds various types of further noise to the data.</source>
          <target state="translated">これは最初に、長さが &lt;code&gt;2*class_sep&lt;/code&gt; の辺を持つ &lt;code&gt;n_informative&lt;/code&gt; 次元の超立方体の頂点の周りに正規分布する点のクラスターを作成し（std = 1）、各クラスに同数のクラスターを割り当てます。これらの機能間の相互依存性を導入し、さまざまなタイプのノイズをデータに追加します。</target>
        </trans-unit>
        <trans-unit id="5b14c6be212126cf2e3bdc1fe1c6fe8f3c7dc46d" translate="yes" xml:space="preserve">
          <source>This interface is &lt;strong&gt;experimental&lt;/strong&gt; as at version 0.20 and subsequent releases may change attributes without notice (although there should only be minor changes to &lt;code&gt;data&lt;/code&gt; and &lt;code&gt;target&lt;/code&gt;).</source>
          <target state="translated">このインターフェースはバージョン0.20の&lt;strong&gt;実験的な&lt;/strong&gt;ものであり、後続のリリースでは通知なしに属性が変更される可能性があります（ただし、 &lt;code&gt;data&lt;/code&gt; と &lt;code&gt;target&lt;/code&gt; にはわずかな変更しかありません）。</target>
        </trans-unit>
        <trans-unit id="7f6be37b4617684744b3ccc169d2c583b6e3ddc1" translate="yes" xml:space="preserve">
          <source>This is a convenience alias to &lt;code&gt;resample(*arrays, replace=False)&lt;/code&gt; to do random permutations of the collections.</source>
          <target state="translated">これは、コレクションのランダム置換を行う &lt;code&gt;resample(*arrays, replace=False)&lt;/code&gt; 便利なエイリアスです。</target>
        </trans-unit>
        <trans-unit id="4632bc2ee98a17257db1d248b06f38b79a53d4ef" translate="yes" xml:space="preserve">
          <source>This is a convenience function; the transformation is done using the default settings for &lt;a href=&quot;sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text.CountVectorizer&lt;/code&gt;&lt;/a&gt;. For more advanced usage (stopword filtering, n-gram extraction, etc.), combine fetch_20newsgroups with a custom &lt;a href=&quot;sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text.CountVectorizer&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text.HashingVectorizer&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text.TfidfTransformer&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text.TfidfVectorizer&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは便利な機能です。変換は&lt;a href=&quot;sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;sklearn.feature_extraction.text.CountVectorizer&lt;/code&gt; &lt;/a&gt;のデフォルト設定を使用して行われます。より高度な使用法（ストップワードフィルタリング、n-gram抽出など）の場合は、fetch_20newsgroupsをカスタムの&lt;a href=&quot;sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;sklearn.feature_extraction.text.CountVectorizer&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt; &lt;code&gt;sklearn.feature_extraction.text.HashingVectorizer&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt; &lt;code&gt;sklearn.feature_extraction.text.TfidfTransformer&lt;/code&gt; &lt;/a&gt;または&lt;a href=&quot;sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt; &lt;code&gt;sklearn.feature_extraction.text.TfidfVectorizer&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="2b3dbf5e1c5e08d66c77786f2bcbc632afc0312a" translate="yes" xml:space="preserve">
          <source>This is a convenience routine for the sake of testing. For many metrics, the utilities in scipy.spatial.distance.cdist and scipy.spatial.distance.pdist will be faster.</source>
          <target state="translated">これはテストのための便利なルーチンです。多くのメトリクスでは、scipy.spatial.distance.cdist と scipy.spatial.distance.pdist のユーティリティの方が高速です。</target>
        </trans-unit>
        <trans-unit id="7a67e0a846a2ab3c88cb06fc2b7950cd30914abe" translate="yes" xml:space="preserve">
          <source>This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets. &lt;a href=&quot;https://goo.gl/U2Uwz2&quot;&gt;https://goo.gl/U2Uwz2&lt;/a&gt;</source>
          <target state="translated">これは、UCI ML乳がんウィスコンシン（診断）データセットのコピーです。&lt;a href=&quot;https://goo.gl/U2Uwz2&quot;&gt;https://goo.gl/U2Uwz2&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="22bae61d9be3213577df5087cb01b12cfdf8dff4" translate="yes" xml:space="preserve">
          <source>This is a copy of UCI ML Wine recognition datasets. &lt;a href=&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&quot;&gt;https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&lt;/a&gt;</source>
          <target state="translated">これは、UCI ML Wine認識データセットのコピーです。&lt;a href=&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&quot;&gt;https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c52f45448ee0e84b694b7c38bdbed7fd0e586461" translate="yes" xml:space="preserve">
          <source>This is a copy of UCI ML housing dataset. &lt;a href=&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/housing/&quot;&gt;https://archive.ics.uci.edu/ml/machine-learning-databases/housing/&lt;/a&gt;</source>
          <target state="translated">これは、UCI MLハウジングデータセットのコピーです。&lt;a href=&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/housing/&quot;&gt;https://archive.ics.uci.edu/ml/machine-learning-databases/housing/&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3dee2146876159a5a0b048cd24a612fae4a810ca" translate="yes" xml:space="preserve">
          <source>This is a copy of the test set of the UCI ML hand-written digits datasets &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits&quot;&gt;http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits&lt;/a&gt;</source>
          <target state="translated">これは、UCI ML手書き数字データセットのテストセットのコピーです&lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits&quot;&gt;。http：//archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="fd4a60c08b29c6d6af237f8cfa14740252c7d04a" translate="yes" xml:space="preserve">
          <source>This is a general function, given points on a curve. For computing the area under the ROC-curve, see &lt;a href=&quot;sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt;&lt;code&gt;roc_auc_score&lt;/code&gt;&lt;/a&gt;. For an alternative way to summarize a precision-recall curve, see &lt;a href=&quot;sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt;&lt;code&gt;average_precision_score&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは、曲線上の点が与えられた場合の一般的な関数です。ROC曲線の下の面積の計算については、&lt;a href=&quot;sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt; &lt;code&gt;roc_auc_score&lt;/code&gt; を&lt;/a&gt;参照してください。精度-再現率曲線を要約する別の方法については、&lt;a href=&quot;sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt; &lt;code&gt;average_precision_score&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="37510c6c60985c0ea76b5bcc4364db965e5a12fd" translate="yes" xml:space="preserve">
          <source>This is a shorthand for the ColumnTransformer constructor; it does not require, and does not permit, naming the transformers. Instead, they will be given names automatically based on their types. It also does not allow weighting.</source>
          <target state="translated">これは ColumnTransformer コンストラクタの略語で、変換器の名前を付ける必要はありませんし、許可することもできません。代わりに、変換器の型に基づいて自動的に名前が付けられます。また、重み付けもできません。</target>
        </trans-unit>
        <trans-unit id="022d95ed0540e35ea0bdce867e9a502603bc5f51" translate="yes" xml:space="preserve">
          <source>This is a shorthand for the FeatureUnion constructor; it does not require, and does not permit, naming the transformers. Instead, they will be given names automatically based on their types. It also does not allow weighting.</source>
          <target state="translated">これは FeatureUnion コンストラクタの略語で、変換器に名前を付ける必要はなく、許可もしていません。代わりに、変換器の型に基づいて自動的に名前が付けられます。また、重み付けもできません。</target>
        </trans-unit>
        <trans-unit id="52a890ca0cc5d284d366294db21e8c380349733e" translate="yes" xml:space="preserve">
          <source>This is a shorthand for the Pipeline constructor; it does not require, and does not permit, naming the estimators. Instead, their names will be set to the lowercase of their types automatically.</source>
          <target state="translated">これは、Pipeline コンストラクタの略語であり、エスティメー タに名前を付ける必要はなく、許可もされていません。代わりに、その名前は自動的にその型の小文字に設定されます。</target>
        </trans-unit>
        <trans-unit id="bcbf4cb6d3eb7ea12d02241a9a60f7a4e6044f4d" translate="yes" xml:space="preserve">
          <source>This is a wrapper for &lt;code&gt;estimator_.predict(X)&lt;/code&gt;.</source>
          <target state="translated">これは &lt;code&gt;estimator_.predict(X)&lt;/code&gt; のラッパーです。</target>
        </trans-unit>
        <trans-unit id="17ebe8027dbbfba976bb150f0e9172e06d0c02ec" translate="yes" xml:space="preserve">
          <source>This is a wrapper for &lt;code&gt;estimator_.score(X, y)&lt;/code&gt;.</source>
          <target state="translated">これは &lt;code&gt;estimator_.score(X, y)&lt;/code&gt; ラッパーです。</target>
        </trans-unit>
        <trans-unit id="602675ab661ad893c615b29b13c1d56146fcfa0b" translate="yes" xml:space="preserve">
          <source>This is an alternative to passing a &lt;code&gt;backend='backend_name'&lt;/code&gt; argument to the &lt;code&gt;Parallel&lt;/code&gt; class constructor. It is particularly useful when calling into library code that uses joblib internally but does not expose the backend argument in its own API.</source>
          <target state="translated">これは、 &lt;code&gt;Parallel&lt;/code&gt; クラスコンストラクターに &lt;code&gt;backend='backend_name'&lt;/code&gt; 引数を渡す代わりに使用できます。これは、joblibを内部で使用するライブラリコードを呼び出すときに特に役立ちますが、独自のAPIでバックエンド引数を公開しません。</target>
        </trans-unit>
        <trans-unit id="47f9c3947e84bb8a914aa6f2f19cb2c5e42e970f" translate="yes" xml:space="preserve">
          <source>This is an example of &lt;strong&gt;bias/variance tradeoff&lt;/strong&gt;: the larger the ridge &lt;code&gt;alpha&lt;/code&gt; parameter, the higher the bias and the lower the variance.</source>
          <target state="translated">これは、&lt;strong&gt;バイアスと分散のトレードオフの&lt;/strong&gt;例です。リッジ &lt;code&gt;alpha&lt;/code&gt; パラメータが大きいほど、バイアスが高くなり、分散が低くなります。</target>
        </trans-unit>
        <trans-unit id="d75c7c933c17fefbabe7c2e292b885d0ecac3a21" translate="yes" xml:space="preserve">
          <source>This is an example of applying &lt;a href=&quot;../../modules/generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;sklearn.decomposition.NMF&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.decomposition.latentdirichletallocation#sklearn.decomposition.LatentDirichletAllocation&quot;&gt;&lt;code&gt;sklearn.decomposition.LatentDirichletAllocation&lt;/code&gt;&lt;/a&gt; on a corpus of documents and extract additive models of the topic structure of the corpus. The output is a list of topics, each represented as a list of terms (weights are not shown).</source>
          <target state="translated">これは、ドキュメントのコーパスに&lt;a href=&quot;../../modules/generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;sklearn.decomposition.NMF&lt;/code&gt; &lt;/a&gt;および&lt;a href=&quot;../../modules/generated/sklearn.decomposition.latentdirichletallocation#sklearn.decomposition.LatentDirichletAllocation&quot;&gt; &lt;code&gt;sklearn.decomposition.LatentDirichletAllocation&lt;/code&gt; &lt;/a&gt;を適用し、コーパスのトピック構造の追加モデルを抽出する例です。出力はトピックのリストであり、それぞれが用語のリストとして表されます（重みは示されていません）。</target>
        </trans-unit>
        <trans-unit id="53176f2993974522405fa17c9a80a84e38a4969c" translate="yes" xml:space="preserve">
          <source>This is an example showing how scikit-learn can be used for classification using an out-of-core approach: learning from data that doesn&amp;rsquo;t fit into main memory. We make use of an online classifier, i.e., one that supports the partial_fit method, that will be fed with batches of examples. To guarantee that the features space remains the same over time we leverage a HashingVectorizer that will project each example into the same feature space. This is especially useful in the case of text classification where new features (words) may appear in each batch.</source>
          <target state="translated">これは、コア外のアプローチを使用した分類にscikit-learnを使用する方法を示す例です。メインメモリに適合しないデータから学習します。例のバッチが供給される、partial_fitメソッドをサポートするオンライン分類子を使用します。機能スペースが長期にわたって同じままであることを保証するために、各例を同じ機能スペースに投影するHashingVectorizerを利用します。これは、新しい機能（単語）が各バッチに表示されるテキスト分類の場合に特に役立ちます。</target>
        </trans-unit>
        <trans-unit id="1620bf9fc1f7795235eabc8e2a67657eca16390d" translate="yes" xml:space="preserve">
          <source>This is an example showing how scikit-learn can be used to classify documents by topics using a bag-of-words approach. This example uses a scipy.sparse matrix to store the features and demonstrates various classifiers that can efficiently handle sparse matrices.</source>
          <target state="translated">これは、単語袋アプローチを使用してトピック別に文書を分類するために scikit-learn がどのように使用できるかを示す例です。この例では、特徴を格納するために scipy.sparse 行列を使用し、スパース行列を効率的に扱える様々な分類器を示しています。</target>
        </trans-unit>
        <trans-unit id="687fdb042e4ef171de769c4722977550577ec678" translate="yes" xml:space="preserve">
          <source>This is an example showing how the scikit-learn can be used to cluster documents by topics using a bag-of-words approach. This example uses a scipy.sparse matrix to store the features instead of standard numpy arrays.</source>
          <target state="translated">これは、scikit-learnを使って、bag-of-wordsアプローチを使ってトピック別に文書をクラスタリングする方法を示す例です。この例では、標準的なnumpy配列の代わりにscipy.sparse行列を用いて特徴を格納しています。</target>
        </trans-unit>
        <trans-unit id="c505c2f7b70a5aa0d5582bdc56a7d9627b32a4d8" translate="yes" xml:space="preserve">
          <source>This is an example showing the prediction latency of various scikit-learn estimators.</source>
          <target state="translated">様々なscikit-learn推定器の予測待ち時間を示した例です。</target>
        </trans-unit>
        <trans-unit id="9e4f7a05490ee1267f03d9980bace7147baa0b76" translate="yes" xml:space="preserve">
          <source>This is an extension of the algorithm in scipy.stats.mode.</source>
          <target state="translated">これは scipy.stats.mode のアルゴリズムを拡張したものです。</target>
        </trans-unit>
        <trans-unit id="375819c22c211b4c7fc97205acd724c3a575f620" translate="yes" xml:space="preserve">
          <source>This is an implementation that uses the result of the previous model to speed up computations along the set of solutions, making it faster than sequentially calling LogisticRegression for the different parameters. Note that there will be no speedup with liblinear solver, since it does not handle warm-starting.</source>
          <target state="translated">これは、前のモデルの結果を利用して、解の集合に沿った計算を高速化する実装で、異なるパラメータに対して逐次的にLogisticRegressionを呼び出すよりも高速になります。liblinearソルバーはウォームスタートを処理しないので、liblinearソルバーを使っても高速化はしないことに注意してください。</target>
        </trans-unit>
        <trans-unit id="89098058da4c55a1db96b87aadaa162a0a15baba" translate="yes" xml:space="preserve">
          <source>This is assumed to implement the scikit-learn estimator interface. Either estimator needs to provide a &lt;code&gt;score&lt;/code&gt; function, or &lt;code&gt;scoring&lt;/code&gt; must be passed.</source>
          <target state="translated">これは、scikit-learn Estimatorインターフェースを実装すると想定されています。推定器が &lt;code&gt;score&lt;/code&gt; 関数を提供するか、スコア &lt;code&gt;scoring&lt;/code&gt; 渡す必要があります。</target>
        </trans-unit>
        <trans-unit id="a9f1a5b0fa7d00ad69170d1ab81bf1031dee11a2" translate="yes" xml:space="preserve">
          <source>This is called a &lt;a href=&quot;../../modules/generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; cross-validation.</source>
          <target state="translated">これは、&lt;a href=&quot;../../modules/generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt;交差検証と呼ばれます。</target>
        </trans-unit>
        <trans-unit id="2e974743bc0fdffbf7238debaf0ee76bb5a5d9b2" translate="yes" xml:space="preserve">
          <source>This is called cosine similarity, because Euclidean (L2) normalization projects the vectors onto the unit sphere, and their dot product is then the cosine of the angle between the points denoted by the vectors.</source>
          <target state="translated">これは、ユークリッド(L2)正規化によってベクトルが単位球に投影され、その点積がベクトルで示される点の間の角度の余弦となるため、余弦類似性と呼ばれています。</target>
        </trans-unit>
        <trans-unit id="9b01365512b47448f649e450ddd111360a73cfc3" translate="yes" xml:space="preserve">
          <source>This is called the &lt;a href=&quot;https://en.wikipedia.org/wiki/Curse_of_dimensionality&quot;&gt;curse of dimensionality&lt;/a&gt; and is a core problem that machine learning addresses.</source>
          <target state="translated">これは&lt;a href=&quot;https://en.wikipedia.org/wiki/Curse_of_dimensionality&quot;&gt;次元の呪い&lt;/a&gt;と呼ばれ、機械学習が対処する中心的な問題です。</target>
        </trans-unit>
        <trans-unit id="25e5e11d6a0e13a60841f1cb72db59989d03472f" translate="yes" xml:space="preserve">
          <source>This is currently implemented in the following classes:</source>
          <target state="translated">現在、以下のクラスで実装されています。</target>
        </trans-unit>
        <trans-unit id="a774a1be5070f83615f896d6d2ec16ebfbe92e4e" translate="yes" xml:space="preserve">
          <source>This is done in 2 steps:</source>
          <target state="translated">これは2つのステップで行います。</target>
        </trans-unit>
        <trans-unit id="0c564a0d4cfad247ff47792f5a12558130a84f0c" translate="yes" xml:space="preserve">
          <source>This is equivalent to fit followed by transform, but more efficiently implemented.</source>
          <target state="translated">これははめ込みに続いて変換を行うのと同じですが、より効率的に実装されています。</target>
        </trans-unit>
        <trans-unit id="831022bba18e9ed70a7a762cd8243e7523afeddb" translate="yes" xml:space="preserve">
          <source>This is especially useful when the whole dataset is too big to fit in memory at once.</source>
          <target state="translated">これは、データセット全体が大きすぎて一度にメモリに収まらない場合に特に便利です。</target>
        </trans-unit>
        <trans-unit id="75c0bef753e28aecf63e47221c52fe362a027981" translate="yes" xml:space="preserve">
          <source>This is implemented as &lt;code&gt;argmax(decision_function(X), axis=1)&lt;/code&gt; which will return the label of the class with most votes by estimators predicting the outcome of a decision for each possible class pair.</source>
          <target state="translated">これは &lt;code&gt;argmax(decision_function(X), axis=1)&lt;/code&gt; として実装され、可能な各クラスペアの決定の結果を予測する推定量によって、最も投票数の多いクラスのラベルを返します。</target>
        </trans-unit>
        <trans-unit id="9b2a6723fed7b2d139e18e341020f963dc7f8450" translate="yes" xml:space="preserve">
          <source>This is implemented by linking the points X into the graph of geodesic distances of the training data. First the &lt;code&gt;n_neighbors&lt;/code&gt; nearest neighbors of X are found in the training data, and from these the shortest geodesic distances from each point in X to each point in the training data are computed in order to construct the kernel. The embedding of X is the projection of this kernel onto the embedding vectors of the training set.</source>
          <target state="translated">これは、ポイントXをトレーニングデータの測地線距離のグラフにリンクすることによって実装されます。最初に、Xの最近傍の &lt;code&gt;n_neighbors&lt;/code&gt; がトレーニングデータで見つかり、カーネルからXの各ポイントからトレーニングデータの各ポイントまでの最短測地線距離が計算されます。Xの埋め込みは、このカーネルをトレーニングセットの埋め込みベクトルに投影したものです。</target>
        </trans-unit>
        <trans-unit id="51ae8a82b3eff6fb295f70aa28171d41c73ac3db" translate="yes" xml:space="preserve">
          <source>This is implemented in &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform&quot;&gt;&lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis.transform&lt;/code&gt;&lt;/a&gt;. The desired dimensionality can be set using the &lt;code&gt;n_components&lt;/code&gt; constructor parameter. This parameter has no influence on &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.fit&quot;&gt;&lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis.fit&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.predict&quot;&gt;&lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis.predict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは、&lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform&quot;&gt; &lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis.transform&lt;/code&gt; に&lt;/a&gt;実装されています。目的の次元は、 &lt;code&gt;n_components&lt;/code&gt; コンストラクター・パラメーターを使用して設定できます。このパラメーターは、&lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.fit&quot;&gt; &lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis.fit&lt;/code&gt; &lt;/a&gt;または&lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.predict&quot;&gt; &lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis.predict&lt;/code&gt; に&lt;/a&gt;影響を与えません。</target>
        </trans-unit>
        <trans-unit id="de8220f3c95931fc4241bdd5334e66fca04da2f0" translate="yes" xml:space="preserve">
          <source>This is known as &lt;a href=&quot;../../modules/generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは&lt;a href=&quot;../../modules/generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt;と呼ばれます。</target>
        </trans-unit>
        <trans-unit id="1d1ef16c8ffe6df8c7a1a81b132f67cdda1b92ea" translate="yes" xml:space="preserve">
          <source>This is more efficient than calling fit followed by transform.</source>
          <target state="translated">これは、はめ込みに続いて変換を呼び出すよりも効率的です。</target>
        </trans-unit>
        <trans-unit id="1dfb3afc660617ced3002fefa24847b5bb1a14dd" translate="yes" xml:space="preserve">
          <source>This is mostly equivalent to calling:</source>
          <target state="translated">これはほとんどの場合、呼び出しに相当します。</target>
        </trans-unit>
        <trans-unit id="bbd51f304b678a157464ff7ab03c52123d04218d" translate="yes" xml:space="preserve">
          <source>This is not a symmetric function.</source>
          <target state="translated">これは対称関数ではありません。</target>
        </trans-unit>
        <trans-unit id="0b5c42967b0e34c52656dd80a4e659c6f0fa2181" translate="yes" xml:space="preserve">
          <source>This is not exactly the same as &lt;code&gt;sklearn.metrics.additive_chi2_kernel&lt;/code&gt;. The authors of &lt;a href=&quot;#vz2010&quot; id=&quot;id4&quot;&gt;[VZ2010]&lt;/a&gt; prefer the version above as it is always positive definite. Since the kernel is additive, it is possible to treat all components \(x_i\) separately for embedding. This makes it possible to sample the Fourier transform in regular intervals, instead of approximating using Monte Carlo sampling.</source>
          <target state="translated">これは &lt;code&gt;sklearn.metrics.additive_chi2_kernel&lt;/code&gt; とまったく同じではありません。&lt;a href=&quot;#vz2010&quot; id=&quot;id4&quot;&gt;[VZ2010]&lt;/a&gt;の作者は、常に正定であるため、上記のバージョンを好みます。カーネルは追加的であるため、すべてのコンポーネントを\（x_i \）で個別に処理して埋め込むことができます。これにより、モンテカルロサンプリングを使用して近似する代わりに、フーリエ変換を一定の間隔でサンプリングできます。</target>
        </trans-unit>
        <trans-unit id="f7f802fcac19c1c8ed1c55f673312d761a2c94a7" translate="yes" xml:space="preserve">
          <source>This is not the case for &lt;a href=&quot;generated/sklearn.metrics.completeness_score#sklearn.metrics.completeness_score&quot;&gt;&lt;code&gt;completeness_score&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.metrics.homogeneity_score#sklearn.metrics.homogeneity_score&quot;&gt;&lt;code&gt;homogeneity_score&lt;/code&gt;&lt;/a&gt;: both are bound by the relationship:</source>
          <target state="translated">これは、&lt;a href=&quot;generated/sklearn.metrics.completeness_score#sklearn.metrics.completeness_score&quot;&gt; &lt;code&gt;completeness_score&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;generated/sklearn.metrics.homogeneity_score#sklearn.metrics.homogeneity_score&quot;&gt; &lt;code&gt;homogeneity_score&lt;/code&gt; の&lt;/a&gt;場合とは異なります。どちらも関係によってバインドされます。</target>
        </trans-unit>
        <trans-unit id="5d73f5b087f2ecb2dadb8778d3d18cfc19507034" translate="yes" xml:space="preserve">
          <source>This is not true for &lt;code&gt;mutual_info_score&lt;/code&gt;, which is therefore harder to judge:</source>
          <target state="translated">これは、 &lt;code&gt;mutual_info_score&lt;/code&gt; には当てはまりません。したがって、判断が難しくなります。</target>
        </trans-unit>
        <trans-unit id="982101a3d677907e48e034c807cde26531a0468b" translate="yes" xml:space="preserve">
          <source>This is only available if no vocabulary was given.</source>
          <target state="translated">これは、語彙が与えられなかった場合にのみ利用可能です。</target>
        </trans-unit>
        <trans-unit id="0ca1a333516e3b52907835d092958662636ea528" translate="yes" xml:space="preserve">
          <source>This is particularly important for doing grid searches:</source>
          <target state="translated">これは、グリッド検索を行うために特に重要です。</target>
        </trans-unit>
        <trans-unit id="2413be66af5e312ff97e484aff33c56868971fbe" translate="yes" xml:space="preserve">
          <source>This is perhaps the best known database to be found in the pattern recognition literature. Fisher&amp;rsquo;s paper is a classic in the field and is referenced frequently to this day. (See Duda &amp;amp; Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other.</source>
          <target state="translated">これはおそらく、パターン認識の文献にある最もよく知られているデータベースです。フィッシャーの論文は、この分野での古典であり、今日でも頻繁に参照されています。（たとえば、Duda＆Hartを参照してください。）データセットには、それぞれ50個のインスタンスの3つのクラスが含まれています。各クラスは、アイリス植物のタイプを示します。1つのクラスは他の2つのクラスから線形的に分離可能です。後者は互いに直線的に分離できません。</target>
        </trans-unit>
        <trans-unit id="32ef6d8e689e0cbccf726fb7a94339c2864c487f" translate="yes" xml:space="preserve">
          <source>This is present only if &lt;code&gt;refit&lt;/code&gt; is not False.</source>
          <target state="translated">これは、 &lt;code&gt;refit&lt;/code&gt; がFalseでない場合にのみ存在します。</target>
        </trans-unit>
        <trans-unit id="717414c2af196799a3d1dc1aec1269a995318377" translate="yes" xml:space="preserve">
          <source>This is similar to the error set size, but weighted by the number of relevant and irrelevant labels. The best performance is achieved with a ranking loss of zero.</source>
          <target state="translated">これはエラーセットのサイズに似ていますが、関連ラベルと無関係ラベルの数で重み付けされています。ランク付けの損失がゼロになると、最高のパフォーマンスが得られます。</target>
        </trans-unit>
        <trans-unit id="00034266cafd87d919959c8134650d981f2c4466" translate="yes" xml:space="preserve">
          <source>This is the class and function reference of scikit-learn. Please refer to the &lt;a href=&quot;http://scikit-learn.org/stable/user_guide.html#user-guide&quot;&gt;full user guide&lt;/a&gt; for further details, as the class and function raw specifications may not be enough to give full guidelines on their uses. For reference on concepts repeated across the API, see &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#glossary&quot;&gt;Glossary of Common Terms and API Elements&lt;/a&gt;.</source>
          <target state="translated">これはscikit-learnのクラスと関数のリファレンスです。クラスと関数のraw仕様では、それらの使用に関する完全なガイドラインを提供するのに十分でない場合があるため、詳細については&lt;a href=&quot;http://scikit-learn.org/stable/user_guide.html#user-guide&quot;&gt;完全なユーザーガイド&lt;/a&gt;を参照してください。API全体で繰り返される概念のリファレンスについては、&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#glossary&quot;&gt;用語集とAPI要素の用語集を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="9c0b38fb17b983574b86706f2172c4c4bac1c6a5" translate="yes" xml:space="preserve">
          <source>This is the loss function used in (multinomial) logistic regression and extensions of it such as neural networks, defined as the negative log-likelihood of the true labels given a probabilistic classifier&amp;rsquo;s predictions. The log loss is only defined for two or more labels. For a single sample with true label yt in {0,1} and estimated probability yp that yt = 1, the log loss is</source>
          <target state="translated">これは、（多項）ロジスティック回帰とニューラルネットワークなどの拡張で使用される損失関数であり、確率的分類子の予測が与えられた真のラベルの負の対数尤度として定義されます。ログ損失は、2つ以上のラベルに対してのみ定義されます。真のラベルytが{0,1}であり、推定確率ypがyt = 1である単一のサンプルの場合、ログ損失は</target>
        </trans-unit>
        <trans-unit id="a9111de5c7f4d9db0e2dc4faf926c5c47ae0eb12" translate="yes" xml:space="preserve">
          <source>This is the result of calling &lt;code&gt;method&lt;/code&gt;</source>
          <target state="translated">これは &lt;code&gt;method&lt;/code&gt; を呼び出した結果です</target>
        </trans-unit>
        <trans-unit id="611492c50f944d397ce592f12eabee4801e28de1" translate="yes" xml:space="preserve">
          <source>This is the structured version, that takes into account some topological structure between samples.</source>
          <target state="translated">これは、サンプル間のトポロジカル構造を考慮した構造化バージョンです。</target>
        </trans-unit>
        <trans-unit id="eb0a6c68cdbb70ef7265210b8b8760243faa6912" translate="yes" xml:space="preserve">
          <source>This is useful for fitting an intercept term with implementations which cannot otherwise fit it directly.</source>
          <target state="translated">これは、他の方法では直接当てはめることができない実装で切片項を当てはめるのに便利です。</target>
        </trans-unit>
        <trans-unit id="1c420e62ce6697ac415dbca5798c0153080b025c" translate="yes" xml:space="preserve">
          <source>This is useful if the stored attributes of a previously used model has to be reused. If set to False, then the coefficients will be rewritten for every call to fit. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">これは、以前に使用されたモデルの格納された属性を再利用する必要がある場合に役立ちます。Falseに設定すると、係数はすべての呼び出しに合わせて書き換えられます。&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;用語集を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="42f2c0052d88e51f5df6c26752c192f81040ec01" translate="yes" xml:space="preserve">
          <source>This kernel is a popular choice for computing the similarity of documents represented as tf-idf vectors. &lt;a href=&quot;generated/sklearn.metrics.pairwise.cosine_similarity#sklearn.metrics.pairwise.cosine_similarity&quot;&gt;&lt;code&gt;cosine_similarity&lt;/code&gt;&lt;/a&gt; accepts &lt;code&gt;scipy.sparse&lt;/code&gt; matrices. (Note that the tf-idf functionality in &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; can produce normalized vectors, in which case &lt;a href=&quot;generated/sklearn.metrics.pairwise.cosine_similarity#sklearn.metrics.pairwise.cosine_similarity&quot;&gt;&lt;code&gt;cosine_similarity&lt;/code&gt;&lt;/a&gt; is equivalent to &lt;a href=&quot;generated/sklearn.metrics.pairwise.linear_kernel#sklearn.metrics.pairwise.linear_kernel&quot;&gt;&lt;code&gt;linear_kernel&lt;/code&gt;&lt;/a&gt;, only slower.)</source>
          <target state="translated">このカーネルは、tf-idfベクトルとして表されるドキュメントの類似性を計算するための一般的な選択です。&lt;a href=&quot;generated/sklearn.metrics.pairwise.cosine_similarity#sklearn.metrics.pairwise.cosine_similarity&quot;&gt; &lt;code&gt;cosine_similarity&lt;/code&gt; &lt;/a&gt;は &lt;code&gt;scipy.sparse&lt;/code&gt; 行列を受け入れます。（ &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; の tf-idf機能は正規化されたベクトルを生成できることに注意してください。この場合、&lt;a href=&quot;generated/sklearn.metrics.pairwise.cosine_similarity#sklearn.metrics.pairwise.cosine_similarity&quot;&gt; &lt;code&gt;cosine_similarity&lt;/code&gt; &lt;/a&gt;はlinear_kernelと同等で&lt;a href=&quot;generated/sklearn.metrics.pairwise.linear_kernel#sklearn.metrics.pairwise.linear_kernel&quot;&gt; &lt;code&gt;linear_kernel&lt;/code&gt; &lt;/a&gt;、遅くなります。）</target>
        </trans-unit>
        <trans-unit id="f1fbcea40cf4aba903be6eddf36c859a1718e3b8" translate="yes" xml:space="preserve">
          <source>This kernel is infinitely differentiable, which implies that GPs with this kernel as covariance function have mean square derivatives of all orders, and are thus very smooth.</source>
          <target state="translated">このカーネルは無限に微分可能であり、このカーネルを共分散関数とするGPはすべての次数の平均二乗導関数を持ち、したがって非常に滑らかであることを示唆している。</target>
        </trans-unit>
        <trans-unit id="6ad60f9f3ef06c9a3898414b0afc593eaff20c1d" translate="yes" xml:space="preserve">
          <source>This kernel is infinitely differentiable, which implies that GPs with this kernel as covariance function have mean square derivatives of all orders, and are thus very smooth. The prior and posterior of a GP resulting from an RBF kernel are shown in the following figure:</source>
          <target state="translated">このカーネルは無限に微分可能であり,このカーネルを共分散関数として持つGPはすべての次数の平均二乗導関数を持ち,したがって非常に滑らかであることを意味します.RBF カーネルから得られる GP の先行と事後を下図に示す.</target>
        </trans-unit>
        <trans-unit id="6cbded70a18b870dfff7fda8d59e204ebd77900f" translate="yes" xml:space="preserve">
          <source>This kind of singular profiles is often seen in practice, for instance:</source>
          <target state="translated">このような特異なプロファイルは、例えば、実際によく見られます。</target>
        </trans-unit>
        <trans-unit id="1df4414d1e47e9ea41e98b8c6e13b5eeb49e06ac" translate="yes" xml:space="preserve">
          <source>This left out portion can be used to estimate the generalization error without having to rely on a separate validation set. This estimate comes &amp;ldquo;for free&amp;rdquo; as no additional data is needed and can be used for model selection.</source>
          <target state="translated">この省略された部分は、別の検証セットに依存する必要なく、汎化エラーを推定するために使用できます。追加のデータは必要なく、モデルの選択に使用できるため、この見積もりは「無料」で提供されます。</target>
        </trans-unit>
        <trans-unit id="4f52c7809ab985057ba93af9b88459b0d10b33a2" translate="yes" xml:space="preserve">
          <source>This makes sure that the loss function is not heavily influenced by the outliers while not completely ignoring their effect.</source>
          <target state="translated">これにより、損失関数が外れ値の影響を完全に無視することなく、外れ値の影響を大きく受けないようにしています。</target>
        </trans-unit>
        <trans-unit id="4826ff4b754b03a246d73c38aa2c971ffdf335e1" translate="yes" xml:space="preserve">
          <source>This means each weight \(w_{i}\) is drawn from a Gaussian distribution, centered on zero and with a precision \(\lambda_{i}\):</source>
          <target state="translated">これは、それぞれの重みが、ガウス分布から描かれ、ゼロを中心に、精度を持っていることを意味しています。</target>
        </trans-unit>
        <trans-unit id="de2308f740624c092eea038ac13deb5af2b1fa80" translate="yes" xml:space="preserve">
          <source>This means that any classifiers handling multi-output multiclass or multi-task classification tasks, support the multi-label classification task as a special case. Multi-task classification is similar to the multi-output classification task with different model formulations. For more information, see the relevant estimator documentation.</source>
          <target state="translated">つまり、マルチ出力マルチクラスまたはマルチタスク分類タスクを扱う任意の分類器は、特殊なケースとしてマルチラベル分類タスクをサポートしています。マルチタスク分類は、モデルの定式化が異なるマルチ出力分類タスクに似ています。詳細については、関連する推定器のドキュメントを参照してください。</target>
        </trans-unit>
        <trans-unit id="6157bc0b8c2f67c8a593bf2d12852c000be43e72" translate="yes" xml:space="preserve">
          <source>This measure is not adjusted for chance. Therefore &lt;a href=&quot;sklearn.metrics.adjusted_mutual_info_score#sklearn.metrics.adjusted_mutual_info_score&quot;&gt;&lt;code&gt;adjusted_mutual_info_score&lt;/code&gt;&lt;/a&gt; might be preferred.</source>
          <target state="translated">この指標は偶然に調整されていません。したがって、&lt;a href=&quot;sklearn.metrics.adjusted_mutual_info_score#sklearn.metrics.adjusted_mutual_info_score&quot;&gt; &lt;code&gt;adjusted_mutual_info_score&lt;/code&gt; &lt;/a&gt;が推奨される場合があります。</target>
        </trans-unit>
        <trans-unit id="cca48ea1404a91d2df7b18cac656df30067cb12b" translate="yes" xml:space="preserve">
          <source>This method allows monitoring (i.e. determine error on testing set) after each boosting iteration.</source>
          <target state="translated">この方法では、各ブースティング・イテレーションの後にモニタリング(テストセットのエラー判定)を行うことができます。</target>
        </trans-unit>
        <trans-unit id="778da8cdceb825f45e49260c13130972c415ba18" translate="yes" xml:space="preserve">
          <source>This method allows monitoring (i.e. determine error on testing set) after each stage.</source>
          <target state="translated">この方法では、各ステージの後にモニタリング(テストセットの誤差判定)を行うことができます。</target>
        </trans-unit>
        <trans-unit id="893e20b8eaafff147aad0ee519c22b2be0783798" translate="yes" xml:space="preserve">
          <source>This method allows to generalize prediction to &lt;em&gt;new observations&lt;/em&gt; (not in the training set). Only available for novelty detection (when novelty is set to True).</source>
          <target state="translated">この方法では、予測を（&lt;em&gt;観測&lt;/em&gt;セットではなく）&lt;em&gt;新しい観測&lt;/em&gt;に一般化できます。新規性検出にのみ使用できます（新規性がTrueに設定されている場合）。</target>
        </trans-unit>
        <trans-unit id="e13bbab31202d773dbd5b155d7e0b7799ca54f84" translate="yes" xml:space="preserve">
          <source>This method computes the least squares solution using a singular value decomposition of X. If X is a matrix of size (n, p) this method has a cost of \(O(n p^2)\), assuming that \(n \geq p\).</source>
          <target state="translated">この方法は、X の特異値分解を用いて最小二乗法を計算する方法である。</target>
        </trans-unit>
        <trans-unit id="dc0919b0c811a79ed295c00492df459fa5ab93e8" translate="yes" xml:space="preserve">
          <source>This method doesn&amp;rsquo;t do anything. It exists purely for compatibility with the scikit-learn transformer API.</source>
          <target state="translated">このメソッドは何もしません。scikit-learnトランスフォーマーAPIとの互換性のためだけに存在します。</target>
        </trans-unit>
        <trans-unit id="89035c0aee87e0125ffcf7bf5f0dbe56fcfb74a9" translate="yes" xml:space="preserve">
          <source>This method has some performance and numerical stability overhead, hence it is better to call partial_fit on chunks of data that are as large as possible (as long as fitting in the memory budget) to hide the overhead.</source>
          <target state="translated">この方法には性能と数値安定性のオーバーヘッドがあるので、オーバーヘッドを隠すために、できるだけ大きなデータの塊(メモリバジェット内に収まる限り)に対してpartial_fitを呼び出す方が良いでしょう。</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
