<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ja" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="65eaa1a409cbf0736a7b1da17a35a153fc9af91f" translate="yes" xml:space="preserve">
          <source>Test samples</source>
          <target state="translated">テストサンプル</target>
        </trans-unit>
        <trans-unit id="29446ed524d3e237184352cbcf6e1c5aaeb464e4" translate="yes" xml:space="preserve">
          <source>Test samples with shape = (n_samples, n_features) or None. For some estimators this may be a precomputed kernel matrix instead, shape = (n_samples, n_samples_fitted], where n_samples_fitted is the number of samples used in the fitting for the estimator. Passing None as test samples gives the same result as passing real test samples, since DummyRegressor operates independently of the sampled observations.</source>
          <target state="translated">shape=(n_samples,n_features)または None でのテストサンプル。推定量によっては、これを代わりに事前計算されたカーネル行列とすることもできます。ダミーレグレッサーはサンプリングされたオブザベーションから独立して動作するので、テストサンプルとして None を渡すと、実際のテストサンプルを渡すのと同じ結果が得られます。</target>
        </trans-unit>
        <trans-unit id="12cad09d9d4837878fb37fd506e5f7fb71a801c9" translate="yes" xml:space="preserve">
          <source>Test samples with shape = (n_samples, n_features) or None. Passing None as test samples gives the same result as passing real test samples, since DummyClassifier operates independently of the sampled observations.</source>
          <target state="translated">shape=(n_samples,n_features)または None を持つテスト・サンプル。None をテスト・サンプルとして渡すと,DummyClassifier がサンプリングされたオブザベーションから独立して動作するので,実際のテスト・サンプルを渡すのと同じ結果が得られます.</target>
        </trans-unit>
        <trans-unit id="0c1d5bbb82f5cfc66b7e35e84179b02f4b13f8b1" translate="yes" xml:space="preserve">
          <source>Test samples.</source>
          <target state="translated">サンプルをテストします。</target>
        </trans-unit>
        <trans-unit id="bdec5057d32ebe97bd4c525fff2435009f4be0a0" translate="yes" xml:space="preserve">
          <source>Test samples. For some estimators this may be a precomputed kernel matrix instead, shape = (n_samples, n_samples_fitted], where n_samples_fitted is the number of samples used in the fitting for the estimator.</source>
          <target state="translated">テストサンプル。推定量によっては、これが代わりに事前計算されたカーネル行列となる場合があります。</target>
        </trans-unit>
        <trans-unit id="7f9baccc70399290d29568f9812594e6335c4ae0" translate="yes" xml:space="preserve">
          <source>Test with permutations the significance of a classification score</source>
          <target state="translated">分類スコアの有意性を順列で検定する</target>
        </trans-unit>
        <trans-unit id="c67f73aee0c3dc1034cba236cfe8c8526d7a9123" translate="yes" xml:space="preserve">
          <source>Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.</source>
          <target state="translated">テキスト解析は機械学習アルゴリズムの主要な応用分野である。しかし、生データである記号の列は、アルゴリズムの多くが可変長の生のテキスト文書ではなく、固定サイズの数値特徴ベクトルを期待しているため、直接アルゴリズムに与えることはできません。</target>
        </trans-unit>
        <trans-unit id="990226708ed5e3f7319c13e5c3e22d22fd438346" translate="yes" xml:space="preserve">
          <source>Text is made of characters, but files are made of bytes. These bytes represent characters according to some &lt;em&gt;encoding&lt;/em&gt;. To work with text files in Python, their bytes must be &lt;em&gt;decoded&lt;/em&gt; to a character set called Unicode. Common encodings are ASCII, Latin-1 (Western Europe), KOI8-R (Russian) and the universal encodings UTF-8 and UTF-16. Many others exist.</source>
          <target state="translated">テキストは文字で構成されていますが、ファイルはバイトで構成されています。これらのバイトは、いくつかの&lt;em&gt;エンコーディング&lt;/em&gt;に従って文字を表します。Pythonでテキストファイルを操作するには、そのバイトをUnicodeと呼ばれる文字セットに&lt;em&gt;デコード&lt;/em&gt;する必要があります。一般的なエンコードは、ASCII、Latin-1（西ヨーロッパ）、KOI8-R（ロシア語）、およびユニバーサルエンコードUTF-8およびUTF-16です。他にもたくさんあります。</target>
        </trans-unit>
        <trans-unit id="2a2f9f7e298485c4a85bf6b791046a1b63087cf9" translate="yes" xml:space="preserve">
          <source>Text preprocessing, tokenizing and filtering of stopwords are all included in &lt;a href=&quot;../../modules/generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt;, which builds a dictionary of features and transforms documents to feature vectors:</source>
          <target state="translated">テキストの前処理、トークン化、ストップワードのフィルタリングはすべて&lt;a href=&quot;../../modules/generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt;に含まれています。これは、特徴の辞書を構築し、ドキュメントを特徴ベクトルに変換します。</target>
        </trans-unit>
        <trans-unit id="79142cb36f8945e4d341c82cf5dc060dc02c8f0f" translate="yes" xml:space="preserve">
          <source>Text summary of the precision, recall, F1 score for each class. Dictionary returned if output_dict is True. Dictionary has the following structure:</source>
          <target state="translated">各クラスの精度、リコール、F1スコアのテキストサマリー。output_dictがTrueの場合に返される辞書。辞書の構造は以下の通り。</target>
        </trans-unit>
        <trans-unit id="f042ff208f7aff2fdebc20ebdcfe3611681222ed" translate="yes" xml:space="preserve">
          <source>Tf is &amp;ldquo;n&amp;rdquo; (natural) by default, &amp;ldquo;l&amp;rdquo; (logarithmic) when &lt;code&gt;sublinear_tf=True&lt;/code&gt;. Idf is &amp;ldquo;t&amp;rdquo; when use_idf is given, &amp;ldquo;n&amp;rdquo; (none) otherwise. Normalization is &amp;ldquo;c&amp;rdquo; (cosine) when &lt;code&gt;norm='l2'&lt;/code&gt;, &amp;ldquo;n&amp;rdquo; (none) when &lt;code&gt;norm=None&lt;/code&gt;.</source>
          <target state="translated">Tfはデフォルトで「n」（自然）、 &lt;code&gt;sublinear_tf=True&lt;/code&gt; の場合は「l」（対数）です。Idfは、use_idfが指定されている場合は「t」、指定されていない場合は「n」（なし）です。正規化は、 &lt;code&gt;norm='l2'&lt;/code&gt; の場合は「c」（コサイン）、 &lt;code&gt;norm=None&lt;/code&gt; の場合は「n」（なし）です。</target>
        </trans-unit>
        <trans-unit id="97730bbab5383bbe19dd65de91be719c29295304" translate="yes" xml:space="preserve">
          <source>Tf means &lt;strong&gt;term-frequency&lt;/strong&gt; while tf&amp;ndash;idf means term-frequency times &lt;strong&gt;inverse document-frequency&lt;/strong&gt;: \(\text{tf-idf(t,d)}=\text{tf(t,d)} \times \text{idf(t)}\).</source>
          <target state="translated">Tfは&lt;strong&gt;用語の頻度&lt;/strong&gt;を意味し、tf&amp;ndash;idfは用語の頻度と&lt;strong&gt;ドキュメントの頻度の逆数を&lt;/strong&gt;意味し&lt;strong&gt;ます&lt;/strong&gt;：\（\ text {tf-idf（t、d）} = \ text {tf（t、d）} \ times \ text {idf （t）} \）。</target>
        </trans-unit>
        <trans-unit id="f1cc0d39fa695e88ce231644990ae2b503602ddb" translate="yes" xml:space="preserve">
          <source>Tf means term-frequency while tf-idf means term-frequency times inverse document-frequency. This is a common term weighting scheme in information retrieval, that has also found good use in document classification.</source>
          <target state="translated">Tfは用語頻度を意味し、tf-idfは用語頻度×逆文書頻度を意味する。これは、情報検索における一般的な用語の重み付け方式であり、文書の分類にもよく使われています。</target>
        </trans-unit>
        <trans-unit id="771178a448f62a1d367a9045d97d08b26eb6869c" translate="yes" xml:space="preserve">
          <source>Tf-idf-weighted document-term matrix.</source>
          <target state="translated">Tf-idf加重文書項行列。</target>
        </trans-unit>
        <trans-unit id="daaa1c74bc4f107e3ab2cba2520cc29b4809af00" translate="yes" xml:space="preserve">
          <source>TfidfVectorizer uses a in-memory vocabulary (a python dict) to map the most frequent words to features indices and hence compute a word occurrence frequency (sparse) matrix. The word frequencies are then reweighted using the Inverse Document Frequency (IDF) vector collected feature-wise over the corpus.</source>
          <target state="translated">TfidfVectorizerは、最も頻繁に使われる単語を特徴指標にマッピングするために、メモリ内の語彙(python dict)を使用して、単語出現頻度(スパース)行列を計算します。単語出現頻度は、コーパス上で特徴的に収集された逆文書頻度(IDF)ベクトルを用いて再重み付けされます。</target>
        </trans-unit>
        <trans-unit id="2c1e749a66bcce49e10daf1b4a981af3bebb9284" translate="yes" xml:space="preserve">
          <source>That this function takes time at least quadratic in n_samples. For large datasets, it&amp;rsquo;s wise to set that parameter to a small value.</source>
          <target state="translated">この関数がn_samplesで少なくとも2次の時間を要すること。大きなデータセットの場合は、そのパラメーターを小さな値に設定するのが賢明です。</target>
        </trans-unit>
        <trans-unit id="fb20b5f965f5eb1a2ab7f1c1219e1e8adbfdfc00" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;balanced&amp;rdquo; heuristic is inspired by Logistic Regression in Rare Events Data, King, Zen, 2001.</source>
          <target state="translated">「バランスの取れた」ヒューリスティックは、レアイベントデータ、キング、禅、2001年のロジスティック回帰に触発されています。</target>
        </trans-unit>
        <trans-unit id="f4d692dace6b9f963c8a80ff6fb54e77b0936bf5" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;balanced&amp;rdquo; mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt;</source>
          <target state="translated">「バランス」モードでは、yの値を使用して、 &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt; として、入力データのクラス頻度に反比例する重みを自動的に調整します。</target>
        </trans-unit>
        <trans-unit id="ef34b0ee7fbdfc2770447dcdf0759da38193f229" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;balanced&amp;rdquo; mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt;.</source>
          <target state="translated">「バランス」モードでは、yの値を使用して、入力データのクラス頻度に反比例する重みを &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt; として自動的に調整します。</target>
        </trans-unit>
        <trans-unit id="66610aa2288acbb0ecf69897c27cb9799d361b2f" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;balanced&amp;rdquo; mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data: &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt;.</source>
          <target state="translated">「バランス」モードでは、yの値を使用して、入力データのクラス頻度に反比例する重みを自動的に調整します： &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="ef1bbf84c17d648e39cb34085672c6faaeb47082" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;balanced_subsample&amp;rdquo; mode is the same as &amp;ldquo;balanced&amp;rdquo; except that weights are computed based on the bootstrap sample for every tree grown.</source>
          <target state="translated">「balanced_subsample」モードは、重みがすべての成長したツリーのブートストラップサンプルに基づいて計算されることを除いて、「balanced」と同じです。</target>
        </trans-unit>
        <trans-unit id="9570d1ba54b75e486c6a8fe70ab0af402d1567cc" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;lbfgs&amp;rdquo;, &amp;ldquo;sag&amp;rdquo; and &amp;ldquo;newton-cg&amp;rdquo; solvers only support L2 penalization and are found to converge faster for some high dimensional data. Setting &lt;code&gt;multi_class&lt;/code&gt; to &amp;ldquo;multinomial&amp;rdquo; with these solvers learns a true multinomial logistic regression model &lt;a href=&quot;#id26&quot; id=&quot;id23&quot;&gt;[5]&lt;/a&gt;, which means that its probability estimates should be better calibrated than the default &amp;ldquo;one-vs-rest&amp;rdquo; setting.</source>
          <target state="translated">「lbfgs」、「sag」、および「newton-cg」ソルバーはL2ペナルティのみをサポートし、一部の高次元データではより速く収束することがわかっています。これらのソルバーを使用して &lt;code&gt;multi_class&lt;/code&gt; を「multinomial」に設定すると、真の多項ロジスティック回帰モデル&lt;a href=&quot;#id26&quot; id=&quot;id23&quot;&gt;[5]が&lt;/a&gt;学習されます。つまり、その確率推定は、デフォルトの「one-vs-rest」設定よりも適切に調整されます。</target>
        </trans-unit>
        <trans-unit id="c76cf618fdea0e603c8990092e5d960628df7c0c" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;new&amp;rdquo; data consists of linear combinations of the input data, with weights probabilistically drawn given the KDE model.</source>
          <target state="translated">「新しい」データは、入力データの線形結合で構成され、KDEモデルが与えられると確率的に重みが描画されます。</target>
        </trans-unit>
        <trans-unit id="420229f24d72cfc948f72b9aaf53e46dfcb25b62" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;sag&amp;rdquo; solver uses a Stochastic Average Gradient descent &lt;a href=&quot;#id27&quot; id=&quot;id24&quot;&gt;[6]&lt;/a&gt;. It is faster than other solvers for large datasets, when both the number of samples and the number of features are large.</source>
          <target state="translated">「サグ」ソルバーは、確率的平均勾配降下法を使用します&lt;a href=&quot;#id27&quot; id=&quot;id24&quot;&gt;[6]&lt;/a&gt;。サンプルの数と特徴の数の両方が大きい場合、大規模なデータセットの他のソルバーよりも高速です。</target>
        </trans-unit>
        <trans-unit id="cf55bf4220bbf6e5ad8c38b76c827b53a1e3f193" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;saga&amp;rdquo; solver &lt;a href=&quot;#id28&quot; id=&quot;id25&quot;&gt;[7]&lt;/a&gt; is a variant of &amp;ldquo;sag&amp;rdquo; that also supports the non-smooth &lt;code&gt;penalty=&amp;rdquo;l1&amp;rdquo;&lt;/code&gt; option. This is therefore the solver of choice for sparse multinomial logistic regression.</source>
          <target state="translated">「saga」ソルバー&lt;a href=&quot;#id28&quot; id=&quot;id25&quot;&gt;[7]&lt;/a&gt;は「sag」の変形で、スムーズでない &lt;code&gt;penalty=&amp;rdquo;l1&amp;rdquo;&lt;/code&gt; オプションもサポートしています。したがって、これはスパース多項ロジスティック回帰に最適なソルバーです。</target>
        </trans-unit>
        <trans-unit id="77bf2f7306c562160b3a78c9199a57470ad6395e" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;saga&amp;rdquo; solver is often the best choice. The &amp;ldquo;liblinear&amp;rdquo; solver is used by default for historical reasons.</source>
          <target state="translated">「サガ」ソルバーは、多くの場合、最良の選択です。「liblinear」ソルバーは、歴史的な理由からデフォルトで使用されます。</target>
        </trans-unit>
        <trans-unit id="068bc43bd479e1422a1e2139866c2ca587dbb3ad" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;steepness&amp;rdquo; of ROC curves is also important, since it is ideal to maximize the true positive rate while minimizing the false positive rate.</source>
          <target state="translated">偽陽性率を最小限に抑えながら真陽性率を最大化することが理想的であるため、ROC曲線の「急峻さ」も重要です。</target>
        </trans-unit>
        <trans-unit id="0eb5d5532023d8acbeeebff557bc347056c3c6a7" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;target&amp;rdquo; for this database is an integer from 0 to 39 indicating the identity of the person pictured; however, with only 10 examples per class, this relatively small dataset is more interesting from an unsupervised or semi-supervised perspective.</source>
          <target state="translated">このデータベースの「ターゲット」は、0〜39の整数で、写真に写っている人物の身元を示します。ただし、クラスごとの例が10個しかないため、この比較的小さなデータセットは、教師なしまたは半教師ありの観点からより興味深いものになります。</target>
        </trans-unit>
        <trans-unit id="6893a2ecba3f5b3ceba43b94c7037a23940a0678" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;auto&amp;rsquo; mode is the default and is intended to pick the cheaper option of the two depending upon the shape and format of the training data.</source>
          <target state="translated">「自動」モードがデフォルトであり、トレーニングデータの形状と形式に応じて、2つのオプションのうち安価なオプションを選択することを目的としています。</target>
        </trans-unit>
        <trans-unit id="0f80449b3a36a9645d51b541d6ac4415080a7df2" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;cd&amp;rsquo; solver can only optimize the Frobenius norm. Due to the underlying non-convexity of NMF, the different solvers may converge to different minima, even when optimizing the same distance function.</source>
          <target state="translated">'cd'ソルバーは、フロベニウスノルムのみを最適化できます。同じ距離関数を最適化している場合でも、NMFの根底にある非凸性が原因で、異なるソルバーが異なる最小値に収束する可能性があります。</target>
        </trans-unit>
        <trans-unit id="370b11b6ae177f24cc2d42a049dda5c0d7e30775" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;eigen&amp;rsquo; solver is based on the optimization of the between class scatter to within class scatter ratio. It can be used for both classification and transform, and it supports shrinkage. However, the &amp;lsquo;eigen&amp;rsquo; solver needs to compute the covariance matrix, so it might not be suitable for situations with a high number of features.</source>
          <target state="translated">「固有値」ソルバーは、クラス散布間のクラス散布比への最適化に基づいています。分類と変換の両方に使用でき、収縮をサポートします。ただし、「固有値」ソルバーは共分散行列を計算する必要があるため、特徴の数が多い状況には適さない可能性があります。</target>
        </trans-unit>
        <trans-unit id="ccda076fda793672987d7568e3ca12c3047fb684" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;log&amp;rsquo; loss gives logistic regression, a probabilistic classifier. &amp;lsquo;modified_huber&amp;rsquo; is another smooth loss that brings tolerance to outliers as well as probability estimates. &amp;lsquo;squared_hinge&amp;rsquo; is like hinge but is quadratically penalized. &amp;lsquo;perceptron&amp;rsquo; is the linear loss used by the perceptron algorithm. The other losses are designed for regression but can be useful in classification as well; see SGDRegressor for a description.</source>
          <target state="translated">「ログ」損失は、確率的分類子であるロジスティック回帰を与えます。'modified_huber'は、外れ値と確率推定に許容誤差をもたらすもう1つの滑らかな損失です。'squared_hinge'はヒンジに似ていますが、二次ペナルティが課されます。'perceptron'は、パーセプトロンアルゴリズムで使用される線形損失です。他の損失は回帰用に設計されていますが、分類にも役立ちます。説明については、SGDRegressorを参照してください。</target>
        </trans-unit>
        <trans-unit id="5302138e8a256151a982f3c737747f8db1fec2f7" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;lsqr&amp;rsquo; solver is an efficient algorithm that only works for classification. It supports shrinkage.</source>
          <target state="translated">'lsqr'ソルバーは、分類に対してのみ機能する効率的なアルゴリズムです。縮みに対応しています。</target>
        </trans-unit>
        <trans-unit id="e2a91334301a1b93c477cd479a707ce044fefdf3" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;newton-cg&amp;rsquo;, &amp;lsquo;sag&amp;rsquo;, and &amp;lsquo;lbfgs&amp;rsquo; solvers support only L2 regularization with primal formulation. The &amp;lsquo;liblinear&amp;rsquo; solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.</source>
          <target state="translated">'newton-cg'、 'sag'、および 'lbfgs'ソルバーは、主公式を使用したL2正則化のみをサポートします。'liblinear'ソルバーはL1とL2の両方の正則化をサポートし、L2ペナルティのみのデュアル定式化を行います。</target>
        </trans-unit>
        <trans-unit id="ade2e6c6872bcfb8e63408411f739881d7395764" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;squared_loss&amp;rsquo; refers to the ordinary least squares fit. &amp;lsquo;huber&amp;rsquo; modifies &amp;lsquo;squared_loss&amp;rsquo; to focus less on getting outliers correct by switching from squared to linear loss past a distance of epsilon. &amp;lsquo;epsilon_insensitive&amp;rsquo; ignores errors less than epsilon and is linear past that; this is the loss function used in SVR. &amp;lsquo;squared_epsilon_insensitive&amp;rsquo; is the same but becomes squared loss past a tolerance of epsilon.</source>
          <target state="translated">「squared_loss」は、通常の最小二乗法を指します。'huber'は、 'squared_loss'を変更して、イプシロンの距離を過ぎると二乗損失から線形損失に切り替えることにより、外れ値の修正に重点を置きません。'epsilon_insensitive'は、イプシロンより小さいエラーを無視し、それを超えて線形になります。これはSVRで使用される損失関数です。'squared_epsilon_insensitive'は同じですが、イプシロンの許容誤差を超えると二乗損失になります。</target>
        </trans-unit>
        <trans-unit id="388443bd992c152f7c80a788085a15982e280e0a" translate="yes" xml:space="preserve">
          <source>The (scaled) interquartile range for each feature in the training set.</source>
          <target state="translated">訓練セットの各特徴の(スケーリングされた)クォリティ間の範囲。</target>
        </trans-unit>
        <trans-unit id="b3533a4edec1fdb05f12a2a421dc320606ca77c6" translate="yes" xml:space="preserve">
          <source>The (sometimes surprising) observation is that this is &lt;em&gt;still a linear model&lt;/em&gt;: to see this, imagine creating a new variable</source>
          <target state="translated">（時々驚くべき）観察はこれが&lt;em&gt;まだ線形モデルであること&lt;/em&gt;です：これを見るには、新しい変数を作成することを想像してください</target>
        </trans-unit>
        <trans-unit id="ae37fbc1863417aba870f086fd7dcb7d12932667" translate="yes" xml:space="preserve">
          <source>The (x,y) position of the lower-left corner, in degrees</source>
          <target state="translated">左下隅の (x,y)位置を度単位で指定します。</target>
        </trans-unit>
        <trans-unit id="bcd6ca42c3472afbe27069a62710b5c531496d9b" translate="yes" xml:space="preserve">
          <source>The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of our knowledge, it was originally collected by Ken Lang, probably for his paper &amp;ldquo;Newsweeder: Learning to filter netnews,&amp;rdquo; though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.</source>
          <target state="translated">20ニュースグループのデータセットは、約20,000のニュースグループドキュメントのコレクションで、20の異なるニュースグループに（ほぼ）均等に分割されます。私たちの知る限りでは、このコレクションは明示的には言及していませんが、おそらく彼の論文「Newsweeder：Learning to filter newnews」のためにケンラングによって最初に収集されました。20のニュースグループコレクションは、テキスト分類やテキストクラスタリングなどの機械学習技術のテキストアプリケーションでの実験用の人気のあるデータセットになっています。</target>
        </trans-unit>
        <trans-unit id="4b2a042059fffe007deb9ebabf02d8062c1e6bda" translate="yes" xml:space="preserve">
          <source>The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.</source>
          <target state="translated">20 のニュースグループデータセットは、20 のトピックに関する約 18,000 件のニュースグループの投稿から構成されており、2 つのサブセットに分割されています:1 つはトレーニング(または開発)用、もう 1 つはテスト(またはパフォーマンス評価)用です。トレーニングセットとテストセットの分割は、特定の日付の前後に投稿されたメッセージに基づいています。</target>
        </trans-unit>
        <trans-unit id="c380ecdb017c04631da3ca1753b6ddf07ce8267f" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.cluster&quot;&gt;&lt;code&gt;sklearn.cluster&lt;/code&gt;&lt;/a&gt; module gathers popular unsupervised clustering algorithms.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.cluster&quot;&gt; &lt;code&gt;sklearn.cluster&lt;/code&gt; の&lt;/a&gt;モジュールは、人気の教師なしクラスタリングアルゴリズムを収集します。</target>
        </trans-unit>
        <trans-unit id="6a798b177e574d6ff4ac12be7b93e3b8f8d74b71" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.covariance&quot;&gt;&lt;code&gt;sklearn.covariance&lt;/code&gt;&lt;/a&gt; module includes methods and algorithms to robustly estimate the covariance of features given a set of points. The precision matrix defined as the inverse of the covariance is also estimated. Covariance estimation is closely related to the theory of Gaussian Graphical Models.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.covariance&quot;&gt; &lt;code&gt;sklearn.covariance&lt;/code&gt; の&lt;/a&gt;モジュールは、ロバスト点の集合が与えられた特徴の共分散を推定する方法およびアルゴリズムを含んでいます。共分散の逆数として定義された精度行列も推定されます。共分散推定は、ガウスグラフィカルモデルの理論と密接に関連しています。</target>
        </trans-unit>
        <trans-unit id="d1230decfda989b60168bc88df7b70ef79122b2d" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.datasets&quot;&gt;&lt;code&gt;sklearn.datasets&lt;/code&gt;&lt;/a&gt; module includes utilities to load datasets, including methods to load and fetch popular reference datasets. It also features some artificial data generators.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.datasets&quot;&gt; &lt;code&gt;sklearn.datasets&lt;/code&gt; の&lt;/a&gt;モジュールをロードし、人気のある参照データセットをフェッチする方法を含む負荷データセットにユーティリティを含みます。また、いくつかの人工データジェネレーターも備えています。</target>
        </trans-unit>
        <trans-unit id="94bdb0abc615359801b0dde8f5ed432fa774aae6" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.decomposition&quot;&gt;&lt;code&gt;sklearn.decomposition&lt;/code&gt;&lt;/a&gt; module includes matrix decomposition algorithms, including among others PCA, NMF or ICA. Most of the algorithms of this module can be regarded as dimensionality reduction techniques.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.decomposition&quot;&gt; &lt;code&gt;sklearn.decomposition&lt;/code&gt; の&lt;/a&gt;モジュールは、行列分解アルゴリズムを含む他のPCA、NMFまたはICAの間を含みます。このモジュールのアルゴリズムのほとんどは、次元削減手法と見なすことができます。</target>
        </trans-unit>
        <trans-unit id="cce83af2900332bbe995457713d2e3977e6cea91" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; module includes ensemble-based methods for classification, regression and anomaly detection.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt;モジュールは、分類、回帰および異常検出のためのアンサンブルに基づく方法を含みます。</target>
        </trans-unit>
        <trans-unit id="a3b34965d608c8571221686c4eaa7dd2128cda34" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.exceptions&quot;&gt;&lt;code&gt;sklearn.exceptions&lt;/code&gt;&lt;/a&gt; module includes all custom warnings and error classes used across scikit-learn.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.exceptions&quot;&gt; &lt;code&gt;sklearn.exceptions&lt;/code&gt; の&lt;/a&gt;モジュールは、学ぶscikit全体で使用されるすべてのカスタム警告とエラークラスが含まれています。</target>
        </trans-unit>
        <trans-unit id="953e85b8304fe86126d3f8d4d49e2c347def818a" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.feature_extraction&quot;&gt;&lt;code&gt;sklearn.feature_extraction&lt;/code&gt;&lt;/a&gt; module deals with feature extraction from raw data. It currently includes methods to extract features from text and images.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.feature_extraction&quot;&gt; &lt;code&gt;sklearn.feature_extraction&lt;/code&gt; の&lt;/a&gt;モジュールは、生データからの特徴抽出を扱います。現在、テキストと画像から特徴を抽出するメソッドが含まれています。</target>
        </trans-unit>
        <trans-unit id="2ff97b1fa019f5f400e3468860cd96aea04f63dc" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.feature_extraction.image&quot;&gt;&lt;code&gt;sklearn.feature_extraction.image&lt;/code&gt;&lt;/a&gt; submodule gathers utilities to extract features from images.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.feature_extraction.image&quot;&gt; &lt;code&gt;sklearn.feature_extraction.image&lt;/code&gt; の&lt;/a&gt;サブモジュールのギャザーユーティリティは、画像から特徴を抽出します。</target>
        </trans-unit>
        <trans-unit id="5fb7f21374928a29d973d39c8eed205507941559" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.feature_extraction.text&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt;&lt;/a&gt; submodule gathers utilities to build feature vectors from text documents.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.feature_extraction.text&quot;&gt; &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; &lt;/a&gt;テキスト文書から特徴ベクトルを構築するためのサブモジュールが収集ユーティリティ。</target>
        </trans-unit>
        <trans-unit id="f8894b12541a4b0b591ccbf9c1c5e42bf4d0c13b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.feature_selection&quot;&gt;&lt;code&gt;sklearn.feature_selection&lt;/code&gt;&lt;/a&gt; module implements feature selection algorithms. It currently includes univariate filter selection methods and the recursive feature elimination algorithm.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.feature_selection&quot;&gt; &lt;code&gt;sklearn.feature_selection&lt;/code&gt; の&lt;/a&gt;モジュールの実装は、選択アルゴリズムを備えています。現在、一変量フィルターの選択方法と再帰的特徴除去アルゴリズムが含まれています。</target>
        </trans-unit>
        <trans-unit id="af392a06a08e896e0c1f9a845ceba81c0151ed14" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.gaussian_process&quot;&gt;&lt;code&gt;sklearn.gaussian_process&lt;/code&gt;&lt;/a&gt; module implements Gaussian Process based regression and classification.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.gaussian_process&quot;&gt; &lt;code&gt;sklearn.gaussian_process&lt;/code&gt; &lt;/a&gt;回帰および分類をベースモジュールが実装ガウスプロセス。</target>
        </trans-unit>
        <trans-unit id="e25959a779b184ae02a906c2808f68686c73aab5" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.kernel_approximation&quot;&gt;&lt;code&gt;sklearn.kernel_approximation&lt;/code&gt;&lt;/a&gt; module implements several approximate kernel feature maps base on Fourier transforms.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.kernel_approximation&quot;&gt; &lt;code&gt;sklearn.kernel_approximation&lt;/code&gt; の&lt;/a&gt;モジュールが実装いくつかのおおよそのカーネル機能では、フーリエ変換のベースをマッピングします。</target>
        </trans-unit>
        <trans-unit id="cf3dc31fd9ef458aef6de4af32bf51a7e61106a1" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.linear_model&quot;&gt;&lt;code&gt;sklearn.linear_model&lt;/code&gt;&lt;/a&gt; module implements generalized linear models. It includes Ridge regression, Bayesian Regression, Lasso and Elastic Net estimators computed with Least Angle Regression and coordinate descent. It also implements Stochastic Gradient Descent related algorithms.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.linear_model&quot;&gt; &lt;code&gt;sklearn.linear_model&lt;/code&gt; の&lt;/a&gt;モデル線形一般化モジュールが実装。これには、リッジ回帰、ベイジアン回帰、なげなわ、最小角回帰と座標降下で計算されたElastic Net推定量が含まれます。また、確率的勾配降下法に関連するアルゴリズムも実装しています。</target>
        </trans-unit>
        <trans-unit id="75e43c84de91a4a1d643ca88db0beef9e86494b8" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.manifold&quot;&gt;&lt;code&gt;sklearn.manifold&lt;/code&gt;&lt;/a&gt; module implements data embedding techniques.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.manifold&quot;&gt; &lt;code&gt;sklearn.manifold&lt;/code&gt; &lt;/a&gt;モジュールの実装データが技術を埋め込みます。</target>
        </trans-unit>
        <trans-unit id="f55aa3d6c230d41fe62ec5929fa59e00645b5d62" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.metrics&quot;&gt;&lt;code&gt;sklearn.metrics&lt;/code&gt;&lt;/a&gt; module includes score functions, performance metrics and pairwise metrics and distance computations.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.metrics&quot;&gt; &lt;code&gt;sklearn.metrics&lt;/code&gt; の&lt;/a&gt;モジュールは、スコア機能、パフォーマンス・メトリックとペアワイズ・メトリックおよび距離の計算を含みます。</target>
        </trans-unit>
        <trans-unit id="90553131dabe004a613ea2c87be35b6b6db9a1ae" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.metrics.cluster&quot;&gt;&lt;code&gt;sklearn.metrics.cluster&lt;/code&gt;&lt;/a&gt; submodule contains evaluation metrics for cluster analysis results. There are two forms of evaluation:</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.metrics.cluster&quot;&gt; &lt;code&gt;sklearn.metrics.cluster&lt;/code&gt; の&lt;/a&gt;サブモジュールは、クラスタ解析結果の評価指標が含まれています。評価には2つの形式があります。</target>
        </trans-unit>
        <trans-unit id="537333336506a029d4e76c0c5320f3e14636c908" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.mixture&quot;&gt;&lt;code&gt;sklearn.mixture&lt;/code&gt;&lt;/a&gt; module implements mixture modeling algorithms.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.mixture&quot;&gt; &lt;code&gt;sklearn.mixture&lt;/code&gt; &lt;/a&gt;アルゴリズムをモデル化モジュールが実装混合物。</target>
        </trans-unit>
        <trans-unit id="1b9bcfe9136ee1328197e574e66457c49aa39ded" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.naive_bayes&quot;&gt;&lt;code&gt;sklearn.naive_bayes&lt;/code&gt;&lt;/a&gt; module implements Naive Bayes algorithms. These are supervised learning methods based on applying Bayes&amp;rsquo; theorem with strong (naive) feature independence assumptions.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.naive_bayes&quot;&gt; &lt;code&gt;sklearn.naive_bayes&lt;/code&gt; は&lt;/a&gt;実装ナイーブベイズアルゴリズムをモジュール。これらは、ベイズの定理を強力な（素朴な）特徴の独立性の仮定に適用することに基づいた教師あり学習方法です。</target>
        </trans-unit>
        <trans-unit id="31da4b6c2407f749b7e6e441bc1101265b243a97" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.neighbors&quot;&gt;&lt;code&gt;sklearn.neighbors&lt;/code&gt;&lt;/a&gt; module implements the k-nearest neighbors algorithm.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.neighbors&quot;&gt; &lt;code&gt;sklearn.neighbors&lt;/code&gt; は&lt;/a&gt;実装にk近傍法をモジュール。</target>
        </trans-unit>
        <trans-unit id="637db5b82af4c4775ad8c11b2cc086c407ac4adc" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.neural_network&quot;&gt;&lt;code&gt;sklearn.neural_network&lt;/code&gt;&lt;/a&gt; module includes models based on neural networks.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.neural_network&quot;&gt; &lt;code&gt;sklearn.neural_network&lt;/code&gt; の&lt;/a&gt;モジュールは、ニューラルネットワークに基づくモデルが含まれています。</target>
        </trans-unit>
        <trans-unit id="97c84f48ddcbce9eff5bb423de61ca9bed7742a5" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline&lt;/code&gt;&lt;/a&gt; module implements utilities to build a composite estimator, as a chain of transforms and estimators.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline&lt;/code&gt; &lt;/a&gt;モジュール実装ユーティリティは、変換及び推定のチェーンとして、複合推定を構築します。</target>
        </trans-unit>
        <trans-unit id="3e4a3abf94a63259dfe9d5546d6b613a02821c2d" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.preprocessing&quot;&gt;&lt;code&gt;sklearn.preprocessing&lt;/code&gt;&lt;/a&gt; module includes scaling, centering, normalization, binarization and imputation methods.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.preprocessing&quot;&gt; &lt;code&gt;sklearn.preprocessing&lt;/code&gt; &lt;/a&gt;モジュールは、スケーリング、センタリング、正規化、二値化と代入方法を含みます。</target>
        </trans-unit>
        <trans-unit id="5fd91efb13a21a364a66a195be3f60dbc3429cc3" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.semi_supervised&quot;&gt;&lt;code&gt;sklearn.semi_supervised&lt;/code&gt;&lt;/a&gt; module implements semi-supervised learning algorithms. These algorithms utilized small amounts of labeled data and large amounts of unlabeled data for classification tasks. This module includes Label Propagation.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.semi_supervised&quot;&gt; &lt;code&gt;sklearn.semi_supervised&lt;/code&gt; &lt;/a&gt;モジュールの実装は、半教師学習アルゴリズム。これらのアルゴリズムは、分類タスクに少量のラベル付きデータと大量のラベルなしデータを利用しました。このモジュールには、ラベル伝播が含まれています。</target>
        </trans-unit>
        <trans-unit id="6b1e5de562db4c7ae4499c2a4fcb3f75a3027318" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.svm&quot;&gt;&lt;code&gt;sklearn.svm&lt;/code&gt;&lt;/a&gt; module includes Support Vector Machine algorithms.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.svm&quot;&gt; &lt;code&gt;sklearn.svm&lt;/code&gt; の&lt;/a&gt;モジュールは、サポートベクトルマシンアルゴリズムを含んでいます。</target>
        </trans-unit>
        <trans-unit id="ef3c16856f883650f7c10c3b8b62a045804fc7da" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.tree&quot;&gt;&lt;code&gt;sklearn.tree&lt;/code&gt;&lt;/a&gt; module includes decision tree-based models for classification and regression.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.tree&quot;&gt; &lt;code&gt;sklearn.tree&lt;/code&gt; &lt;/a&gt;モジュールは、分類および回帰の決定ツリーベースのモデルを含んでいます。</target>
        </trans-unit>
        <trans-unit id="fd392963cb16a5b60813f22af8246fa4065eb565" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.utils&quot;&gt;&lt;code&gt;sklearn.utils&lt;/code&gt;&lt;/a&gt; module includes various utilities.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.utils&quot;&gt; &lt;code&gt;sklearn.utils&lt;/code&gt; の&lt;/a&gt;モジュールは、様々なユーティリティを含みます。</target>
        </trans-unit>
        <trans-unit id="9c614be243d558a71ca4ede548ecf4767e4adc7c" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;../../auto_examples/classification/plot_digits_classification#sphx-glr-auto-examples-classification-plot-digits-classification-py&quot;&gt;simple example on this dataset&lt;/a&gt; illustrates how starting from the original problem one can shape the data for consumption in scikit-learn.</source>
          <target state="translated">&lt;a href=&quot;../../auto_examples/classification/plot_digits_classification#sphx-glr-auto-examples-classification-plot-digits-classification-py&quot;&gt;このデータセット&lt;/a&gt;の簡単な例は、元の問題から始めて、scikit-learnで使用するデータをどのように形成できるかを示しています。</target>
        </trans-unit>
        <trans-unit id="424aa7402b9869b036306a671e3630b4177e36b0" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;../../modules/tree#tree&quot;&gt;decision trees&lt;/a&gt; is used to fit a sine curve with addition noisy observation. As a result, it learns local linear regressions approximating the sine curve.</source>
          <target state="translated">&lt;a href=&quot;../../modules/tree#tree&quot;&gt;決定木は、&lt;/a&gt;さらにノイズの多い観測の正弦曲線を適合させるために使用されます。その結果、正弦曲線を近似する局所線形回帰が学習されます。</target>
        </trans-unit>
        <trans-unit id="eb2cbae46431d84a4889d55d659950b594e78664" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;../../modules/tree#tree&quot;&gt;decision trees&lt;/a&gt; is used to predict simultaneously the noisy x and y observations of a circle given a single underlying feature. As a result, it learns local linear regressions approximating the circle.</source>
          <target state="translated">&lt;a href=&quot;../../modules/tree#tree&quot;&gt;決定木は、&lt;/a&gt;同時に騒々しいX及び単一下層の機能を与えられた円のY観察を予測するために使用されます。その結果、円を近似する局所線形回帰を学習します。</target>
        </trans-unit>
        <trans-unit id="4ea0ad8f51ec5bec92f088b272fa90a6ac2d5b55" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_20newsgroups#sklearn.datasets.fetch_20newsgroups&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_20newsgroups&lt;/code&gt;&lt;/a&gt; function is a data fetching / caching functions that downloads the data archive from the original &lt;a href=&quot;http://people.csail.mit.edu/jrennie/20Newsgroups/&quot;&gt;20 newsgroups website&lt;/a&gt;, extracts the archive contents in the &lt;code&gt;~/scikit_learn_data/20news_home&lt;/code&gt; folder and calls the &lt;a href=&quot;../modules/generated/sklearn.datasets.load_files#sklearn.datasets.load_files&quot;&gt;&lt;code&gt;sklearn.datasets.load_files&lt;/code&gt;&lt;/a&gt; on either the training or testing set folder, or both of them:</source>
          <target state="translated">&lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_20newsgroups#sklearn.datasets.fetch_20newsgroups&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_20newsgroups&lt;/code&gt; の&lt;/a&gt;機能は、元からのデータのアーカイブをダウンロードする機能をキャッシュするデータフェッチ/ある&lt;a href=&quot;http://people.csail.mit.edu/jrennie/20Newsgroups/&quot;&gt;20のニュースグループのウェブサイトを&lt;/a&gt;、でアーカイブの内容を抽出し、 &lt;code&gt;~/scikit_learn_data/20news_home&lt;/code&gt; フォルダと呼び出す&lt;a href=&quot;../modules/generated/sklearn.datasets.load_files#sklearn.datasets.load_files&quot;&gt; &lt;code&gt;sklearn.datasets.load_files&lt;/code&gt; の&lt;/a&gt;訓練のいずれかにまたはテストセットフォルダー、またはその両方：</target>
        </trans-unit>
        <trans-unit id="26c038b3ea935758dab579b3237ee5588d78f251" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_lfw_pairs#sklearn.datasets.fetch_lfw_pairs&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_lfw_pairs&lt;/code&gt;&lt;/a&gt; datasets is subdivided into 3 subsets: the development &lt;code&gt;train&lt;/code&gt; set, the development &lt;code&gt;test&lt;/code&gt; set and an evaluation &lt;code&gt;10_folds&lt;/code&gt; set meant to compute performance metrics using a 10-folds cross validation scheme.</source>
          <target state="translated">&lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_lfw_pairs#sklearn.datasets.fetch_lfw_pairs&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_lfw_pairs&lt;/code&gt; の&lt;/a&gt;データセットは、3つのサブセットに細分される：現像 &lt;code&gt;train&lt;/code&gt; セット、開発 &lt;code&gt;test&lt;/code&gt; セットと評価 &lt;code&gt;10_folds&lt;/code&gt; が 10フォールドクロスバリデーション方式を使用して計算パフォーマンス・メトリックを意味セット。</target>
        </trans-unit>
        <trans-unit id="d14958ad2582740fd909337c2882b7ba18717e2a" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; module includes two averaging algorithms based on randomized &lt;a href=&quot;tree#tree&quot;&gt;decision trees&lt;/a&gt;: the RandomForest algorithm and the Extra-Trees method. Both algorithms are perturb-and-combine techniques &lt;a href=&quot;#b1998&quot; id=&quot;id5&quot;&gt;[B1998]&lt;/a&gt; specifically designed for trees. This means a diverse set of classifiers is created by introducing randomness in the classifier construction. The prediction of the ensemble is given as the averaged prediction of the individual classifiers.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt;モジュールには、ランダム化された&lt;a href=&quot;tree#tree&quot;&gt;決定木に&lt;/a&gt;基づく2つの平均化アルゴリズム、RandomForestアルゴリズムとExtra-Treesメソッドが含まれています。両方のアルゴリズムは、特に木のために設計された摂動と結合の手法&lt;a href=&quot;#b1998&quot; id=&quot;id5&quot;&gt;[B1998]&lt;/a&gt;です。これは、分類子の構成にランダム性を導入することにより、多様な分類子のセットが作成されることを意味します。アンサンブルの予測は、個々の分類子の平均予測として与えられます。</target>
        </trans-unit>
        <trans-unit id="fbeef59e0313a7e281a500dd36152abed677fa2e" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.feature_extraction&quot;&gt;&lt;code&gt;sklearn.feature_extraction&lt;/code&gt;&lt;/a&gt; module can be used to extract features in a format supported by machine learning algorithms from datasets consisting of formats such as text and image.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.feature_extraction&quot;&gt; &lt;code&gt;sklearn.feature_extraction&lt;/code&gt; の&lt;/a&gt;モジュールは、テキストや画像などの形式からなるデータセットからの機械学習アルゴリズムによってサポートされている形式で抽出機能を使用することができます。</target>
        </trans-unit>
        <trans-unit id="565412031e53246181e593ab56b9ab7f3accb362" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt;&lt;code&gt;sklearn.metrics&lt;/code&gt;&lt;/a&gt; module implements several loss, score, and utility functions to measure classification performance. Some metrics might require probability estimates of the positive class, confidence values, or binary decisions values. Most implementations allow each sample to provide a weighted contribution to the overall score, through the &lt;code&gt;sample_weight&lt;/code&gt; parameter.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt; &lt;code&gt;sklearn.metrics&lt;/code&gt; は、&lt;/a&gt;分類性能を測定するための道具を何損失、スコア、およびユーティリティ機能をモジュール。一部のメトリックでは、陽性クラスの確率推定、信頼値、またはバイナリ決定値が必要になる場合があります。ほとんどの実装では、 &lt;code&gt;sample_weight&lt;/code&gt; パラメーターを介して、各サンプルが全体的なスコアに加重寄与を提供できます。</target>
        </trans-unit>
        <trans-unit id="6986be647f522d4ad92a86deccdacfb4588f163b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt;&lt;code&gt;sklearn.metrics&lt;/code&gt;&lt;/a&gt; module implements several loss, score, and utility functions to measure regression performance. Some of those have been enhanced to handle the multioutput case: &lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt;&lt;code&gt;mean_squared_error&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.metrics.mean_absolute_error#sklearn.metrics.mean_absolute_error&quot;&gt;&lt;code&gt;mean_absolute_error&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt;&lt;code&gt;explained_variance_score&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;r2_score&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt; &lt;code&gt;sklearn.metrics&lt;/code&gt; は&lt;/a&gt;、回帰のパフォーマンスを測定するための道具を何損失、スコア、およびユーティリティ機能をモジュール。それらのいくつかは、複数出力のケースを処理するように拡張されています：&lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt; &lt;code&gt;mean_squared_error&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/sklearn.metrics.mean_absolute_error#sklearn.metrics.mean_absolute_error&quot;&gt; &lt;code&gt;mean_absolute_error&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt; &lt;code&gt;explained_variance_score&lt;/code&gt; &lt;/a&gt;および&lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt; &lt;code&gt;r2_score&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="e3af9dc32993fb04e5c47da4dea690da48a6baa4" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt;&lt;code&gt;sklearn.metrics&lt;/code&gt;&lt;/a&gt; module implements several loss, score, and utility functions. For more information see the &lt;a href=&quot;clustering#clustering-evaluation&quot;&gt;Clustering performance evaluation&lt;/a&gt; section for instance clustering, and &lt;a href=&quot;biclustering#biclustering-evaluation&quot;&gt;Biclustering evaluation&lt;/a&gt; for biclustering.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt; &lt;code&gt;sklearn.metrics&lt;/code&gt; は&lt;/a&gt;道具を何損失、スコア、およびユーティリティ機能をモジュール。詳細については、インスタンスクラスタリングの&lt;a href=&quot;clustering#clustering-evaluation&quot;&gt;クラスタリングパフォーマンス評価&lt;/a&gt;セクション、および&lt;a href=&quot;biclustering#biclustering-evaluation&quot;&gt;バイ&lt;/a&gt;クラスタリングのバイクラスタリング評価を参照してください。</target>
        </trans-unit>
        <trans-unit id="095cb4e1ad7cf586616a563cdbf95404fbb2310e" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt;&lt;/a&gt; submodule implements utilities to evaluate pairwise distances or affinity of sets of samples.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt; &lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt; の&lt;/a&gt;サンプルのセットの対距離または親和性を評価するためのサブモジュールが実装ユーティリティ。</target>
        </trans-unit>
        <trans-unit id="60f0063776d96ccddba5880841f7defdb7f0d5d9" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.multiclass&quot;&gt;&lt;code&gt;sklearn.multiclass&lt;/code&gt;&lt;/a&gt; module implements &lt;em&gt;meta-estimators&lt;/em&gt; to solve &lt;code&gt;multiclass&lt;/code&gt; and &lt;code&gt;multilabel&lt;/code&gt; classification problems by decomposing such problems into binary classification problems. Multitarget regression is also supported.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.multiclass&quot;&gt; &lt;code&gt;sklearn.multiclass&lt;/code&gt; &lt;/a&gt;モジュール実装&lt;em&gt;メタ推定量&lt;/em&gt;解決する &lt;code&gt;multiclass&lt;/code&gt; と &lt;code&gt;multilabel&lt;/code&gt; バイナリ分類問題にこのような問題を分解することにより、分類の問題を。マルチターゲット回帰もサポートされています。</target>
        </trans-unit>
        <trans-unit id="8db5d205727541fd60809b9d143967244bf8e79b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.random_projection&quot;&gt;&lt;code&gt;sklearn.random_projection&lt;/code&gt;&lt;/a&gt; module implements a simple and computationally efficient way to reduce the dimensionality of the data by trading a controlled amount of accuracy (as additional variance) for faster processing times and smaller model sizes. This module implements two types of unstructured random matrix: &lt;a href=&quot;#gaussian-random-matrix&quot;&gt;Gaussian random matrix&lt;/a&gt; and &lt;a href=&quot;#sparse-random-matrix&quot;&gt;sparse random matrix&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.random_projection&quot;&gt; &lt;code&gt;sklearn.random_projection&lt;/code&gt; の&lt;/a&gt;モジュールが実装単純で計算上効率的な方法は、より高速な処理時間およびより小さなモデルサイズの（追加の分散など）の精度の制御された量を取引することにより、データの次元を低減します。このモジュールは、&lt;a href=&quot;#gaussian-random-matrix&quot;&gt;ガウスランダムマトリックス&lt;/a&gt;と&lt;a href=&quot;#sparse-random-matrix&quot;&gt;スパースランダムマトリックスの&lt;/a&gt; 2種類の非構造化ランダムマトリックスを実装します。</target>
        </trans-unit>
        <trans-unit id="aaa03339275413a44471cce8ed9110742f7e29fb" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.agglomerativeclustering#sklearn.cluster.AgglomerativeClustering&quot;&gt;&lt;code&gt;AgglomerativeClustering&lt;/code&gt;&lt;/a&gt; object performs a hierarchical clustering using a bottom up approach: each observation starts in its own cluster, and clusters are successively merged together. The linkage criteria determines the metric used for the merge strategy:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.cluster.agglomerativeclustering#sklearn.cluster.AgglomerativeClustering&quot;&gt; &lt;code&gt;AgglomerativeClustering&lt;/code&gt; の&lt;/a&gt;オブジェクトが実行ボトムアップアプローチを用いて階層的クラスタリング：独自のクラスタ内の各観測を開始し、クラスタを順次一緒にマージされます。リンク基準は、マージ戦略に使用されるメトリックを決定します。</target>
        </trans-unit>
        <trans-unit id="913b5a9805377fabb258d2653b5b70e8adeffb2c" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.bicluster.spectralbiclustering#sklearn.cluster.bicluster.SpectralBiclustering&quot;&gt;&lt;code&gt;SpectralBiclustering&lt;/code&gt;&lt;/a&gt; algorithm assumes that the input data matrix has a hidden checkerboard structure. The rows and columns of a matrix with this structure may be partitioned so that the entries of any bicluster in the Cartesian product of row clusters and column clusters are approximately constant. For instance, if there are two row partitions and three column partitions, each row will belong to three biclusters, and each column will belong to two biclusters.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.cluster.bicluster.spectralbiclustering#sklearn.cluster.bicluster.SpectralBiclustering&quot;&gt; &lt;code&gt;SpectralBiclustering&lt;/code&gt; &lt;/a&gt;アルゴリズムは、入力データ行列は、隠されたチェッカーボード構造を有していることを前提としています。この構造を持つ行列の行と列は、行クラスターと列クラスターのデカルト積のバイクラスターのエントリがほぼ一定になるように分割できます。たとえば、2つの行パーティションと3つの列パーティションがある場合、各行は3つのバイクラスターに属し、各列は2つのバイクラスターに属します。</target>
        </trans-unit>
        <trans-unit id="96812a842015efa920168397038c120e6e957561" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.bicluster.spectralcoclustering#sklearn.cluster.bicluster.SpectralCoclustering&quot;&gt;&lt;code&gt;SpectralCoclustering&lt;/code&gt;&lt;/a&gt; algorithm finds biclusters with values higher than those in the corresponding other rows and columns. Each row and each column belongs to exactly one bicluster, so rearranging the rows and columns to make partitions contiguous reveals these high values along the diagonal:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.cluster.bicluster.spectralcoclustering#sklearn.cluster.bicluster.SpectralCoclustering&quot;&gt; &lt;code&gt;SpectralCoclustering&lt;/code&gt; &lt;/a&gt;アルゴリズムは、対応する他の行に比べて高い値と列とbiclustersを見出します。各行と各列は1つのバイクラスターに属しているため、行と列を並べ替えてパーティションを隣接させると、対角線に沿ってこれらの高い値がわかります。</target>
        </trans-unit>
        <trans-unit id="9debcd56df8be7e32ea091b79dc8e313d63ea1d3" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.birch#sklearn.cluster.Birch&quot;&gt;&lt;code&gt;Birch&lt;/code&gt;&lt;/a&gt; builds a tree called the Characteristic Feature Tree (CFT) for the given data. The data is essentially lossy compressed to a set of Characteristic Feature nodes (CF Nodes). The CF Nodes have a number of subclusters called Characteristic Feature subclusters (CF Subclusters) and these CF Subclusters located in the non-terminal CF Nodes can have CF Nodes as children.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.cluster.birch#sklearn.cluster.Birch&quot;&gt; &lt;code&gt;Birch&lt;/code&gt; &lt;/a&gt;与えられたデータに特徴的な性質ツリー（CFT）と呼ばれるツリーを構築します。データは本質的に非可逆圧縮され、一連のCharacteristic Featureノード（CFノード）に圧縮されます。 CFノードにはCharacteristic Featureサブクラスター（CFサブクラスター）と呼ばれるいくつかのサブクラスターがあり、非ターミナルCFノードにあるこれらのCFサブクラスターはCFノードを子として持つことができます。</target>
        </trans-unit>
        <trans-unit id="e8b115edebda7f7bf86445503d0ce08900b4f8de" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.dbscan#sklearn.cluster.DBSCAN&quot;&gt;&lt;code&gt;DBSCAN&lt;/code&gt;&lt;/a&gt; algorithm views clusters as areas of high density separated by areas of low density. Due to this rather generic view, clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are convex shaped. The central component to the DBSCAN is the concept of &lt;em&gt;core samples&lt;/em&gt;, which are samples that are in areas of high density. A cluster is therefore a set of core samples, each close to each other (measured by some distance measure) and a set of non-core samples that are close to a core sample (but are not themselves core samples). There are two parameters to the algorithm, &lt;code&gt;min_samples&lt;/code&gt; and &lt;code&gt;eps&lt;/code&gt;, which define formally what we mean when we say &lt;em&gt;dense&lt;/em&gt;. Higher &lt;code&gt;min_samples&lt;/code&gt; or lower &lt;code&gt;eps&lt;/code&gt; indicate higher density necessary to form a cluster.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.cluster.dbscan#sklearn.cluster.DBSCAN&quot;&gt; &lt;code&gt;DBSCAN&lt;/code&gt; &lt;/a&gt;低密度の領域によって分離された高濃度の領域としてアルゴリズムビューのクラスター。このかなり一般的なビューのため、DBSCANによって検出されたクラスターは、クラスターが凸型であると仮定するk平均法とは異なり、任意の形状にすることができます。 DBSCANの中心的なコンポーネントは、高密度の領域にある&lt;em&gt;サンプル&lt;/em&gt;である&lt;em&gt;コアサンプル&lt;/em&gt;の概念です。したがって、クラスターは、それぞれが互いに近い（いくつかの距離測定によって測定された）コアサンプルのセットと、コアサンプルに近い（ただし、それ自体はコアサンプルではない）非コアサンプルのセットです。アルゴリズム、には2つのパラメータがあり &lt;code&gt;min_samples&lt;/code&gt; と &lt;code&gt;eps&lt;/code&gt; 我々が言うとき、私たちが何を意味するか正式に定義し、&lt;em&gt;密に&lt;/em&gt;。より高い &lt;code&gt;min_samples&lt;/code&gt; またはより低い &lt;code&gt;eps&lt;/code&gt; は、クラスターを形成するために必要なより高い密度を示します。</target>
        </trans-unit>
        <trans-unit id="844222980d29de5ed47698200091f13bdd09a284" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.featureagglomeration#sklearn.cluster.FeatureAgglomeration&quot;&gt;&lt;code&gt;FeatureAgglomeration&lt;/code&gt;&lt;/a&gt; uses agglomerative clustering to group together features that look very similar, thus decreasing the number of features. It is a dimensionality reduction tool, see &lt;a href=&quot;unsupervised_reduction#data-reduction&quot;&gt;Unsupervised dimensionality reduction&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.cluster.featureagglomeration#sklearn.cluster.FeatureAgglomeration&quot;&gt; &lt;code&gt;FeatureAgglomeration&lt;/code&gt; は&lt;/a&gt;一緒にグループに凝集クラスタリングを使用してこのような機能の数を減少させる、非常に類似している外観を備えています。これは、次元削減ツールです。&lt;a href=&quot;unsupervised_reduction#data-reduction&quot;&gt;教師なし次元削減を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="3ccd68ae912b5d8e7b609345a756782aaea1f1b3" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans&quot;&gt;&lt;code&gt;KMeans&lt;/code&gt;&lt;/a&gt; algorithm clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the &lt;a href=&quot;inertia&quot;&gt;inertia&lt;/a&gt; or within-cluster sum-of-squares. This algorithm requires the number of clusters to be specified. It scales well to large number of samples and has been used across a large range of application areas in many different fields.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans&quot;&gt; &lt;code&gt;KMeans&lt;/code&gt; &lt;/a&gt;として知られている基準最小化、等分散のN個のグループに別々の試料を試みることによってクラスタデータを、アルゴリズム&lt;a href=&quot;inertia&quot;&gt;慣性&lt;/a&gt;または内クラスタ和の二乗です。このアルゴリズムでは、クラスターの数を指定する必要があります。多数のサンプルに対応し、さまざまな分野の幅広いアプリケーション領域で使用されています。</target>
        </trans-unit>
        <trans-unit id="f8f9ba49e304c2e7e84cbf4122c9838a65e0d463" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans&quot;&gt;&lt;code&gt;MiniBatchKMeans&lt;/code&gt;&lt;/a&gt; is a variant of the &lt;a href=&quot;generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans&quot;&gt;&lt;code&gt;KMeans&lt;/code&gt;&lt;/a&gt; algorithm which uses mini-batches to reduce the computation time, while still attempting to optimise the same objective function. Mini-batches are subsets of the input data, randomly sampled in each training iteration. These mini-batches drastically reduce the amount of computation required to converge to a local solution. In contrast to other algorithms that reduce the convergence time of k-means, mini-batch k-means produces results that are generally only slightly worse than the standard algorithm.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans&quot;&gt; &lt;code&gt;MiniBatchKMeans&lt;/code&gt; で&lt;/a&gt;の変異体である&lt;a href=&quot;generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans&quot;&gt; &lt;code&gt;KMeans&lt;/code&gt; &lt;/a&gt;依然として同じ目的関数を最適化しようとしたとき、計算時間を短縮するために、ミニバッチを使用するアルゴリズム。ミニバッチは入力データのサブセットであり、各トレーニング反復でランダムにサンプリングされます。これらのミニバッチは、ローカルソリューションに収束するために必要な計算量を大幅に削減します。 k-meansの収束時間を短縮する他のアルゴリズムとは対照的に、ミニバッチk-meansは、一般的に標準アルゴリズムよりわずかに悪い結果しか生成しません。</target>
        </trans-unit>
        <trans-unit id="381def8c4d001638003d40e7acf9264b0a49ea0f" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;ColumnTransformer&lt;/code&gt;&lt;/a&gt; helps performing different transformations for different columns of the data, within a &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;Pipeline&lt;/code&gt;&lt;/a&gt; that is safe from data leakage and that can be parametrized. &lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;ColumnTransformer&lt;/code&gt;&lt;/a&gt; works on arrays, sparse matrices, and &lt;a href=&quot;http://pandas.pydata.org/pandas-docs/stable/&quot;&gt;pandas DataFrames&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt; &lt;code&gt;ColumnTransformer&lt;/code&gt; は&lt;/a&gt;内、データの異なる列に異なる変換を実行することができます&lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;Pipeline&lt;/code&gt; &lt;/a&gt;データ漏洩やそのパラメータ化することができますから、安全であること。&lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt; &lt;code&gt;ColumnTransformer&lt;/code&gt; は&lt;/a&gt;配列、スパース行列、そして上で動作&lt;a href=&quot;http://pandas.pydata.org/pandas-docs/stable/&quot;&gt;パンダのデータフレーム&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="37d727244bb97826f98eb0365b95bf6cd4afb239" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;compose.ColumnTransformer&lt;/code&gt;&lt;/a&gt; class is experimental and the API is subject to change.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt; &lt;code&gt;compose.ColumnTransformer&lt;/code&gt; の&lt;/a&gt;クラスでは、実験的であるとAPIは変更されることがあります。</target>
        </trans-unit>
        <trans-unit id="83e5137b54932bec66ccc36542bace6834598b69" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.covariance.graphicallasso#sklearn.covariance.GraphicalLasso&quot;&gt;&lt;code&gt;GraphicalLasso&lt;/code&gt;&lt;/a&gt; estimator uses an l1 penalty to enforce sparsity on the precision matrix: the higher its &lt;code&gt;alpha&lt;/code&gt; parameter, the more sparse the precision matrix. The corresponding &lt;a href=&quot;generated/sklearn.covariance.graphicallassocv#sklearn.covariance.GraphicalLassoCV&quot;&gt;&lt;code&gt;GraphicalLassoCV&lt;/code&gt;&lt;/a&gt; object uses cross-validation to automatically set the &lt;code&gt;alpha&lt;/code&gt; parameter.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.covariance.graphicallasso#sklearn.covariance.GraphicalLasso&quot;&gt; &lt;code&gt;GraphicalLasso&lt;/code&gt; &lt;/a&gt;高いその：推定精度行列にスパースを強制するL1ペナルティを使用して &lt;code&gt;alpha&lt;/code&gt; より疎なパラメーター、精密マトリックス。対応する&lt;a href=&quot;generated/sklearn.covariance.graphicallassocv#sklearn.covariance.GraphicalLassoCV&quot;&gt; &lt;code&gt;GraphicalLassoCV&lt;/code&gt; &lt;/a&gt;オブジェクトは、交差検証を使用して、 &lt;code&gt;alpha&lt;/code&gt; パラメーターを自動的に設定します。</target>
        </trans-unit>
        <trans-unit id="9447390bf3cfd368da76e6282f132428b32dfff8" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; object also provides a probabilistic interpretation of the PCA that can give a likelihood of data based on the amount of variance it explains. As such it implements a &lt;code&gt;score&lt;/code&gt; method that can be used in cross-validation:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; の&lt;/a&gt;オブジェクトは、それが説明分散の量に基づいてデータの可能性を与えることができPCAの確率的解釈を提供します。そのため、相互検証で使用できる &lt;code&gt;score&lt;/code&gt; メソッドを実装しています。</target>
        </trans-unit>
        <trans-unit id="de1125bcd2177e15b5b35e281621b5bbf18681e1" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; object is very useful, but has certain limitations for large datasets. The biggest limitation is that &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; only supports batch processing, which means all of the data to be processed must fit in main memory. The &lt;a href=&quot;generated/sklearn.decomposition.incrementalpca#sklearn.decomposition.IncrementalPCA&quot;&gt;&lt;code&gt;IncrementalPCA&lt;/code&gt;&lt;/a&gt; object uses a different form of processing and allows for partial computations which almost exactly match the results of &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; while processing the data in a minibatch fashion. &lt;a href=&quot;generated/sklearn.decomposition.incrementalpca#sklearn.decomposition.IncrementalPCA&quot;&gt;&lt;code&gt;IncrementalPCA&lt;/code&gt;&lt;/a&gt; makes it possible to implement out-of-core Principal Component Analysis either by:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; の&lt;/a&gt;オブジェクトは、非常に便利ですが、大規模なデータセットのために一定の制限があります。最大の制限は、&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt;がバッチ処理のみをサポートすることです。つまり、処理されるすべてのデータがメインメモリに収まる必要があります。&lt;a href=&quot;generated/sklearn.decomposition.incrementalpca#sklearn.decomposition.IncrementalPCA&quot;&gt; &lt;code&gt;IncrementalPCA&lt;/code&gt; の&lt;/a&gt;オブジェクトは、処理の異なる形態を使用し、ほぼ正確の結果と一致する部分の計算を可能に&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; を&lt;/a&gt; minibatch方式でデータを処理している間。&lt;a href=&quot;generated/sklearn.decomposition.incrementalpca#sklearn.decomposition.IncrementalPCA&quot;&gt; &lt;code&gt;IncrementalPCA&lt;/code&gt; を使用&lt;/a&gt;すると、次のいずれかの方法でコア外主成分分析を実装できます。</target>
        </trans-unit>
        <trans-unit id="ecd6b33d3bd2199aadcf263cff0e5246cde4bd39" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.decomposition.sparsecoder#sklearn.decomposition.SparseCoder&quot;&gt;&lt;code&gt;SparseCoder&lt;/code&gt;&lt;/a&gt; object is an estimator that can be used to transform signals into sparse linear combination of atoms from a fixed, precomputed dictionary such as a discrete wavelet basis. This object therefore does not implement a &lt;code&gt;fit&lt;/code&gt; method. The transformation amounts to a sparse coding problem: finding a representation of the data as a linear combination of as few dictionary atoms as possible. All variations of dictionary learning implement the following transform methods, controllable via the &lt;code&gt;transform_method&lt;/code&gt; initialization parameter:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.decomposition.sparsecoder#sklearn.decomposition.SparseCoder&quot;&gt; &lt;code&gt;SparseCoder&lt;/code&gt; の&lt;/a&gt;オブジェクトが固定からの原子のスパース線形結合に信号を変換するために使用することができる推定され、このような離散ウェーブレット基盤として辞書予め計算。したがって、このオブジェクトは &lt;code&gt;fit&lt;/code&gt; メソッドを実装していません。変換はまばらなコーディングの問題に相当します。データの表現を、できるだけ少ない辞書アトムの線形結合として見つけることです。辞書学習のすべてのバリエーションは、 &lt;code&gt;transform_method&lt;/code&gt; 初期化パラメーターを介して制御可能な次の変換メソッドを実装します。</target>
        </trans-unit>
        <trans-unit id="51b760d25143490b212d3dce7b63a475beffe57d" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.feature_extraction.image.extract_patches_2d#sklearn.feature_extraction.image.extract_patches_2d&quot;&gt;&lt;code&gt;extract_patches_2d&lt;/code&gt;&lt;/a&gt; function extracts patches from an image stored as a two-dimensional array, or three-dimensional with color information along the third axis. For rebuilding an image from all its patches, use &lt;a href=&quot;generated/sklearn.feature_extraction.image.reconstruct_from_patches_2d#sklearn.feature_extraction.image.reconstruct_from_patches_2d&quot;&gt;&lt;code&gt;reconstruct_from_patches_2d&lt;/code&gt;&lt;/a&gt;. For example let use generate a 4x4 pixel picture with 3 color channels (e.g. in RGB format):</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.feature_extraction.image.extract_patches_2d#sklearn.feature_extraction.image.extract_patches_2d&quot;&gt; &lt;code&gt;extract_patches_2d&lt;/code&gt; &lt;/a&gt;画像から機能抽出パッチは、二次元配列として格納され、又は第3の軸に沿って色情報を有する三次元。すべてのパッチからイメージを&lt;a href=&quot;generated/sklearn.feature_extraction.image.reconstruct_from_patches_2d#sklearn.feature_extraction.image.reconstruct_from_patches_2d&quot;&gt; &lt;code&gt;reconstruct_from_patches_2d&lt;/code&gt; &lt;/a&gt;するには、reconstruct_from_patches_2dを使用します。たとえば、3つのカラーチャネルを持つ4x4ピクセルの画像（RGB形式など）を生成してみましょう。</target>
        </trans-unit>
        <trans-unit id="13de12da96c62cdbe967814b1ded04e8c85eef13" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.feature_extraction.image.patchextractor#sklearn.feature_extraction.image.PatchExtractor&quot;&gt;&lt;code&gt;PatchExtractor&lt;/code&gt;&lt;/a&gt; class works in the same way as &lt;a href=&quot;generated/sklearn.feature_extraction.image.extract_patches_2d#sklearn.feature_extraction.image.extract_patches_2d&quot;&gt;&lt;code&gt;extract_patches_2d&lt;/code&gt;&lt;/a&gt;, only it supports multiple images as input. It is implemented as an estimator, so it can be used in pipelines. See:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.feature_extraction.image.patchextractor#sklearn.feature_extraction.image.PatchExtractor&quot;&gt; &lt;code&gt;PatchExtractor&lt;/code&gt; の&lt;/a&gt;クラスと同じように動作します&lt;a href=&quot;generated/sklearn.feature_extraction.image.extract_patches_2d#sklearn.feature_extraction.image.extract_patches_2d&quot;&gt; &lt;code&gt;extract_patches_2d&lt;/code&gt; &lt;/a&gt;、それだけでは、入力として複数の画像をサポートしています。推定器として実装されているため、パイプラインで使用できます。見る：</target>
        </trans-unit>
        <trans-unit id="5f1d9b11617dc21530d4a8e947334af50d14c144" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt;&lt;code&gt;HashingVectorizer&lt;/code&gt;&lt;/a&gt; also comes with the following limitations:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt; &lt;code&gt;HashingVectorizer&lt;/code&gt; は&lt;/a&gt;また、以下の制限が付属しています：</target>
        </trans-unit>
        <trans-unit id="dec1e79879a530cf5d8d2ea5bf189d8118bb1961" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.gaussianprocessclassifier#sklearn.gaussian_process.GaussianProcessClassifier&quot;&gt;&lt;code&gt;GaussianProcessClassifier&lt;/code&gt;&lt;/a&gt; implements Gaussian processes (GP) for classification purposes, more specifically for probabilistic classification, where test predictions take the form of class probabilities. GaussianProcessClassifier places a GP prior on a latent function \(f\), which is then squashed through a link function to obtain the probabilistic classification. The latent function \(f\) is a so-called nuisance function, whose values are not observed and are not relevant by themselves. Its purpose is to allow a convenient formulation of the model, and \(f\) is removed (integrated out) during prediction. GaussianProcessClassifier implements the logistic link function, for which the integral cannot be computed analytically but is easily approximated in the binary case.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.gaussianprocessclassifier#sklearn.gaussian_process.GaussianProcessClassifier&quot;&gt; &lt;code&gt;GaussianProcessClassifier&lt;/code&gt; は&lt;/a&gt;、分類の目的、より具体的には確率的分類のためにガウスプロセス（GP）を実装します。この場合、テスト予測はクラス確率の形式をとります。 GaussianProcessClassifierはGPを潜在関数\（f \）の前に配置します。これは、リンク関数を介して押しつぶされ、確率的分類を取得します。潜在関数\（f \）は、いわゆる迷惑関数であり、その値は観測されず、それ自体では関係ありません。その目的は、モデルの便利な定式化を可能にすることであり、\（f \）は予測中に削除（統合）されます。 GaussianProcessClassifierはロジスティックリンク関数を実装します。この関数では、積分を解析的に計算することはできませんが、バイナリの場合は簡単に近似できます。</target>
        </trans-unit>
        <trans-unit id="2a70b80f4163a2c6bfd08f3c8b84b458a9627cc7" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.gaussianprocessregressor#sklearn.gaussian_process.GaussianProcessRegressor&quot;&gt;&lt;code&gt;GaussianProcessRegressor&lt;/code&gt;&lt;/a&gt; implements Gaussian processes (GP) for regression purposes. For this, the prior of the GP needs to be specified. The prior mean is assumed to be constant and zero (for &lt;code&gt;normalize_y=False&lt;/code&gt;) or the training data&amp;rsquo;s mean (for &lt;code&gt;normalize_y=True&lt;/code&gt;). The prior&amp;rsquo;s covariance is specified by a passing a &lt;a href=&quot;#gp-kernels&quot;&gt;kernel&lt;/a&gt; object. The hyperparameters of the kernel are optimized during fitting of GaussianProcessRegressor by maximizing the log-marginal-likelihood (LML) based on the passed &lt;code&gt;optimizer&lt;/code&gt;. As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying &lt;code&gt;n_restarts_optimizer&lt;/code&gt;. The first run is always conducted starting from the initial hyperparameter values of the kernel; subsequent runs are conducted from hyperparameter values that have been chosen randomly from the range of allowed values. If the initial hyperparameters should be kept fixed, &lt;code&gt;None&lt;/code&gt; can be passed as optimizer.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.gaussianprocessregressor#sklearn.gaussian_process.GaussianProcessRegressor&quot;&gt; &lt;code&gt;GaussianProcessRegressor&lt;/code&gt; は&lt;/a&gt;、回帰の目的のためにガウス過程（GP）を実装しています。このためには、GPの事前を指定する必要があります。以前の平均は、定数およびゼロ（ &lt;code&gt;normalize_y=False&lt;/code&gt; の場合）またはトレーニングデータの平均（ &lt;code&gt;normalize_y=True&lt;/code&gt; の場合）と見なされます。事前分布の共分散は、&lt;a href=&quot;#gp-kernels&quot;&gt;カーネル&lt;/a&gt;オブジェクトを渡すことによって指定されます。カーネルのハイパーパラメーターは、GaussianProcessRegressorのフィッティング中に、渡さ &lt;code&gt;optimizer&lt;/code&gt; 基づいてlog-marginal-likelihood（LML）を最大化することにより最適化されます。 LMLには複数のローカルオプティマがある可能性があるため、 &lt;code&gt;n_restarts_optimizer&lt;/code&gt; を指定することでオプティマイザーを繰り返し起動できます。。最初の実行は常に、カーネルの初期ハイパーパラメーター値から開始されます。後続の実行は、許容値の範囲からランダムに選択されたハイパーパラメーター値から行われます。初期ハイパーパラメータを固定したままにする必要がある場合は、 &lt;code&gt;None&lt;/code&gt; をオプティマイザとして渡すことができます。</target>
        </trans-unit>
        <trans-unit id="a013933edad2184a36ef1d15fcbf21a794c0c7f5" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.constantkernel#sklearn.gaussian_process.kernels.ConstantKernel&quot;&gt;&lt;code&gt;ConstantKernel&lt;/code&gt;&lt;/a&gt; kernel can be used as part of a &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.product#sklearn.gaussian_process.kernels.Product&quot;&gt;&lt;code&gt;Product&lt;/code&gt;&lt;/a&gt; kernel where it scales the magnitude of the other factor (kernel) or as part of a &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.sum#sklearn.gaussian_process.kernels.Sum&quot;&gt;&lt;code&gt;Sum&lt;/code&gt;&lt;/a&gt; kernel, where it modifies the mean of the Gaussian process. It depends on a parameter \(constant\_value\). It is defined as:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.constantkernel#sklearn.gaussian_process.kernels.ConstantKernel&quot;&gt; &lt;code&gt;ConstantKernel&lt;/code&gt; の&lt;/a&gt;カーネルの一部として使用することができる&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.product#sklearn.gaussian_process.kernels.Product&quot;&gt; &lt;code&gt;Product&lt;/code&gt; &lt;/a&gt;は、他の因子（カーネル）の大きさやの一部としてスケーリングカーネル&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.sum#sklearn.gaussian_process.kernels.Sum&quot;&gt; &lt;code&gt;Sum&lt;/code&gt; &lt;/a&gt;がガウス過程の平均修正カーネルを、。パラメータ\（constant \ _value \）に依存します。次のように定義されます。</target>
        </trans-unit>
        <trans-unit id="a1a78e3b1d5985ce1973c8989ff0075d7d078b79" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt;&lt;code&gt;DotProduct&lt;/code&gt;&lt;/a&gt; kernel is commonly combined with exponentiation. An example with exponent 2 is shown in the following figure:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt; &lt;code&gt;DotProduct&lt;/code&gt; &lt;/a&gt;カーネルは、一般的に、べき乗と組み合わされます。指数2の例を次の図に示します。</target>
        </trans-unit>
        <trans-unit id="299194d50f816028e01666a91e5aaf031d88d5c0" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt;&lt;code&gt;DotProduct&lt;/code&gt;&lt;/a&gt; kernel is non-stationary and can be obtained from linear regression by putting \(N(0, 1)\) priors on the coefficients of \(x_d (d = 1, . . . , D)\) and a prior of \(N(0, \sigma_0^2)\) on the bias. The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt;&lt;code&gt;DotProduct&lt;/code&gt;&lt;/a&gt; kernel is invariant to a rotation of the coordinates about the origin, but not translations. It is parameterized by a parameter \(\sigma_0^2\). For \(\sigma_0^2 = 0\), the kernel is called the homogeneous linear kernel, otherwise it is inhomogeneous. The kernel is given by</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt; &lt;code&gt;DotProduct&lt;/code&gt; &lt;/a&gt;カーネルは非定常であり、\ \（x_d（D = 1、。。。、D）\）の係数に（N（0、1）\）事前確率を置くことによって、線形回帰から得られ、前ことができますバイアスの\（N（0、\ sigma_0 ^ 2）\）の。&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt; &lt;code&gt;DotProduct&lt;/code&gt; &lt;/a&gt;カーネルは、起源については、座標ではなく、翻訳の回転に対して不変です。パラメータ\（\ sigma_0 ^ 2 \）によってパラメータ化されます。\（\ sigma_0 ^ 2 = 0 \）の場合、カーネルは同種線形カーネルと呼ばれ、それ以外の場合は不均一です。カーネルは</target>
        </trans-unit>
        <trans-unit id="9b254fd92e2584b8ca8c7f30ca756b0bac24ec2b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.expsinesquared#sklearn.gaussian_process.kernels.ExpSineSquared&quot;&gt;&lt;code&gt;ExpSineSquared&lt;/code&gt;&lt;/a&gt; kernel allows modeling periodic functions. It is parameterized by a length-scale parameter \(l&amp;gt;0\) and a periodicity parameter \(p&amp;gt;0\). Only the isotropic variant where \(l\) is a scalar is supported at the moment. The kernel is given by:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.expsinesquared#sklearn.gaussian_process.kernels.ExpSineSquared&quot;&gt; &lt;code&gt;ExpSineSquared&lt;/code&gt; の&lt;/a&gt;カーネルは定期的な機能をモデル化できます。これは、長さスケールのパラメーター\（l&amp;gt; 0 \）および周期性パラメーター\（p&amp;gt; 0 \）によってパラメーター化されます。現時点では、\（l \）がスカラーである等方性バリアントのみがサポートされています。カーネルは以下によって与えられます：</target>
        </trans-unit>
        <trans-unit id="4fa9c3925ee31fe17ddb7d4f95d9aa563f446e7b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.matern#sklearn.gaussian_process.kernels.Matern&quot;&gt;&lt;code&gt;Matern&lt;/code&gt;&lt;/a&gt; kernel is a stationary kernel and a generalization of the &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt; kernel. It has an additional parameter \(\nu\) which controls the smoothness of the resulting function. It is parameterized by a length-scale parameter \(l&amp;gt;0\), which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs \(x\) (anisotropic variant of the kernel). The kernel is given by:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.matern#sklearn.gaussian_process.kernels.Matern&quot;&gt; &lt;code&gt;Matern&lt;/code&gt; &lt;/a&gt;カーネルは、固定カーネルとの一般化である&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt; &lt;code&gt;RBF&lt;/code&gt; の&lt;/a&gt;カーネル。結果の関数の滑らかさを制御する追加のパラメーター\（\ nu \）があります。これは、長さスケールパラメーター\（l&amp;gt; 0 \）によってパラメーター化されます。これは、スカラー（カーネルの等方性バリアント）または入力と同じ次元数のベクトル\（x \）（異方性バリアント）のいずれかです。カーネルの）。カーネルは以下によって与えられます：</target>
        </trans-unit>
        <trans-unit id="9cc391d0a3f24c35e3e834b704611ffc08dfc23c" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rationalquadratic#sklearn.gaussian_process.kernels.RationalQuadratic&quot;&gt;&lt;code&gt;RationalQuadratic&lt;/code&gt;&lt;/a&gt; kernel can be seen as a scale mixture (an infinite sum) of &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt; kernels with different characteristic length-scales. It is parameterized by a length-scale parameter \(l&amp;gt;0\) and a scale mixture parameter \(\alpha&amp;gt;0\) Only the isotropic variant where \(l\) is a scalar is supported at the moment. The kernel is given by:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rationalquadratic#sklearn.gaussian_process.kernels.RationalQuadratic&quot;&gt; &lt;code&gt;RationalQuadratic&lt;/code&gt; &lt;/a&gt;カーネルのスケール混合物（無限和）として見ることができる&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt; &lt;code&gt;RBF&lt;/code&gt; の&lt;/a&gt;異なる特性長さスケールを有するカーネル。長さスケールパラメーター\（l&amp;gt; 0 \）およびスケール混合パラメーター\（\ alpha&amp;gt; 0 \）によってパラメーター化されます。現時点では、\（l \）がスカラーである等方性バリアントのみがサポートされています。カーネルは以下によって与えられます：</target>
        </trans-unit>
        <trans-unit id="a0d4e8e5df8534d7f797dec945fa5951797b46d9" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt; kernel is a stationary kernel. It is also known as the &amp;ldquo;squared exponential&amp;rdquo; kernel. It is parameterized by a length-scale parameter \(l&amp;gt;0\), which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs \(x\) (anisotropic variant of the kernel). The kernel is given by:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt; &lt;code&gt;RBF&lt;/code&gt; の&lt;/a&gt;カーネルは、静止したカーネルです。「二乗指数」カーネルとしても知られています。これは、長さスケールパラメーター\（l&amp;gt; 0 \）によってパラメーター化されます。これは、スカラー（カーネルの等方性バリアント）または入力と同じ次元数のベクトル\（x \）（異方性バリアント）のいずれかです。カーネルの）。カーネルは以下によって与えられます：</target>
        </trans-unit>
        <trans-unit id="8f4b1aa7c1e397df865fdc8d1fd65546b5eaaf2f" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.impute.missingindicator#sklearn.impute.MissingIndicator&quot;&gt;&lt;code&gt;MissingIndicator&lt;/code&gt;&lt;/a&gt; transformer is useful to transform a dataset into corresponding binary matrix indicating the presence of missing values in the dataset. This transformation is useful in conjunction with imputation. When using imputation, preserving the information about which values had been missing can be informative.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.impute.missingindicator#sklearn.impute.MissingIndicator&quot;&gt; &lt;code&gt;MissingIndicator&lt;/code&gt; の&lt;/a&gt;変圧器は、データセット内の欠損値の存在を示すバイナリ行列を対応にデータセットを変換するのに有用です。この変換は補完と組み合わせて使用​​すると便利です。補完を使用する場合、どの値が欠落していたかについての情報を保持することは有益です。</target>
        </trans-unit>
        <trans-unit id="c0c9736c8ad276e3deabee46eb181026e0204e8e" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt;&lt;code&gt;SimpleImputer&lt;/code&gt;&lt;/a&gt; class also supports categorical data represented as string values or pandas categoricals when using the &lt;code&gt;'most_frequent'&lt;/code&gt; or &lt;code&gt;'constant'&lt;/code&gt; strategy:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt; &lt;code&gt;SimpleImputer&lt;/code&gt; の&lt;/a&gt;クラスには、使用している場合、文字列値またはパンダcategoricalsとして表さカテゴリーデータサポート &lt;code&gt;'most_frequent'&lt;/code&gt; または &lt;code&gt;'constant'&lt;/code&gt; の戦略を：</target>
        </trans-unit>
        <trans-unit id="4d17103c250c5ab6ac126fd9a857f81de53fed6f" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt;&lt;code&gt;SimpleImputer&lt;/code&gt;&lt;/a&gt; class also supports sparse matrices:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt; &lt;code&gt;SimpleImputer&lt;/code&gt; の&lt;/a&gt;クラスには、スパース行列をサポートしています。</target>
        </trans-unit>
        <trans-unit id="618df5d6360d655fcf582933900e1cb3bcf02379" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt;&lt;code&gt;SimpleImputer&lt;/code&gt;&lt;/a&gt; class provides basic strategies for imputing missing values. Missing values can be imputed with a provided constant value, or using the statistics (mean, median or most frequent) of each column in which the missing values are located. This class also allows for different missing values encodings.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt; &lt;code&gt;SimpleImputer&lt;/code&gt; の&lt;/a&gt;クラスでは、欠損値を帰するための基本的な戦略を提供します。欠落値は、提供された定数値を使用して、または欠落値が配置されている各列の統計（平均、中央値、または最も頻度の高い）を使用して補完できます。このクラスでは、さまざまな欠損値エンコーディングも可能です。</target>
        </trans-unit>
        <trans-unit id="6f59968771d1dbbd03a744853045d3c0b7aa414b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; constructs an approximate mapping for the radial basis function kernel, also known as &lt;em&gt;Random Kitchen Sinks&lt;/em&gt;&lt;a href=&quot;#rr2007&quot; id=&quot;id2&quot;&gt;[RR2007]&lt;/a&gt;. This transformation can be used to explicitly model a kernel map, prior to applying a linear algorithm, for example a linear SVM:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; は、&lt;/a&gt;としても知られているラジアル基底関数カーネルの近似マッピング、構築&lt;em&gt;ランダムキッチンシンク&lt;/em&gt;&lt;a href=&quot;#rr2007&quot; id=&quot;id2&quot;&gt;[RR2007を]&lt;/a&gt;。この変換は、線形SVMなどの線形アルゴリズムを適用する前に、カーネルマップを明示的にモデル化するために使用できます。</target>
        </trans-unit>
        <trans-unit id="44948166b7a6695399209dd8e1e1f1af0b0058e5" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.linear_model.huberregressor#sklearn.linear_model.HuberRegressor&quot;&gt;&lt;code&gt;HuberRegressor&lt;/code&gt;&lt;/a&gt; differs from using &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; with loss set to &lt;code&gt;huber&lt;/code&gt; in the following ways.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.huberregressor#sklearn.linear_model.HuberRegressor&quot;&gt; &lt;code&gt;HuberRegressor&lt;/code&gt; の&lt;/a&gt;使用と異なっ&lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; を&lt;/a&gt;する損失セットで &lt;code&gt;huber&lt;/code&gt; 、次の方法インチ</target>
        </trans-unit>
        <trans-unit id="7ab9e90f2b3f808e98761c90cab46994be6820bb" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.linear_model.huberregressor#sklearn.linear_model.HuberRegressor&quot;&gt;&lt;code&gt;HuberRegressor&lt;/code&gt;&lt;/a&gt; is different to &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt; because it applies a linear loss to samples that are classified as outliers. A sample is classified as an inlier if the absolute error of that sample is lesser than a certain threshold. It differs from &lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt;&lt;code&gt;TheilSenRegressor&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.linear_model.ransacregressor#sklearn.linear_model.RANSACRegressor&quot;&gt;&lt;code&gt;RANSACRegressor&lt;/code&gt;&lt;/a&gt; because it does not ignore the effect of the outliers but gives a lesser weight to them.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.huberregressor#sklearn.linear_model.HuberRegressor&quot;&gt; &lt;code&gt;HuberRegressor&lt;/code&gt; は&lt;/a&gt;と異なる&lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; &lt;/a&gt;それは外れ値として分類されているサンプルに直線的損失を適用するため。サンプルの絶対誤差が特定のしきい値よりも小さい場合、サンプルはインライアとして分類されます。外れ値の影響を無視せず、外れ値の重みを小さくするため、&lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt; &lt;code&gt;TheilSenRegressor&lt;/code&gt; &lt;/a&gt;および&lt;a href=&quot;generated/sklearn.linear_model.ransacregressor#sklearn.linear_model.RANSACRegressor&quot;&gt; &lt;code&gt;RANSACRegressor&lt;/code&gt; &lt;/a&gt;とは異なります。</target>
        </trans-unit>
        <trans-unit id="70ef4a25a40856b26fd987533d68c36540345c20" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt;&lt;code&gt;Lasso&lt;/code&gt;&lt;/a&gt; is a linear model that estimates sparse coefficients. It is useful in some contexts due to its tendency to prefer solutions with fewer parameter values, effectively reducing the number of variables upon which the given solution is dependent. For this reason, the Lasso and its variants are fundamental to the field of compressed sensing. Under certain conditions, it can recover the exact set of non-zero weights (see &lt;a href=&quot;../auto_examples/applications/plot_tomography_l1_reconstruction#sphx-glr-auto-examples-applications-plot-tomography-l1-reconstruction-py&quot;&gt;Compressive sensing: tomography reconstruction with L1 prior (Lasso)&lt;/a&gt;).</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt; &lt;code&gt;Lasso&lt;/code&gt; &lt;/a&gt;スパース係数を推定線形モデルです。これは、パラメーター値の少ないソリューションを優先する傾向があるため、状況によっては有用であり、所定のソリューションが依存する変数の数を効果的に削減します。このため、投げ縄とその変形は、圧縮センシングの分野の基本です。特定の条件下では、ゼロ以外の重みの正確なセットを復元できます（&lt;a href=&quot;../auto_examples/applications/plot_tomography_l1_reconstruction#sphx-glr-auto-examples-applications-plot-tomography-l1-reconstruction-py&quot;&gt;圧縮センシング：L1以前のトモグラフィー再構成（ラッソ）を&lt;/a&gt;参照）。</target>
        </trans-unit>
        <trans-unit id="8c1095adf7bd87312f73373efdee9c54e778b1be" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnet#sklearn.linear_model.MultiTaskElasticNet&quot;&gt;&lt;code&gt;MultiTaskElasticNet&lt;/code&gt;&lt;/a&gt; is an elastic-net model that estimates sparse coefficients for multiple regression problems jointly: &lt;code&gt;Y&lt;/code&gt; is a 2D array, of shape &lt;code&gt;(n_samples, n_tasks)&lt;/code&gt;. The constraint is that the selected features are the same for all the regression problems, also called tasks.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnet#sklearn.linear_model.MultiTaskElasticNet&quot;&gt; &lt;code&gt;MultiTaskElasticNet&lt;/code&gt; は&lt;/a&gt;：共同重回帰問題の疎係数を推定する弾性ネットモデルである &lt;code&gt;Y&lt;/code&gt; は、形状の2次元アレイである &lt;code&gt;(n_samples, n_tasks)&lt;/code&gt; 。制約は、選択された機能が、タスクとも呼ばれるすべての回帰問題で同じであることです。</target>
        </trans-unit>
        <trans-unit id="0855f24dbbabd45aa8775e800911f6fb3f3411c3" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.linear_model.multitasklasso#sklearn.linear_model.MultiTaskLasso&quot;&gt;&lt;code&gt;MultiTaskLasso&lt;/code&gt;&lt;/a&gt; is a linear model that estimates sparse coefficients for multiple regression problems jointly: &lt;code&gt;y&lt;/code&gt; is a 2D array, of shape &lt;code&gt;(n_samples, n_tasks)&lt;/code&gt;. The constraint is that the selected features are the same for all the regression problems, also called tasks.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.multitasklasso#sklearn.linear_model.MultiTaskLasso&quot;&gt; &lt;code&gt;MultiTaskLasso&lt;/code&gt; は&lt;/a&gt;共同重回帰問題の疎係数を推定する線形モデルである： &lt;code&gt;y&lt;/code&gt; は形状の2次元アレイである &lt;code&gt;(n_samples, n_tasks)&lt;/code&gt; 。制約は、選択された機能が、タスクとも呼ばれるすべての回帰問題で同じであることです。</target>
        </trans-unit>
        <trans-unit id="d927ce91bac1b6df649580c487bdf34ce5f21e13" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.linear_model.perceptron#sklearn.linear_model.Perceptron&quot;&gt;&lt;code&gt;Perceptron&lt;/code&gt;&lt;/a&gt; is another simple classification algorithm suitable for large scale learning. By default:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.perceptron#sklearn.linear_model.Perceptron&quot;&gt; &lt;code&gt;Perceptron&lt;/code&gt; &lt;/a&gt;大規模な学習に適した別の単純な分類アルゴリズムです。デフォルトでは：</target>
        </trans-unit>
        <trans-unit id="dc6941408ce5829c04ee30dacb5eec313120d42e" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt;&lt;code&gt;TheilSenRegressor&lt;/code&gt;&lt;/a&gt; estimator uses a generalization of the median in multiple dimensions. It is thus robust to multivariate outliers. Note however that the robustness of the estimator decreases quickly with the dimensionality of the problem. It looses its robustness properties and becomes no better than an ordinary least squares in high dimension.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt; &lt;code&gt;TheilSenRegressor&lt;/code&gt; の&lt;/a&gt;推定は、複数の次元での中央値の一般化を使用します。したがって、多変量外れ値に対してロバストです。ただし、推定量のロバスト性は、問題の次元数とともに急速に低下することに注意してください。堅牢性が失われ、高次元の通常の最小二乗と同等になります。</target>
        </trans-unit>
        <trans-unit id="497dd0db8e84137ac4b140cff3317a3423c09e22" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.accuracy_score#sklearn.metrics.accuracy_score&quot;&gt;&lt;code&gt;accuracy_score&lt;/code&gt;&lt;/a&gt; function computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Accuracy_and_precision&quot;&gt;accuracy&lt;/a&gt;, either the fraction (default) or the count (normalize=False) of correct predictions.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.accuracy_score#sklearn.metrics.accuracy_score&quot;&gt; &lt;code&gt;accuracy_score&lt;/code&gt; の&lt;/a&gt;関数は計算&lt;a href=&quot;https://en.wikipedia.org/wiki/Accuracy_and_precision&quot;&gt;精度を&lt;/a&gt;正しい予測のいずれかの画分（デフォルト）またはカウント（正規=偽）。</target>
        </trans-unit>
        <trans-unit id="734b9a83298cb0e5bb404a0eb951bb89a38c30c1" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt;&lt;code&gt;average_precision_score&lt;/code&gt;&lt;/a&gt; function computes the &lt;a href=&quot;http://en.wikipedia.org/w/index.php?title=Information_retrieval&amp;amp;oldid=793358396#Average_precision&quot;&gt;average precision&lt;/a&gt; (AP) from prediction scores. The value is between 0 and 1 and higher is better. AP is defined as</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt; &lt;code&gt;average_precision_score&lt;/code&gt; &lt;/a&gt;関数が計算&lt;a href=&quot;http://en.wikipedia.org/w/index.php?title=Information_retrieval&amp;amp;oldid=793358396#Average_precision&quot;&gt;平均精度&lt;/a&gt;予測スコアから（AP）を。値は0から1の間で、より高い方が良いです。APは次のよ​​うに定義されます</target>
        </trans-unit>
        <trans-unit id="4c1e547613cb8b25282a0a1d2e2d0fa8b86fab4a" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.balanced_accuracy_score#sklearn.metrics.balanced_accuracy_score&quot;&gt;&lt;code&gt;balanced_accuracy_score&lt;/code&gt;&lt;/a&gt; function computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Accuracy_and_precision&quot;&gt;balanced accuracy&lt;/a&gt;, which avoids inflated performance estimates on imbalanced datasets. It is the macro-average of recall scores per class or, equivalently, raw accuracy where each sample is weighted according to the inverse prevalence of its true class. Thus for balanced datasets, the score is equal to accuracy.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.balanced_accuracy_score#sklearn.metrics.balanced_accuracy_score&quot;&gt; &lt;code&gt;balanced_accuracy_score&lt;/code&gt; の&lt;/a&gt;関数は、計算&lt;a href=&quot;https://en.wikipedia.org/wiki/Accuracy_and_precision&quot;&gt;バランス精度&lt;/a&gt;不均衡データセットに膨張性能見積もりを回避します。これは、クラスごとの再現スコアのマクロ平均です。つまり、各サンプルがその真のクラスの逆有病率に従って重み付けされている生の精度です。したがって、バランスのとれたデータセットの場合、スコアは正確さに等しくなります。</target>
        </trans-unit>
        <trans-unit id="6c29a08df4ccee7316d3d3b84f8a1be99122d005" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.brier_score_loss#sklearn.metrics.brier_score_loss&quot;&gt;&lt;code&gt;brier_score_loss&lt;/code&gt;&lt;/a&gt; function computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Brier_score&quot;&gt;Brier score&lt;/a&gt; for binary classes. Quoting Wikipedia:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.brier_score_loss#sklearn.metrics.brier_score_loss&quot;&gt; &lt;code&gt;brier_score_loss&lt;/code&gt; &lt;/a&gt;関数は、計算&lt;a href=&quot;https://en.wikipedia.org/wiki/Brier_score&quot;&gt;ブライヤースコア&lt;/a&gt;バイナリクラスのために。ウィキペディアの引用：</target>
        </trans-unit>
        <trans-unit id="80ef899573ebb3be6112621f7df5aadf4a0d99d9" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.classification_report#sklearn.metrics.classification_report&quot;&gt;&lt;code&gt;classification_report&lt;/code&gt;&lt;/a&gt; function builds a text report showing the main classification metrics. Here is a small example with custom &lt;code&gt;target_names&lt;/code&gt; and inferred labels:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.classification_report#sklearn.metrics.classification_report&quot;&gt; &lt;code&gt;classification_report&lt;/code&gt; の&lt;/a&gt;機能は、メインの分類指標を示すテキスト形式のレポートを作成します。カスタム &lt;code&gt;target_names&lt;/code&gt; と推定ラベルを使用した小さな例を次に示します。</target>
        </trans-unit>
        <trans-unit id="646495d784f725b3203da7b1895753c47a46c957" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.confusion_matrix#sklearn.metrics.confusion_matrix&quot;&gt;&lt;code&gt;confusion_matrix&lt;/code&gt;&lt;/a&gt; function evaluates classification accuracy by computing the confusion matrix with each row corresponding to the true class &amp;lt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Confusion_matrix&quot;&gt;https://en.wikipedia.org/wiki/Confusion_matrix&lt;/a&gt;&amp;gt;`_. (Wikipedia and other references may use different convention for axes.)</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.confusion_matrix#sklearn.metrics.confusion_matrix&quot;&gt; &lt;code&gt;confusion_matrix&lt;/code&gt; の&lt;/a&gt;機能は、真のクラスに対応する各行との混同行列を計算することによって分類精度を評価する&amp;lt; &lt;a href=&quot;https://en.wikipedia.org/wiki/Confusion_matrix&quot;&gt;https://en.wikipedia.org/wiki/Confusion_matrix&lt;/a&gt; &amp;gt; `_。（ウィキペディアや他のリファレンスでは、軸に異なる規則を使用している場合があります。）</target>
        </trans-unit>
        <trans-unit id="627c762ce2611d603b6cf9dd93706bacfe9a64ab" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.coverage_error#sklearn.metrics.coverage_error&quot;&gt;&lt;code&gt;coverage_error&lt;/code&gt;&lt;/a&gt; function computes the average number of labels that have to be included in the final prediction such that all true labels are predicted. This is useful if you want to know how many top-scored-labels you have to predict in average without missing any true one. The best value of this metrics is thus the average number of true labels.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.coverage_error#sklearn.metrics.coverage_error&quot;&gt; &lt;code&gt;coverage_error&lt;/code&gt; の&lt;/a&gt;機能は、全ての真のラベルが予測されるように、最終的な予測に含まれなければならないラベルの平均数を計算します。これは、真のラベルを見落とすことなく、平均して予測する必要があるトップスコアラベルの数を知りたい場合に役立ちます。したがって、このメトリックの最良の値は、真のラベルの平均数です。</target>
        </trans-unit>
        <trans-unit id="365c5eb64f0dc2e33be205a551210f568e81546d" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt;&lt;code&gt;explained_variance_score&lt;/code&gt;&lt;/a&gt; computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Explained_variation&quot;&gt;explained variance regression score&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt; &lt;code&gt;explained_variance_score&lt;/code&gt; は&lt;/a&gt;計算&lt;a href=&quot;https://en.wikipedia.org/wiki/Explained_variation&quot;&gt;説明された分散回帰スコアを&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="16accfb21d784810c328541c85b1894b818cde8e" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.hamming_loss#sklearn.metrics.hamming_loss&quot;&gt;&lt;code&gt;hamming_loss&lt;/code&gt;&lt;/a&gt; computes the average Hamming loss or &lt;a href=&quot;https://en.wikipedia.org/wiki/Hamming_distance&quot;&gt;Hamming distance&lt;/a&gt; between two sets of samples.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.hamming_loss#sklearn.metrics.hamming_loss&quot;&gt; &lt;code&gt;hamming_loss&lt;/code&gt; は&lt;/a&gt;平均ハミング損失又は計算&lt;a href=&quot;https://en.wikipedia.org/wiki/Hamming_distance&quot;&gt;ハミング距離&lt;/a&gt;サンプルの2つのセット間を。</target>
        </trans-unit>
        <trans-unit id="6d1238c9791f472ba1850e50c0898d87bebfa2d3" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt;&lt;code&gt;hinge_loss&lt;/code&gt;&lt;/a&gt; function computes the average distance between the model and the data using &lt;a href=&quot;https://en.wikipedia.org/wiki/Hinge_loss&quot;&gt;hinge loss&lt;/a&gt;, a one-sided metric that considers only prediction errors. (Hinge loss is used in maximal margin classifiers such as support vector machines.)</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt; &lt;code&gt;hinge_loss&lt;/code&gt; &lt;/a&gt;関数は、モデルと使用してデータ間の平均距離計算&lt;a href=&quot;https://en.wikipedia.org/wiki/Hinge_loss&quot;&gt;ヒンジ損失を&lt;/a&gt;、唯一の予測誤差を考慮し、その1つのメトリックは、両面。（ヒンジ損失は、サポートベクターマシンなどの最大マージン分類器で使用されます。）</target>
        </trans-unit>
        <trans-unit id="29930da8eb2c0b1ff7129cc1cbfbb0416883031e" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.jaccard_similarity_score#sklearn.metrics.jaccard_similarity_score&quot;&gt;&lt;code&gt;jaccard_similarity_score&lt;/code&gt;&lt;/a&gt; function computes the average (default) or sum of &lt;a href=&quot;https://en.wikipedia.org/wiki/Jaccard_index&quot;&gt;Jaccard similarity coefficients&lt;/a&gt;, also called the Jaccard index, between pairs of label sets.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.jaccard_similarity_score#sklearn.metrics.jaccard_similarity_score&quot;&gt; &lt;code&gt;jaccard_similarity_score&lt;/code&gt; の&lt;/a&gt;関数の平均（デフォルト）または和演算&lt;a href=&quot;https://en.wikipedia.org/wiki/Jaccard_index&quot;&gt;ジャカード類似性係数&lt;/a&gt;ラベルセットの対の間にもジャカードインデックスと呼ばれるが、、。</target>
        </trans-unit>
        <trans-unit id="cbf6f35f94b93c090ec2e48a35d245f91dcfca65" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.label_ranking_average_precision_score#sklearn.metrics.label_ranking_average_precision_score&quot;&gt;&lt;code&gt;label_ranking_average_precision_score&lt;/code&gt;&lt;/a&gt; function implements label ranking average precision (LRAP). This metric is linked to the &lt;a href=&quot;generated/sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt;&lt;code&gt;average_precision_score&lt;/code&gt;&lt;/a&gt; function, but is based on the notion of label ranking instead of precision and recall.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.label_ranking_average_precision_score#sklearn.metrics.label_ranking_average_precision_score&quot;&gt; &lt;code&gt;label_ranking_average_precision_score&lt;/code&gt; の&lt;/a&gt;機能の実装は、ランキングの平均精度（LRAP）を標識します。このメトリックは&lt;a href=&quot;generated/sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt; &lt;code&gt;average_precision_score&lt;/code&gt; &lt;/a&gt;関数にリンクされていますが、精度と再現率ではなく、ラベルランキングの概念に基づいています。</target>
        </trans-unit>
        <trans-unit id="79620a85c10a9be19922ad33cc7395bed79c9e4e" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.label_ranking_loss#sklearn.metrics.label_ranking_loss&quot;&gt;&lt;code&gt;label_ranking_loss&lt;/code&gt;&lt;/a&gt; function computes the ranking loss which averages over the samples the number of label pairs that are incorrectly ordered, i.e. true labels have a lower score than false labels, weighted by the inverse of the number of ordered pairs of false and true labels. The lowest achievable ranking loss is zero.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.label_ranking_loss#sklearn.metrics.label_ranking_loss&quot;&gt; &lt;code&gt;label_ranking_loss&lt;/code&gt; &lt;/a&gt;機能は、真のラベルが偽と真のラベルの順序対の数の逆数で重み付けし、偽のラベルよりも低いスコアを持っていたサンプルを超える平均値が誤って注文しているラベルのペアの数、すなわち、ランキングの損失を計算します。達成可能な最低のランキング損失はゼロです。</target>
        </trans-unit>
        <trans-unit id="4b0810d3cef1ca02b990e21053d774c24a0e430b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.log_loss#sklearn.metrics.log_loss&quot;&gt;&lt;code&gt;log_loss&lt;/code&gt;&lt;/a&gt; function computes log loss given a list of ground-truth labels and a probability matrix, as returned by an estimator&amp;rsquo;s &lt;code&gt;predict_proba&lt;/code&gt; method.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.log_loss#sklearn.metrics.log_loss&quot;&gt; &lt;code&gt;log_loss&lt;/code&gt; の&lt;/a&gt;関数計算するには、推定ので返される損失は、グラウンドトゥルースラベルと確率行列のリストを与えられたログ &lt;code&gt;predict_proba&lt;/code&gt; の方法。</target>
        </trans-unit>
        <trans-unit id="3661b0b19cd7cbd747b2bf1ce7b4a383102a7abe" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.matthews_corrcoef#sklearn.metrics.matthews_corrcoef&quot;&gt;&lt;code&gt;matthews_corrcoef&lt;/code&gt;&lt;/a&gt; function computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Matthews_correlation_coefficient&quot;&gt;Matthew&amp;rsquo;s correlation coefficient (MCC)&lt;/a&gt; for binary classes. Quoting Wikipedia:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.matthews_corrcoef#sklearn.metrics.matthews_corrcoef&quot;&gt; &lt;code&gt;matthews_corrcoef&lt;/code&gt; の&lt;/a&gt;関数は、計算&lt;a href=&quot;https://en.wikipedia.org/wiki/Matthews_correlation_coefficient&quot;&gt;マシューの相関係数（MCC）を&lt;/a&gt;バイナリクラスの。ウィキペディアの引用：</target>
        </trans-unit>
        <trans-unit id="8af6e2db7d7843522a3d6755e0b8d1e9cbd1e8f1" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.mean_absolute_error#sklearn.metrics.mean_absolute_error&quot;&gt;&lt;code&gt;mean_absolute_error&lt;/code&gt;&lt;/a&gt; function computes &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_absolute_error&quot;&gt;mean absolute error&lt;/a&gt;, a risk metric corresponding to the expected value of the absolute error loss or \(l1\)-norm loss.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.mean_absolute_error#sklearn.metrics.mean_absolute_error&quot;&gt; &lt;code&gt;mean_absolute_error&lt;/code&gt; の&lt;/a&gt;関数は、計算&lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_absolute_error&quot;&gt;、平均絶対誤差&lt;/a&gt;ノルム喪失- 、絶対誤差の損失や\（L1 \）の期待値に対応するメトリックリスク。</target>
        </trans-unit>
        <trans-unit id="798074cb4600d0906a9ad4975942309f6067f034" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt;&lt;code&gt;mean_squared_error&lt;/code&gt;&lt;/a&gt; function computes &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_squared_error&quot;&gt;mean square error&lt;/a&gt;, a risk metric corresponding to the expected value of the squared (quadratic) error or loss.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt; &lt;code&gt;mean_squared_error&lt;/code&gt; の&lt;/a&gt;関数は、計算し&lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_squared_error&quot;&gt;た平均二乗誤差&lt;/a&gt;、二乗（二次）エラー又は損失の期待値に対応するメトリックリスク。</target>
        </trans-unit>
        <trans-unit id="e4bbccf6d12a691e935e91e2611be809f1bb4329" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.mean_squared_log_error#sklearn.metrics.mean_squared_log_error&quot;&gt;&lt;code&gt;mean_squared_log_error&lt;/code&gt;&lt;/a&gt; function computes a risk metric corresponding to the expected value of the squared logarithmic (quadratic) error or loss.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.mean_squared_log_error#sklearn.metrics.mean_squared_log_error&quot;&gt; &lt;code&gt;mean_squared_log_error&lt;/code&gt; の&lt;/a&gt;関数は、二乗対数（二次）エラー又は損失の期待値に対応するメトリックリスクを計算します。</target>
        </trans-unit>
        <trans-unit id="39f48c0bbd67ae6ad01f06368c176486b0da9e3b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.median_absolute_error#sklearn.metrics.median_absolute_error&quot;&gt;&lt;code&gt;median_absolute_error&lt;/code&gt;&lt;/a&gt; does not support multioutput.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.median_absolute_error#sklearn.metrics.median_absolute_error&quot;&gt; &lt;code&gt;median_absolute_error&lt;/code&gt; は、&lt;/a&gt;多出力をサポートしていません。</target>
        </trans-unit>
        <trans-unit id="4c03eab2aa446025510f65f3a7e796a70c97a820" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.median_absolute_error#sklearn.metrics.median_absolute_error&quot;&gt;&lt;code&gt;median_absolute_error&lt;/code&gt;&lt;/a&gt; is particularly interesting because it is robust to outliers. The loss is calculated by taking the median of all absolute differences between the target and the prediction.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.median_absolute_error#sklearn.metrics.median_absolute_error&quot;&gt; &lt;code&gt;median_absolute_error&lt;/code&gt; は&lt;/a&gt;、それが外れ値に対してロバストであるため、特に興味深いものです。損失は​​、ターゲットと予測の間のすべての絶対差の中央値を取ることによって計算されます。</target>
        </trans-unit>
        <trans-unit id="9f5631acff2238ea5bcb79376fef6d2e143756a6" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.precision_recall_curve#sklearn.metrics.precision_recall_curve&quot;&gt;&lt;code&gt;precision_recall_curve&lt;/code&gt;&lt;/a&gt; computes a precision-recall curve from the ground truth label and a score given by the classifier by varying a decision threshold.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.precision_recall_curve#sklearn.metrics.precision_recall_curve&quot;&gt; &lt;code&gt;precision_recall_curve&lt;/code&gt; は&lt;/a&gt;グランドトゥルースラベルから精密リコール曲線と判定閾値を変化させることにより、分類器によって与えられたスコアを計算します。</target>
        </trans-unit>
        <trans-unit id="d16f1c84f45a895677960253a6c6c45cf8b671e5" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;r2_score&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt;&lt;code&gt;explained_variance_score&lt;/code&gt;&lt;/a&gt; accept an additional value &lt;code&gt;'variance_weighted'&lt;/code&gt; for the &lt;code&gt;multioutput&lt;/code&gt; parameter. This option leads to a weighting of each individual score by the variance of the corresponding target variable. This setting quantifies the globally captured unscaled variance. If the target variables are of different scale, then this score puts more importance on well explaining the higher variance variables. &lt;code&gt;multioutput='variance_weighted'&lt;/code&gt; is the default value for &lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;r2_score&lt;/code&gt;&lt;/a&gt; for backward compatibility. This will be changed to &lt;code&gt;uniform_average&lt;/code&gt; in the future.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt; &lt;code&gt;r2_score&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt; &lt;code&gt;explained_variance_score&lt;/code&gt; は&lt;/a&gt;付加価値受け入れる &lt;code&gt;'variance_weighted'&lt;/code&gt; のための &lt;code&gt;multioutput&lt;/code&gt; パラメータを。このオプションは、対応するターゲット変数の分散による個々のスコアの重み付けにつながります。この設定は、グローバルにキャプチャされたスケーリングされていない分散を定量化します。ターゲット変数のスケールが異なる場合、このスコアは、より高い分散変数をうまく説明することをより重視します。 &lt;code&gt;multioutput='variance_weighted'&lt;/code&gt; は、下位互換性のための&lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt; &lt;code&gt;r2_score&lt;/code&gt; &lt;/a&gt;のデフォルト値です。これは将来、 &lt;code&gt;uniform_average&lt;/code&gt; に変更されます。</target>
        </trans-unit>
        <trans-unit id="093b69e0a2a3ccbb33c0abc5ad459d307bcf6553" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;r2_score&lt;/code&gt;&lt;/a&gt; function computes R&amp;sup2;, the &lt;a href=&quot;https://en.wikipedia.org/wiki/Coefficient_of_determination&quot;&gt;coefficient of determination&lt;/a&gt;. It provides a measure of how well future samples are likely to be predicted by the model. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt; &lt;code&gt;r2_score&lt;/code&gt; の&lt;/a&gt;機能は、R2、計算&lt;a href=&quot;https://en.wikipedia.org/wiki/Coefficient_of_determination&quot;&gt;決意の係数&lt;/a&gt;。これは、将来のサンプルがモデルによってどれほど予測される可能性が高いかを示す尺度を提供します。最良のスコアは1.0であり、負の値になる可能性があります（モデルが恣意的に悪化する可能性があるため）。入力フィーチャを無視して、yの期待値を常に予測する定数モデルの場合、R ^ 2スコアは0.0になります。</target>
        </trans-unit>
        <trans-unit id="6cdf15299db25ef1f3b3628cdbe3b0de1b0d0ab3" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt;&lt;code&gt;roc_auc_score&lt;/code&gt;&lt;/a&gt; function computes the area under the receiver operating characteristic (ROC) curve, which is also denoted by AUC or AUROC. By computing the area under the roc curve, the curve information is summarized in one number. For more information see the &lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve&quot;&gt;Wikipedia article on AUC&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt; &lt;code&gt;roc_auc_score&lt;/code&gt; の&lt;/a&gt;機能はまた、AUC又はAUROCで示される受信者動作特性（ROC）曲線下面積を計算します。roc曲線の下の面積を計算することにより、曲線情報が1つの数値に要約されます。詳細については、&lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve&quot;&gt;AUCに関するWikipediaの記事を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="fbc2259a8dc85fa7ed24780d0f0e2ac678fbcad9" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.zero_one_loss#sklearn.metrics.zero_one_loss&quot;&gt;&lt;code&gt;zero_one_loss&lt;/code&gt;&lt;/a&gt; function computes the sum or the average of the 0-1 classification loss (\(L_{0-1}\)) over \(n_{\text{samples}}\). By default, the function normalizes over the sample. To get the sum of the \(L_{0-1}\), set &lt;code&gt;normalize&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.zero_one_loss#sklearn.metrics.zero_one_loss&quot;&gt; &lt;code&gt;zero_one_loss&lt;/code&gt; &lt;/a&gt;関数は和または0-1分類損失（\（L_ {0-1} \））上\（N _ {\テキスト{サンプル}} \）の平均値を算出します。デフォルトでは、関数はサンプルを正規化します。\（L_ {0-1} \）の合計を取得するには、 &lt;code&gt;normalize&lt;/code&gt; を &lt;code&gt;False&lt;/code&gt; に設定します。</target>
        </trans-unit>
        <trans-unit id="0ce78ef531a28f4df4b9620cf7454901e00bf965" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.mixture.bayesiangaussianmixture#sklearn.mixture.BayesianGaussianMixture&quot;&gt;&lt;code&gt;BayesianGaussianMixture&lt;/code&gt;&lt;/a&gt; object implements a variant of the Gaussian mixture model with variational inference algorithms. The API is similar as the one defined by &lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture&quot;&gt;&lt;code&gt;GaussianMixture&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.mixture.bayesiangaussianmixture#sklearn.mixture.BayesianGaussianMixture&quot;&gt; &lt;code&gt;BayesianGaussianMixture&lt;/code&gt; の&lt;/a&gt;オブジェクトは、変分推論アルゴリズムを有するガウス混合モデルの変形を実現します。APIは&lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture&quot;&gt; &lt;code&gt;GaussianMixture&lt;/code&gt; で&lt;/a&gt;定義されているものと似ています。</target>
        </trans-unit>
        <trans-unit id="7755b185d3bd9567bc77a31c3d84e8be2c332f90" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture&quot;&gt;&lt;code&gt;GaussianMixture&lt;/code&gt;&lt;/a&gt; comes with different options to constrain the covariance of the difference classes estimated: spherical, diagonal, tied or full covariance.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture&quot;&gt; &lt;code&gt;GaussianMixture&lt;/code&gt; は&lt;/a&gt;、球状の対角線、縛られたり、完全な共分散：推定差異クラスの共分散を拘束するためにさまざまなオプションが付属しています。</target>
        </trans-unit>
        <trans-unit id="9cb1ef419e28ff804b54eef9fc18747641996ac9" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture&quot;&gt;&lt;code&gt;GaussianMixture&lt;/code&gt;&lt;/a&gt; object implements the &lt;a href=&quot;#expectation-maximization&quot;&gt;expectation-maximization&lt;/a&gt; (EM) algorithm for fitting mixture-of-Gaussian models. It can also draw confidence ellipsoids for multivariate models, and compute the Bayesian Information Criterion to assess the number of clusters in the data. A &lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture.fit&quot;&gt;&lt;code&gt;GaussianMixture.fit&lt;/code&gt;&lt;/a&gt; method is provided that learns a Gaussian Mixture Model from train data. Given test data, it can assign to each sample the Gaussian it mostly probably belong to using the &lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture.predict&quot;&gt;&lt;code&gt;GaussianMixture.predict&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture&quot;&gt; &lt;code&gt;GaussianMixture&lt;/code&gt; の&lt;/a&gt;オブジェクトの実装は、&lt;a href=&quot;#expectation-maximization&quot;&gt;期待値最大化&lt;/a&gt;混合-のガウスモデルをフィッティングするための（EM）アルゴリズム。また、多変量モデルの信頼楕円体を描画し、ベイズ情報量基準を計算して、データ内のクラスター数を評価することもできます。A &lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture.fit&quot;&gt; &lt;code&gt;GaussianMixture.fit&lt;/code&gt; の&lt;/a&gt;方法は、列車のデータから学習ガウス混合モデルが提供されます。テストデータが与えられると、&lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture.predict&quot;&gt; &lt;code&gt;GaussianMixture.predict&lt;/code&gt; &lt;/a&gt;メソッドを使用して、各サンプルに主に属するガウシアンを割り当てることができます。</target>
        </trans-unit>
        <trans-unit id="0de55d1a8cabbad74e59064d298db364a4949211" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt; instance implements the usual estimator API: when &amp;ldquo;fitting&amp;rdquo; it on a dataset all the possible combinations of parameter values are evaluated and the best combination is retained.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;GridSearchCV&lt;/code&gt; の&lt;/a&gt;インスタンスを実装通常の推定API：データセットの上に「フィッティング」すべてのパラメータ値の可能な組み合わせが評価され、最良の組み合わせが保持されます。</target>
        </trans-unit>
        <trans-unit id="45a8a9b77f54ed1a1a49e1eebdf058e73c1ad33a" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.model_selection.groupshufflesplit#sklearn.model_selection.GroupShuffleSplit&quot;&gt;&lt;code&gt;GroupShuffleSplit&lt;/code&gt;&lt;/a&gt; iterator behaves as a combination of &lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt;&lt;code&gt;ShuffleSplit&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.model_selection.leavepgroupsout#sklearn.model_selection.LeavePGroupsOut&quot;&gt;&lt;code&gt;LeavePGroupsOut&lt;/code&gt;&lt;/a&gt;, and generates a sequence of randomized partitions in which a subset of groups are held out for each split.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.model_selection.groupshufflesplit#sklearn.model_selection.GroupShuffleSplit&quot;&gt; &lt;code&gt;GroupShuffleSplit&lt;/code&gt; &lt;/a&gt;イテレータの組み合わせとして動作&lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt; &lt;code&gt;ShuffleSplit&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;generated/sklearn.model_selection.leavepgroupsout#sklearn.model_selection.LeavePGroupsOut&quot;&gt; &lt;code&gt;LeavePGroupsOut&lt;/code&gt; &lt;/a&gt;、およびグループのサブセットが各分割のために保持されたランダム化パーティションのシーケンスを生成します。</target>
        </trans-unit>
        <trans-unit id="ec15ef26f9a075815cd5139e68468c23802a4936" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt;&lt;code&gt;ShuffleSplit&lt;/code&gt;&lt;/a&gt; iterator will generate a user defined number of independent train / test dataset splits. Samples are first shuffled and then split into a pair of train and test sets.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt; &lt;code&gt;ShuffleSplit&lt;/code&gt; は、&lt;/a&gt;反復子は、独立した列車/テストデータセット分割のユーザ定義された数を生成します。サンプルは最初にシャッフルされ、次にトレーニングとテストセットのペアに分割されます。</target>
        </trans-unit>
        <trans-unit id="9d19931407bdaf26c2091f0ff552c9705b61d84b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt;&lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt;&lt;/a&gt; (LOF) algorithm computes a score (called local outlier factor) reflecting the degree of abnormality of the observations. It measures the local density deviation of a given data point with respect to its neighbors. The idea is to detect the samples that have a substantially lower density than their neighbors.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt; &lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt; &lt;/a&gt;（LOF）アルゴリズムは、観測値の異常の程度を反映する（局所的外れ値の因子と呼ばれる）スコアを計算します。これは、特定のデータポイントの近傍に対する局所的な密度偏差を測定します。アイデアは、隣接するサンプルよりも密度が大幅に低いサンプルを検出することです。</target>
        </trans-unit>
        <trans-unit id="7ee66eef276fb6deecd16d4b6508f3c8417f58ed" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.neighbors.nearestcentroid#sklearn.neighbors.NearestCentroid&quot;&gt;&lt;code&gt;NearestCentroid&lt;/code&gt;&lt;/a&gt; classifier has a &lt;code&gt;shrink_threshold&lt;/code&gt; parameter, which implements the nearest shrunken centroid classifier. In effect, the value of each feature for each centroid is divided by the within-class variance of that feature. The feature values are then reduced by &lt;code&gt;shrink_threshold&lt;/code&gt;. Most notably, if a particular feature value crosses zero, it is set to zero. In effect, this removes the feature from affecting the classification. This is useful, for example, for removing noisy features.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.neighbors.nearestcentroid#sklearn.neighbors.NearestCentroid&quot;&gt; &lt;code&gt;NearestCentroid&lt;/code&gt; &lt;/a&gt;分類器は持ってい &lt;code&gt;shrink_threshold&lt;/code&gt; 最短収縮重心分類器を実装したパラメータを、。実際には、各重心の各特徴の値は、その特徴のクラス内分散で除算されます。次に、機能値は &lt;code&gt;shrink_threshold&lt;/code&gt; によって削減されます。特に、特定の機能値がゼロと交差する場合は、ゼロに設定されます。事実上、これにより分類に影響を与える機能が削除されます。これは、ノイズの多い機能を削除する場合などに便利です。</target>
        </trans-unit>
        <trans-unit id="43771e48f74cd166aa989881024956f81686fd39" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.neighbors.nearestcentroid#sklearn.neighbors.NearestCentroid&quot;&gt;&lt;code&gt;NearestCentroid&lt;/code&gt;&lt;/a&gt; classifier is a simple algorithm that represents each class by the centroid of its members. In effect, this makes it similar to the label updating phase of the &lt;code&gt;sklearn.KMeans&lt;/code&gt; algorithm. It also has no parameters to choose, making it a good baseline classifier. It does, however, suffer on non-convex classes, as well as when classes have drastically different variances, as equal variance in all dimensions is assumed. See Linear Discriminant Analysis (&lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis&quot;&gt;&lt;code&gt;sklearn.discriminant_analysis.LinearDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt;) and Quadratic Discriminant Analysis (&lt;a href=&quot;generated/sklearn.discriminant_analysis.quadraticdiscriminantanalysis#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&quot;&gt;&lt;code&gt;sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt;) for more complex methods that do not make this assumption. Usage of the default &lt;a href=&quot;generated/sklearn.neighbors.nearestcentroid#sklearn.neighbors.NearestCentroid&quot;&gt;&lt;code&gt;NearestCentroid&lt;/code&gt;&lt;/a&gt; is simple:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.neighbors.nearestcentroid#sklearn.neighbors.NearestCentroid&quot;&gt; &lt;code&gt;NearestCentroid&lt;/code&gt; &lt;/a&gt;分類器は、そのメンバーの重心により、各クラスを表す単純なアルゴリズムです。実質的に、これは &lt;code&gt;sklearn.KMeans&lt;/code&gt; アルゴリズムのラベル更新フェーズと同様になります。また、選択するパラメーターがないため、ベースライン分類器として適しています。ただし、すべての次元で等しい分散が想定されているため、クラスが大幅に異なる分散を持つ場合と同様に、非凸クラスにも影響を与えます。この仮定を行わないより複雑な方法については、線形判別分析（&lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis&quot;&gt; &lt;code&gt;sklearn.discriminant_analysis.LinearDiscriminantAnalysis&lt;/code&gt; &lt;/a&gt;）および2次判別分析（&lt;a href=&quot;generated/sklearn.discriminant_analysis.quadraticdiscriminantanalysis#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&quot;&gt; &lt;code&gt;sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&lt;/code&gt; &lt;/a&gt;）を参照してください。デフォルトの&lt;a href=&quot;generated/sklearn.neighbors.nearestcentroid#sklearn.neighbors.NearestCentroid&quot;&gt; &lt;code&gt;NearestCentroid&lt;/code&gt; の&lt;/a&gt;使用 簡単です：</target>
        </trans-unit>
        <trans-unit id="39bfe25102ea45948a285842eddb73a441e962a3" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;Pipeline&lt;/code&gt;&lt;/a&gt; is built using a list of &lt;code&gt;(key, value)&lt;/code&gt; pairs, where the &lt;code&gt;key&lt;/code&gt; is a string containing the name you want to give this step and &lt;code&gt;value&lt;/code&gt; is an estimator object:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;Pipeline&lt;/code&gt; &lt;/a&gt;リスト使用して構築された &lt;code&gt;(key, value)&lt;/code&gt; のペア、 &lt;code&gt;key&lt;/code&gt; あなたがこのステップと付ける名前を含む文字列である &lt;code&gt;value&lt;/code&gt; 推定オブジェクトです：</target>
        </trans-unit>
        <trans-unit id="c923ad3a865326ec6c72af48e5305bcc89674896" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.random_projection.gaussianrandomprojection#sklearn.random_projection.GaussianRandomProjection&quot;&gt;&lt;code&gt;sklearn.random_projection.GaussianRandomProjection&lt;/code&gt;&lt;/a&gt; reduces the dimensionality by projecting the original input space on a randomly generated matrix where components are drawn from the following distribution \(N(0, \frac{1}{n_{components}})\).</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.random_projection.gaussianrandomprojection#sklearn.random_projection.GaussianRandomProjection&quot;&gt; &lt;code&gt;sklearn.random_projection.GaussianRandomProjection&lt;/code&gt; は、&lt;/a&gt;成分は以下の分布から引き出されるランダムに生成された行列\（N（0、\ FRAC {1} {N_ {成分}）\）で元の入力空間を投影することによって次元を減少させます。</target>
        </trans-unit>
        <trans-unit id="bf402b0deb2f6872588357c0d1a4ee6663793993" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.random_projection.sparserandomprojection#sklearn.random_projection.SparseRandomProjection&quot;&gt;&lt;code&gt;sklearn.random_projection.SparseRandomProjection&lt;/code&gt;&lt;/a&gt; reduces the dimensionality by projecting the original input space using a sparse random matrix.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.random_projection.sparserandomprojection#sklearn.random_projection.SparseRandomProjection&quot;&gt; &lt;code&gt;sklearn.random_projection.SparseRandomProjection&lt;/code&gt; は&lt;/a&gt;スパースランダム行列を使用して、元の入力空間を投影することによって次元を減少させます。</target>
        </trans-unit>
        <trans-unit id="bfc6d74a43acae56b2f26724a2d5ae8338eaf262" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.tree.export_graphviz#sklearn.tree.export_graphviz&quot;&gt;&lt;code&gt;export_graphviz&lt;/code&gt;&lt;/a&gt; exporter also supports a variety of aesthetic options, including coloring nodes by their class (or value for regression) and using explicit variable and class names if desired. Jupyter notebooks also render these plots inline automatically:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.tree.export_graphviz#sklearn.tree.export_graphviz&quot;&gt; &lt;code&gt;export_graphviz&lt;/code&gt; の&lt;/a&gt;輸出はまた、所望であれば、それらのクラス（または回帰の値）ノードを着色し、明示的な変数及びクラス名を使用することを含む美的オプション、様々なサポート。Jupyterノートブックは、これらのプロットを自動的にインラインでレンダリングします。</target>
        </trans-unit>
        <trans-unit id="4a15d0b16a3ea1ca7d8280fbccb678c643c12770" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;https://en.wikipedia.org/wiki/F1_score&quot;&gt;F-measure&lt;/a&gt; (\(F_\beta\) and \(F_1\) measures) can be interpreted as a weighted harmonic mean of the precision and recall. A \(F_\beta\) measure reaches its best value at 1 and its worst score at 0. With \(\beta = 1\), \(F_\beta\) and \(F_1\) are equivalent, and the recall and the precision are equally important.</source>
          <target state="translated">&lt;a href=&quot;https://en.wikipedia.org/wiki/F1_score&quot;&gt;F値&lt;/a&gt;（\（F_ \ベータ版\）と\（F_1 \）措置は）精度と再現率の加重調和平均と解釈することができます。\（F_ \ beta \）メジャーは、1でその最高値に達し、0でその最悪スコアに達します。\（\ beta = 1 \）では、\（F_ \ beta \）と\（F_1 \）は同等であり、リコールと精度も同様に重要です。</target>
        </trans-unit>
        <trans-unit id="88cf2fa597f50207550c56a5d725499fc42ad462" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma&quot;&gt;Johnson-Lindenstrauss lemma&lt;/a&gt; states that any high dimensional dataset can be randomly projected into a lower dimensional Euclidean space while controlling the distortion in the pairwise distances.</source>
          <target state="translated">&lt;a href=&quot;https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma&quot;&gt;ジョンソン・Lindenstrauss補題は、&lt;/a&gt;ペアワイズ距離の歪みを制御しながら、任意の高次元のデータセットがランダムに低次元のユークリッド空間に投影することができると述べています。</target>
        </trans-unit>
        <trans-unit id="96678c00449216bcbe65a0961a8b25b8baf7a396" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;BayesianGaussianMixture&lt;/code&gt; class can adapt its number of mixture components automatically. The parameter &lt;code&gt;weight_concentration_prior&lt;/code&gt; has a direct link with the resulting number of components with non-zero weights. Specifying a low value for the concentration prior will make the model put most of the weight on few components set the remaining components weights very close to zero. High values of the concentration prior will allow a larger number of components to be active in the mixture.</source>
          <target state="translated">&lt;code&gt;BayesianGaussianMixture&lt;/code&gt; のクラスが自動的に混合成分のその数を適応させることができます。パラメータ &lt;code&gt;weight_concentration_prior&lt;/code&gt; は、重みがゼロでないコンポーネントの結果の数と直接リンクしています。事前に濃度に低い値を指定すると、モデルはほとんどの重量を少数のコンポーネントに配置し、残りのコンポーネントの重量を非常にゼロに設定します。前の濃度の値が高いと、混合物中でより多くの成分を活性化できます。</target>
        </trans-unit>
        <trans-unit id="c17c439e95320993d0276d174b035cd14b7ce3b3" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;C&lt;/code&gt; parameter controls the amount of regularization in the &lt;a href=&quot;../../modules/generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; object: a large value for &lt;code&gt;C&lt;/code&gt; results in less regularization. &lt;code&gt;penalty=&quot;l2&quot;&lt;/code&gt; gives &lt;a href=&quot;#shrinkage&quot;&gt;Shrinkage&lt;/a&gt; (i.e. non-sparse coefficients), while &lt;code&gt;penalty=&quot;l1&quot;&lt;/code&gt; gives &lt;a href=&quot;#sparsity&quot;&gt;Sparsity&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;C&lt;/code&gt; のパラメータ制御における正則化の量&lt;a href=&quot;../../modules/generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt;目的のための：大きな値 &lt;code&gt;C&lt;/code&gt; を以下正則化をもたらします。 &lt;code&gt;penalty=&quot;l2&quot;&lt;/code&gt; 得られる&lt;a href=&quot;#shrinkage&quot;&gt;収縮&lt;/a&gt;しながら、（すなわち、非スパース係数） &lt;code&gt;penalty=&quot;l1&quot;&lt;/code&gt; 得られる&lt;a href=&quot;#sparsity&quot;&gt;スパースを&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="9164d9a9144eaecf5fe284f2e40277e82f0b8068" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;C&lt;/code&gt; parameter trades off correct classification of training examples against maximization of the decision function&amp;rsquo;s margin. For larger values of &lt;code&gt;C&lt;/code&gt;, a smaller margin will be accepted if the decision function is better at classifying all training points correctly. A lower &lt;code&gt;C&lt;/code&gt; will encourage a larger margin, therefore a simpler decision function, at the cost of training accuracy. In other words``C`` behaves as a regularization parameter in the SVM.</source>
          <target state="translated">&lt;code&gt;C&lt;/code&gt; の決定関数のマージンの最大化に対する訓練例の正しい分類オフパラメータ取引。 &lt;code&gt;C&lt;/code&gt; の値が大きい場合、すべてのトレーニングポイントを正しく分類するための決定関数の方が優れていれば、マージンは小さくなります。 &lt;code&gt;C&lt;/code&gt; が低いと、マージンが大きくなるため、決定関数が単純になりますが、トレーニングの精度は犠牲になります。言い換えると、「C」はSVMの正則化パラメーターとして動作します。</target>
        </trans-unit>
        <trans-unit id="9aa2de3f6ced8022ed53d959fe2e18d24e70ecf1" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;DESCR&lt;/code&gt; contains a free-text description of the data, while &lt;code&gt;details&lt;/code&gt; contains a dictionary of meta-data stored by openml, like the dataset id. For more details, see the &lt;a href=&quot;https://docs.openml.org/#data&quot;&gt;OpenML documentation&lt;/a&gt; The &lt;code&gt;data_id&lt;/code&gt; of the mice protein dataset is 40966, and you can use this (or the name) to get more information on the dataset on the openml website:</source>
          <target state="translated">&lt;code&gt;DESCR&lt;/code&gt; は一方で、データのフリーテキスト記述が含まれています &lt;code&gt;details&lt;/code&gt; データセットIDのように、openmlによって保存されたメタデータの辞書が含まれています。詳細については、&lt;a href=&quot;https://docs.openml.org/#data&quot;&gt;OpenMLのドキュメントを&lt;/a&gt; &lt;code&gt;data_id&lt;/code&gt; マウスタンパク質データセットのが40966である、とあなたがopenmlウェブサイト上のデータセットに関する詳細な情報を取得するには、この（または名前）を使用することができます。</target>
        </trans-unit>
        <trans-unit id="81a0037eca65e4ed69e2a49ca4871b0ce138bc10" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;Normalizer&lt;/code&gt; rescales the vector for each sample to have unit norm, independently of the distribution of the samples. It can be seen on both figures below where all samples are mapped onto the unit circle. In our example the two selected features have only positive values; therefore the transformed data only lie in the positive quadrant. This would not be the case if some original features had a mix of positive and negative values.</source>
          <target state="translated">&lt;code&gt;Normalizer&lt;/code&gt; 独立して、サンプルの分布の、単位ノルムを有するように各サンプルのベクトルを再スケール。これは、すべてのサンプルが単位円にマッピングされている以下の両方の図で確認できます。この例では、選択された2つの機能は正の値のみを持っています。したがって、変換されたデータは正の象限にのみ存在します。一部の元の機能に正と負の値が混在している場合、これは当てはまりません。</target>
        </trans-unit>
        <trans-unit id="c108938c180fba7834742cb26f12054acc7a2184" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;PCA&lt;/code&gt; fitting is only computed at the evaluation of the first configuration of the &lt;code&gt;C&lt;/code&gt; parameter of the &lt;code&gt;LinearSVC&lt;/code&gt; classifier. The other configurations of &lt;code&gt;C&lt;/code&gt; will trigger the loading of the cached &lt;code&gt;PCA&lt;/code&gt; estimator data, leading to save processing time. Therefore, the use of caching the pipeline using &lt;code&gt;memory&lt;/code&gt; is highly beneficial when fitting a transformer is costly.</source>
          <target state="translated">&lt;code&gt;PCA&lt;/code&gt; のフィッティングのみの第一の構成の評価で計算された &lt;code&gt;C&lt;/code&gt; 用のパラメータ &lt;code&gt;LinearSVC&lt;/code&gt; のクラシファイア。 &lt;code&gt;C&lt;/code&gt; の他の構成は、キャッシュされた &lt;code&gt;PCA&lt;/code&gt; 見積もりデータのロードをトリガーし、処理時間を節約します。したがって、 &lt;code&gt;memory&lt;/code&gt; を使用してパイプラインをキャッシュすることは、トランスフォーマーの取り付けにコストがかかる場合に非常に有益です。</target>
        </trans-unit>
        <trans-unit id="17baab07595bc9a1b9010bf52fc5ebb7c1555943" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;RandomForestClassifier&lt;/code&gt; is trained using &lt;em&gt;bootstrap aggregation&lt;/em&gt;, where each new tree is fit from a bootstrap sample of the training observations \(z_i = (x_i, y_i)\). The &lt;em&gt;out-of-bag&lt;/em&gt; (OOB) error is the average error for each \(z_i\) calculated using predictions from the trees that do not contain \(z_i\) in their respective bootstrap sample. This allows the &lt;code&gt;RandomForestClassifier&lt;/code&gt; to be fit and validated whilst being trained &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;RandomForestClassifier&lt;/code&gt; を使用して訓練された&lt;em&gt;ブートストラップ集約&lt;/em&gt;それぞれの新しいツリーがトレーニング観測\（z_i =（X_I、Y_I）\）のブートストラップサンプルからのフィット感です。&lt;em&gt;アウトオブバッグ&lt;/em&gt;（OOB）エラーは、各\（z_i \）には、それぞれのブートストラップサンプル中（z_i \）\含まれていない木からの予測を使用して計算のための平均誤差です。これにより、トレーニング中に &lt;code&gt;RandomForestClassifier&lt;/code&gt; を適合および検証することができます&lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="093b9af7ff877738a1c191bf8fe58f666ce1b93c" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;VotingClassifier&lt;/code&gt; can also be used together with &lt;code&gt;GridSearch&lt;/code&gt; in order to tune the hyperparameters of the individual estimators:</source>
          <target state="translated">&lt;code&gt;VotingClassifier&lt;/code&gt; はまた、一緒に使用することができる &lt;code&gt;GridSearch&lt;/code&gt; 個々推定のハイパー調整するために：</target>
        </trans-unit>
        <trans-unit id="a92f24d7703ca3728952e82f17755a0b7604fe2c" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;alpha&lt;/code&gt; parameter controls the degree of sparsity of the coefficients estimated.</source>
          <target state="translated">&lt;code&gt;alpha&lt;/code&gt; パラメータは、推定された係数のスパース性の程度を制御します。</target>
        </trans-unit>
        <trans-unit id="d22b7c9a5ce685adf8551f77a733c13cab8fe2d4" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;best_estimator_&lt;/code&gt;, &lt;code&gt;best_index_&lt;/code&gt;, &lt;code&gt;best_score_&lt;/code&gt; and &lt;code&gt;best_params_&lt;/code&gt; correspond to the scorer (key) that is set to the &lt;code&gt;refit&lt;/code&gt; attribute.</source>
          <target state="translated">&lt;code&gt;best_estimator_&lt;/code&gt; 、 &lt;code&gt;best_index_&lt;/code&gt; 、 &lt;code&gt;best_score_&lt;/code&gt; と &lt;code&gt;best_params_&lt;/code&gt; に設定されている得点（キー）に対応する &lt;code&gt;refit&lt;/code&gt; 属性。</target>
        </trans-unit>
        <trans-unit id="ea04fcbf02a8de4070a990d8e4f932cdf0278b0d" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;beta&lt;/code&gt; parameter determines the weight of precision in the combined score. &lt;code&gt;beta &amp;lt; 1&lt;/code&gt; lends more weight to precision, while &lt;code&gt;beta &amp;gt; 1&lt;/code&gt; favors recall (&lt;code&gt;beta -&amp;gt; 0&lt;/code&gt; considers only precision, &lt;code&gt;beta -&amp;gt; inf&lt;/code&gt; only recall).</source>
          <target state="translated">&lt;code&gt;beta&lt;/code&gt; パラメータは、複合スコアの精度の重量を決定します。 &lt;code&gt;beta &amp;lt; 1&lt;/code&gt; は精度を重視し、 &lt;code&gt;beta &amp;gt; 1&lt;/code&gt; は再現を優先します（ &lt;code&gt;beta -&amp;gt; 0&lt;/code&gt; は精度のみを考慮し、 &lt;code&gt;beta -&amp;gt; inf&lt;/code&gt; は再現のみを考慮します）。</target>
        </trans-unit>
        <trans-unit id="1cbc8f71751f51b8523a564fa7c56816a950df46" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;clf&lt;/code&gt; (for classifier) estimator instance is first fitted to the model; that is, it must &lt;em&gt;learn&lt;/em&gt; from the model. This is done by passing our training set to the &lt;code&gt;fit&lt;/code&gt; method. For the training set, we&amp;rsquo;ll use all the images from our dataset, except for the last image, which we&amp;rsquo;ll reserve for our predicting. We select the training set with the &lt;code&gt;[:-1]&lt;/code&gt; Python syntax, which produces a new array that contains all but the last item from &lt;code&gt;digits.data&lt;/code&gt;:</source>
          <target state="translated">&lt;code&gt;clf&lt;/code&gt; 推定インスタンス（分類器のための）は、最初のモデルに装着されています。つまり、モデルから&lt;em&gt;学習&lt;/em&gt;する必要があり&lt;em&gt;ます&lt;/em&gt;。これは、トレーニングセットを &lt;code&gt;fit&lt;/code&gt; メソッドに渡すことで行われます。トレーニングセットでは、予測のために予約する最後の画像を除いて、データセットのすべての画像を使用します。 &lt;code&gt;[:-1]&lt;/code&gt; Python構文を使用してトレーニングセットを選択します。これにより、 &lt;code&gt;digits.data&lt;/code&gt; の最後の項目を除くすべてを含む新しい配列が生成されます。</target>
        </trans-unit>
        <trans-unit id="0655de3e2b717fc73c590188fdaa9881c37414a7" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;cross_validate&lt;/code&gt; function differs from &lt;code&gt;cross_val_score&lt;/code&gt; in two ways -</source>
          <target state="translated">&lt;code&gt;cross_validate&lt;/code&gt; 機能が異なり &lt;code&gt;cross_val_score&lt;/code&gt; 二つの方法で-</target>
        </trans-unit>
        <trans-unit id="f3aad90428722c87b3422d97c1856aa8204966ed" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;cv_results_&lt;/code&gt; parameter can be easily imported into pandas as a &lt;code&gt;DataFrame&lt;/code&gt; for further inspection.</source>
          <target state="translated">&lt;code&gt;cv_results_&lt;/code&gt; のパラメータを簡単としてパンダにインポートすることができ &lt;code&gt;DataFrame&lt;/code&gt; 詳細な検査をするために。</target>
        </trans-unit>
        <trans-unit id="6d0144f231c5dfa868fda545c176e4a6eca95147" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;data_id&lt;/code&gt; also uniquely identifies a dataset from OpenML:</source>
          <target state="translated">&lt;code&gt;data_id&lt;/code&gt; また、一意OpenMLからデータセットを識別する。</target>
        </trans-unit>
        <trans-unit id="7bfd5d978dedbc7159d3a401f357dd25ad25428a" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;decision_function&lt;/code&gt; method is also defined from the scoring function, in such a way that negative values are outliers and non-negative ones are inliers:</source>
          <target state="translated">また、 &lt;code&gt;decision_function&lt;/code&gt; メソッドは、負の値が外れ値で、非負の値が外れ値になるように、スコアリング関数から定義されます。</target>
        </trans-unit>
        <trans-unit id="62eacbcbc4132ef1f5317a0a939d640165e31df3" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;decision_function&lt;/code&gt; method of &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.nusvc#sklearn.svm.NuSVC&quot;&gt;&lt;code&gt;NuSVC&lt;/code&gt;&lt;/a&gt; gives per-class scores for each sample (or a single score per sample in the binary case). When the constructor option &lt;code&gt;probability&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, class membership probability estimates (from the methods &lt;code&gt;predict_proba&lt;/code&gt; and &lt;code&gt;predict_log_proba&lt;/code&gt;) are enabled. In the binary case, the probabilities are calibrated using Platt scaling: logistic regression on the SVM&amp;rsquo;s scores, fit by an additional cross-validation on the training data. In the multiclass case, this is extended as per Wu et al. (2004).</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;SVC&lt;/code&gt; &lt;/a&gt;および&lt;a href=&quot;generated/sklearn.svm.nusvc#sklearn.svm.NuSVC&quot;&gt; &lt;code&gt;NuSVC&lt;/code&gt; &lt;/a&gt;の &lt;code&gt;decision_function&lt;/code&gt; メソッドは、各サンプルのクラスごとのスコア（またはバイナリの場合はサンプルごとに単一のスコア）を提供します。コンストラクタオプションの &lt;code&gt;probability&lt;/code&gt; が &lt;code&gt;True&lt;/code&gt; に設定されている場合、（メソッド &lt;code&gt;predict_proba&lt;/code&gt; および &lt;code&gt;predict_log_proba&lt;/code&gt; からの）クラスメンバーシップ確率推定が有効になります。バイナリの場合、確率はプラットスケーリングを使用して調整されます。SVMのスコアのロジスティック回帰は、トレーニングデータの追加の相互検証によって適合されます。マルチクラスの場合、これはWuらのように拡張されます。 （2004）。</target>
        </trans-unit>
        <trans-unit id="ac9899847d23144b2277382b5ec5bf6360733941" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;features&lt;/code&gt; parameter can be set to &lt;code&gt;'all'&lt;/code&gt; to returned all features whether or not they contain missing values:</source>
          <target state="translated">&lt;code&gt;features&lt;/code&gt; パラメータがに設定することができます &lt;code&gt;'all'&lt;/code&gt; ：彼らは欠損値が含まれているかどうかに関係なく、すべての機能を返却します</target>
        </trans-unit>
        <trans-unit id="0e29ac108f496200ac17f2eb1912fca379623586" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;features&lt;/code&gt; parameter is used to choose the features for which the mask is constructed. By default, it is &lt;code&gt;'missing-only'&lt;/code&gt; which returns the imputer mask of the features containing missing values at &lt;code&gt;fit&lt;/code&gt; time:</source>
          <target state="translated">&lt;code&gt;features&lt;/code&gt; マスクが構築されている機能を選択するために使用されるパラメータ。デフォルトでは、それは &lt;code&gt;'missing-only'&lt;/code&gt; であり、 &lt;code&gt;fit&lt;/code&gt; 時に欠損値を含む特徴の入力マスクを返します。</target>
        </trans-unit>
        <trans-unit id="9b06298ca8347e48ebdf3df08a4c5d05e9d2dcbb" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;fit&lt;/code&gt; function takes two arguments: &lt;code&gt;n_components&lt;/code&gt;, which is the target dimensionality of the feature transform, and &lt;code&gt;gamma&lt;/code&gt;, the parameter of the RBF-kernel. A higher &lt;code&gt;n_components&lt;/code&gt; will result in a better approximation of the kernel and will yield results more similar to those produced by a kernel SVM. Note that &amp;ldquo;fitting&amp;rdquo; the feature function does not actually depend on the data given to the &lt;code&gt;fit&lt;/code&gt; function. Only the dimensionality of the data is used. Details on the method can be found in &lt;a href=&quot;#rr2007&quot; id=&quot;id3&quot;&gt;[RR2007]&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; ：関数は2つの引数取り &lt;code&gt;n_components&lt;/code&gt; 機能のターゲット次元変換であると、 &lt;code&gt;gamma&lt;/code&gt; 、RBFカーネルのパラメータを。 &lt;code&gt;n_components&lt;/code&gt; が高いほど、カーネルの近似が向上し、カーネルSVMによって生成される結果により近い結果が得られます。特徴関数の「適合」は、実際には &lt;code&gt;fit&lt;/code&gt; 関数に与えられたデータに依存しないことに注意してください。データの次元のみが使用されます。この方法の詳細は&lt;a href=&quot;#rr2007&quot; id=&quot;id3&quot;&gt;[RR2007]にあり&lt;/a&gt;ます。</target>
        </trans-unit>
        <trans-unit id="759d68cbc1b0e0c960b6f5234a5fe3b985174684" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;init&lt;/code&gt; attribute determines the initialization method applied, which has a great impact on the performance of the method. &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;NMF&lt;/code&gt;&lt;/a&gt; implements the method Nonnegative Double Singular Value Decomposition. NNDSVD &lt;a href=&quot;#id13&quot; id=&quot;id7&quot;&gt;[4]&lt;/a&gt; is based on two SVD processes, one approximating the data matrix, the other approximating positive sections of the resulting partial SVD factors utilizing an algebraic property of unit rank matrices. The basic NNDSVD algorithm is better fit for sparse factorization. Its variants NNDSVDa (in which all zeros are set equal to the mean of all elements of the data), and NNDSVDar (in which the zeros are set to random perturbations less than the mean of the data divided by 100) are recommended in the dense case.</source>
          <target state="translated">&lt;code&gt;init&lt;/code&gt; 属性は、メソッドのパフォーマンスに大きな影響を与える適用初期化方法を決定します。&lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;NMF&lt;/code&gt; &lt;/a&gt;は、メソッドNonnegative Double Singular Value Decompositionを実装しています。 NNDSVD &lt;a href=&quot;#id13&quot; id=&quot;id7&quot;&gt;[4]&lt;/a&gt;は2つのSVDプロセスに基づいており、1つはデータマトリックスを近似し、もう1つは、ユニットランクマトリックスの代数プロパティを利用して、結果の部分SVD因子の正のセクションを近似します。基本的なNNDSVDアルゴリズムは、スパース分解に適しています。そのバリアントNNDSVDa（すべてのゼロはデータのすべての要素の平均に等しく設定されます）、およびNNDSVDar（ゼロはデータの平均を100で割った値よりも小さいランダムな摂動に設定されます）は密に推奨されます場合。</target>
        </trans-unit>
        <trans-unit id="5aaf2ff68858d9c24eece58235794e4a322e1ce9" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;intercept_&lt;/code&gt; member is not converted.</source>
          <target state="translated">&lt;code&gt;intercept_&lt;/code&gt; メンバーは変換されません。</target>
        </trans-unit>
        <trans-unit id="0e6b2b935d052640da205c359a0d82666ebb9942" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;is_data_valid&lt;/code&gt; and &lt;code&gt;is_model_valid&lt;/code&gt; functions allow to identify and reject degenerate combinations of random sub-samples. If the estimated model is not needed for identifying degenerate cases, &lt;code&gt;is_data_valid&lt;/code&gt; should be used as it is called prior to fitting the model and thus leading to better computational performance.</source>
          <target state="translated">&lt;code&gt;is_data_valid&lt;/code&gt; と &lt;code&gt;is_model_valid&lt;/code&gt; 機能はランダムサブサンプルの縮退の組み合わせを識別し、拒否することを可能にします。推定モデルが縮退したケースを識別するために必要でない場合は、モデルの近似前に呼び出される &lt;code&gt;is_data_valid&lt;/code&gt; を使用して、計算パフォーマンスを向上させる必要があります。</target>
        </trans-unit>
        <trans-unit id="a1aa8bc8d7f393abce9beb6161257c50a1665624" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;len(features)&lt;/code&gt; plots are arranged in a grid with &lt;code&gt;n_cols&lt;/code&gt; columns. Two-way partial dependence plots are plotted as contour plots.</source>
          <target state="translated">&lt;code&gt;len(features)&lt;/code&gt; プロットして格子状に配置され &lt;code&gt;n_cols&lt;/code&gt; の列。双方向の部分依存プロットは、等高線図としてプロットされます。</target>
        </trans-unit>
        <trans-unit id="4d39915194cee376ca662b61de9924274942d60a" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;make_columntransformer&lt;/code&gt; function is available to more easily create a &lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;ColumnTransformer&lt;/code&gt;&lt;/a&gt; object. Specifically, the names will be given automatically. The equivalent for the above example would be:</source>
          <target state="translated">&lt;code&gt;make_columntransformer&lt;/code&gt; の機能をより簡単に作成することが可能です&lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt; &lt;code&gt;ColumnTransformer&lt;/code&gt; の&lt;/a&gt;オブジェクトを。具体的には、名前は自動的に付けられます。上記の例と同等のものは次のとおりです。</target>
        </trans-unit>
        <trans-unit id="e3ab7886f45f0e5bcfcb494c5dda13f6aee54058" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;mean_fit_time&lt;/code&gt;, &lt;code&gt;std_fit_time&lt;/code&gt;, &lt;code&gt;mean_score_time&lt;/code&gt; and &lt;code&gt;std_score_time&lt;/code&gt; are all in seconds.</source>
          <target state="translated">&lt;code&gt;mean_fit_time&lt;/code&gt; 、 &lt;code&gt;std_fit_time&lt;/code&gt; 、 &lt;code&gt;mean_score_time&lt;/code&gt; と &lt;code&gt;std_score_time&lt;/code&gt; は数秒ですべてです。</target>
        </trans-unit>
        <trans-unit id="4dc710fdb008b124893854b1db0d772123e2f23c" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;out_of_bounds&lt;/code&gt; parameter handles how x-values outside of the training domain are handled. When set to &amp;ldquo;nan&amp;rdquo;, predicted y-values will be NaN. When set to &amp;ldquo;clip&amp;rdquo;, predicted y-values will be set to the value corresponding to the nearest train interval endpoint. When set to &amp;ldquo;raise&amp;rdquo;, allow &lt;code&gt;interp1d&lt;/code&gt; to throw ValueError.</source>
          <target state="translated">&lt;code&gt;out_of_bounds&lt;/code&gt; ハンドルパラメータがどのようにx値外の研修ドメインの処理されます。「nan」に設定すると、予測されるy値はNaNになります。「clip」に設定すると、予測されるy値は、最も近い列車間隔のエンドポイントに対応する値に設定されます。「raise」に設定すると、 &lt;code&gt;interp1d&lt;/code&gt; がValueErrorをスローできるようになります。</target>
        </trans-unit>
        <trans-unit id="3c0e73047db48d9d2c7fabbdd5b52d96e2b806a9" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;partial_fit&lt;/code&gt; method call of naive Bayes models introduces some computational overhead. It is recommended to use data chunk sizes that are as large as possible, that is as the available RAM allows.</source>
          <target state="translated">単純ベイズモデルの &lt;code&gt;partial_fit&lt;/code&gt; メソッド呼び出しは、いくつかの計算オーバーヘッドをもたらします。使用可能なRAMが許す限り、可能な限り大きいデータチャンクサイズを使用することをお勧めします。</target>
        </trans-unit>
        <trans-unit id="c075895944959b3ce70972d2605db496c74ee36b" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;preprocessing&lt;/code&gt; module further provides a utility class &lt;a href=&quot;generated/sklearn.preprocessing.normalizer#sklearn.preprocessing.Normalizer&quot;&gt;&lt;code&gt;Normalizer&lt;/code&gt;&lt;/a&gt; that implements the same operation using the &lt;code&gt;Transformer&lt;/code&gt; API (even though the &lt;code&gt;fit&lt;/code&gt; method is useless in this case: the class is stateless as this operation treats samples independently).</source>
          <target state="translated">&lt;code&gt;preprocessing&lt;/code&gt; モジュールはさらに、ユーティリティクラス提供&lt;a href=&quot;generated/sklearn.preprocessing.normalizer#sklearn.preprocessing.Normalizer&quot;&gt; &lt;code&gt;Normalizer&lt;/code&gt; &lt;/a&gt;実装同じ操作を使用してその &lt;code&gt;Transformer&lt;/code&gt; （にもかかわらずAPIを &lt;code&gt;fit&lt;/code&gt; ：クラスは、独立して、この操作扱いサンプルとしてステートレスである方法は、この場合には無用です）。</target>
        </trans-unit>
        <trans-unit id="deb9c47cdc32b652eb6b8e87508a50e73ddd6520" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;preprocessing&lt;/code&gt; module further provides a utility class &lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt;&lt;code&gt;StandardScaler&lt;/code&gt;&lt;/a&gt; that implements the &lt;code&gt;Transformer&lt;/code&gt; API to compute the mean and standard deviation on a training set so as to be able to later reapply the same transformation on the testing set. This class is hence suitable for use in the early steps of a &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">&lt;code&gt;preprocessing&lt;/code&gt; モジュールはさらに、ユーティリティクラス提供&lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt; &lt;code&gt;StandardScaler&lt;/code&gt; を&lt;/a&gt;実装する &lt;code&gt;Transformer&lt;/code&gt; テストセットに後で再適用同じ変換することができるように、APIは、トレーニングセット上の平均値と標準偏差を計算します。したがって、このクラスは&lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt; の&lt;/a&gt;初期段階での使用に適しています。</target>
        </trans-unit>
        <trans-unit id="1c988d2d2cf78f045059c60f8f22ddfe99f6b9b8" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;random_state&lt;/code&gt; parameter defaults to &lt;code&gt;None&lt;/code&gt;, meaning that the shuffling will be different every time &lt;code&gt;KFold(..., shuffle=True)&lt;/code&gt; is iterated. However, &lt;code&gt;GridSearchCV&lt;/code&gt; will use the same shuffling for each set of parameters validated by a single call to its &lt;code&gt;fit&lt;/code&gt; method.</source>
          <target state="translated">&lt;code&gt;random_state&lt;/code&gt; へのパラメータのデフォルト値 &lt;code&gt;None&lt;/code&gt; シャッフルは毎回異なるものになることを意味し、 &lt;code&gt;KFold(..., shuffle=True)&lt;/code&gt; 繰り返されます。ただし、 &lt;code&gt;GridSearchCV&lt;/code&gt; は、 &lt;code&gt;fit&lt;/code&gt; メソッドの1回の呼び出しで検証されたパラメーターの各セットに同じシャッフルを使用します。</target>
        </trans-unit>
        <trans-unit id="269b993dc29ab1401256004d5e02aa3f5322b3b6" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;remainder&lt;/code&gt; parameter can be set to an estimator to transform the remaining rating columns. The transformed values are appended to the end of the transformation:</source>
          <target state="translated">&lt;code&gt;remainder&lt;/code&gt; パラメータは、残りの格付け列を変換するために推定に設定することができます。変換された値は、変換の最後に追加されます。</target>
        </trans-unit>
        <trans-unit id="5cf025ac399ddc1c71c004c7cb8bd73171357561" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;shrinkage&lt;/code&gt; parameter can also be manually set between 0 and 1. In particular, a value of 0 corresponds to no shrinkage (which means the empirical covariance matrix will be used) and a value of 1 corresponds to complete shrinkage (which means that the diagonal matrix of variances will be used as an estimate for the covariance matrix). Setting this parameter to a value between these two extrema will estimate a shrunk version of the covariance matrix.</source>
          <target state="translated">&lt;code&gt;shrinkage&lt;/code&gt; 完全収縮パラメータは、手動で（経験的な共分散行列が使用されることを意味する）、具体的には0と1の間に収縮0相当の値を設定することができ、1相当の値（平均対角線その分散行列は、共分散行列の推定値として使用されます）。このパラメーターをこれら2つの極値の間の値に設定すると、共分散行列の縮小バージョンが推定されます。</target>
        </trans-unit>
        <trans-unit id="b439b9c10b67475737ae3e151e90a6ed815edbd1" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.covariance&lt;/code&gt; package implements a robust estimator of covariance, the Minimum Covariance Determinant &lt;a href=&quot;#id11&quot; id=&quot;id8&quot;&gt;[3]&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;sklearn.covariance&lt;/code&gt; のパッケージは、共分散のロバストな推定器を実装最小共分散行列式&lt;a href=&quot;#id11&quot; id=&quot;id8&quot;&gt;[3] &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="6908af1748b7fe666814b53ca6e0a8cab66eb012" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.datasets&lt;/code&gt; package embeds some small toy datasets as introduced in the &lt;a href=&quot;../tutorial/basic/tutorial#loading-example-dataset&quot;&gt;Getting Started&lt;/a&gt; section.</source>
          <target state="translated">&lt;code&gt;sklearn.datasets&lt;/code&gt; のパッケージはで導入されたとして、いくつかの小さなおもちゃのデータセットを埋め込む&lt;a href=&quot;../tutorial/basic/tutorial#loading-example-dataset&quot;&gt;はじめ&lt;/a&gt;セクション。</target>
        </trans-unit>
        <trans-unit id="82c9f1d0e48a88b08f4d85a858e0d70b67f55eec" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.datasets&lt;/code&gt; package is able to download datasets from the repository using the function &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_openml#sklearn.datasets.fetch_openml&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_openml&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;sklearn.datasets&lt;/code&gt; のパッケージには、機能の使用してリポジトリからデータセットをダウンロードすることができます&lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_openml#sklearn.datasets.fetch_openml&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_openml&lt;/code&gt; を&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="8a80b4da04f92037fdd1ffd5771651becd3a649e" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.preprocessing&lt;/code&gt; package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators.</source>
          <target state="translated">&lt;code&gt;sklearn.preprocessing&lt;/code&gt; パッケージには、下流の推定のために、より適している表現に生の特徴ベクトルを変更するには、いくつかの一般的なユーティリティ機能とトランスクラスを提供します。</target>
        </trans-unit>
        <trans-unit id="9a666cbe94ae4d4ef285ecec57ff29087d1b1c92" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;stop_words_&lt;/code&gt; attribute can get large and increase the model size when pickling. This attribute is provided only for introspection and can be safely removed using delattr or set to None before pickling.</source>
          <target state="translated">&lt;code&gt;stop_words_&lt;/code&gt; の属性は、大規模な取得し、酸洗時にモデルのサイズを大きくすることができます。この属性はイントロスペクションのためにのみ提供されており、酸洗いの前にdelattrを使用して安全に削除するか、Noneに設定できます。</target>
        </trans-unit>
        <trans-unit id="a2df492be59e0b5e94ab9ece47f7ab58734f8d4a" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;svm.OneClassSVM&lt;/code&gt; is known to be sensitive to outliers and thus does not perform very well for outlier detection. This estimator is best suited for novelty detection when the training set is not contaminated by outliers. That said, outlier detection in high-dimension, or without any assumptions on the distribution of the inlying data is very challenging, and a One-class SVM might give useful results in these situations depending on the value of its hyperparameters.</source>
          <target state="translated">&lt;code&gt;svm.OneClassSVM&lt;/code&gt; は外れ値に敏感であることが知られているので、外れ値検出のために非常によく実行されません。この推定器は、トレーニングセットが外れ値で汚染されていない場合の新規性検出に最適です。とはいえ、高次元での異常値の検出、または元となるデータの分布を想定せずに行うことは非常に困難であり、1クラスSVMは、ハイパーパラメーターの値によっては、このような状況で有用な結果をもたらす可能性があります。</target>
        </trans-unit>
        <trans-unit id="a0172c625369e43fa830669101114953189c751e" translate="yes" xml:space="preserve">
          <source>The &lt;em&gt;kernel function&lt;/em&gt; can be any of the following:</source>
          <target state="translated">&lt;em&gt;カーネル関数は&lt;/em&gt;、次のいずれかになります。</target>
        </trans-unit>
        <trans-unit id="6eff32d476fb25ebcdfdcb46623a8711aa972809" translate="yes" xml:space="preserve">
          <source>The &lt;strong&gt;conditional entropy of clusters given class&lt;/strong&gt;\(H(K|C)\) and the &lt;strong&gt;entropy of clusters&lt;/strong&gt;\(H(K)\) are defined in a symmetric manner.</source>
          <target state="translated">&lt;strong&gt;クラス&lt;/strong&gt; \（H（K | C ）\）が指定された&lt;strong&gt;クラスター&lt;/strong&gt;の&lt;strong&gt;条件付きエントロピーとクラスター&lt;/strong&gt; \（H（K）\）の&lt;strong&gt;エントロピーは&lt;/strong&gt;、対称的に定義されます。</target>
        </trans-unit>
        <trans-unit id="2143ba51f2aad99e99e8cd60a62ccfdecbf0f265" translate="yes" xml:space="preserve">
          <source>The AMI returns a value of 1 when the two partitions are identical (ie perfectly matched). Random partitions (independent labellings) have an expected AMI around 0 on average hence can be negative.</source>
          <target state="translated">AMIは、2つのパーティションが同じ(完全に一致している)場合、1の値を返します。ランダムなパーティション(独立したラベリング)は、平均して0前後のAMIが期待されるため、負の値になることがあります。</target>
        </trans-unit>
        <trans-unit id="00e3722885bdadae5430c6fecaacfc1cced893f6" translate="yes" xml:space="preserve">
          <source>The API is experimental in version 0.20 (particularly the return value structure), and might have small backward-incompatible changes in future releases.</source>
          <target state="translated">このAPIはバージョン0.20では実験的なものであり(特に戻り値構造)、将来のリリースでは下位互換性のない小さな変更があるかもしれません。</target>
        </trans-unit>
        <trans-unit id="22556f00c2cdfc8c60673e1c663299619a64901c" translate="yes" xml:space="preserve">
          <source>The BIC criterion can be used to select the number of components in a Gaussian Mixture in an efficient way. In theory, it recovers the true number of components only in the asymptotic regime (i.e. if much data is available and assuming that the data was actually generated i.i.d. from a mixture of Gaussian distribution). Note that using a &lt;a href=&quot;#bgmm&quot;&gt;Variational Bayesian Gaussian mixture&lt;/a&gt; avoids the specification of the number of components for a Gaussian mixture model.</source>
          <target state="translated">BIC基準を使用すると、混合ガウス成分の数を効率的に選択できます。理論的には、それは漸近レジームでのみコンポーネントの真の数を回復します（つまり、大量のデータが利用可能で、データがガウス分布の混合から実際に生成されたと仮定した場合）。&lt;a href=&quot;#bgmm&quot;&gt;変分ベイズ混合ガウス混合&lt;/a&gt;を使用すると、混合ガウスモデルのコンポーネント数の指定が回避されることに注意してください。</target>
        </trans-unit>
        <trans-unit id="c912c462511f5921d016fed26135f626922ccc57" translate="yes" xml:space="preserve">
          <source>The Barnes-Hut implementation only works when the target dimensionality is 3 or less. The 2D case is typical when building visualizations.</source>
          <target state="translated">Barnes-Hutの実装は、ターゲットの次元数が3以下の場合にのみ動作します。2Dの場合は、ビジュアライゼーションを構築する場合に典型的です。</target>
        </trans-unit>
        <trans-unit id="49027a83447d6307ec530f0dd81bd5ad76360faa" translate="yes" xml:space="preserve">
          <source>The Barnes-Hut t-SNE method is limited to two or three dimensional embeddings.</source>
          <target state="translated">バーンズハットのt-SNE法は、2次元または3次元の埋め込みに限定されている。</target>
        </trans-unit>
        <trans-unit id="f05f901deabcb1ee376f8697fc79932cbf746ce6" translate="yes" xml:space="preserve">
          <source>The Barnes-Hut t-SNE that has been implemented here is usually much slower than other manifold learning algorithms. The optimization is quite difficult and the computation of the gradient is \(O[d N log(N)]\), where \(d\) is the number of output dimensions and \(N\) is the number of samples. The Barnes-Hut method improves on the exact method where t-SNE complexity is \(O[d N^2]\), but has several other notable differences:</source>
          <target state="translated">ここで実装されているバーンズハットのt-SNEは、他の多様性学習アルゴリズムに比べて、通常は非常に遅い。最適化は非常に難しく、勾配の計算は、\(O[d N log(N)]\(O[d N log(N)]\(D\(D\))は出力次元の数、\(N)(N)はサンプルの数です。Barnes-Hut法は、t-SNEの複雑さが \(O[d N^2]W\)である厳密法を改良したものですが、他にもいくつかの顕著な違いがあります。</target>
        </trans-unit>
        <trans-unit id="e724094f6d5c410991717a9e2fc045d6798def45" translate="yes" xml:space="preserve">
          <source>The Birch algorithm has two parameters, the threshold and the branching factor. The branching factor limits the number of subclusters in a node and the threshold limits the distance between the entering sample and the existing subclusters.</source>
          <target state="translated">Birchアルゴリズムには,閾値と分岐係数の2つのパラメータがあります.分岐係数はノード内のサブクラスターの数を制限し,閾値は入力サンプルと既存のサブクラスターの間の距離を制限します.</target>
        </trans-unit>
        <trans-unit id="8fa70be719c3475e8e012f73acc572a21c03cdd8" translate="yes" xml:space="preserve">
          <source>The Boston house-price data has been used in many machine learning papers that address regression problems.</source>
          <target state="translated">ボストンの住宅価格データは、回帰問題を扱う多くの機械学習の論文で使用されています。</target>
        </trans-unit>
        <trans-unit id="455243540177422da87f9c3aa93de30456a2bae3" translate="yes" xml:space="preserve">
          <source>The Boston house-price data of Harrison, D. and Rubinfeld, D.L. &amp;lsquo;Hedonic prices and the demand for clean air&amp;rsquo;, J. Environ. Economics &amp;amp; Management, vol.5, 81-102, 1978. Used in Belsley, Kuh &amp;amp; Welsch, &amp;lsquo;Regression diagnostics &amp;hellip;&amp;rsquo;, Wiley, 1980. N.B. Various transformations are used in the table on pages 244-261 of the latter.</source>
          <target state="translated">ハリソンD.とルビンフェルドDLのボストンの住宅価格データ「快楽価格ときれいな空気の需要」、J。エンビロン。Economics＆Management、vol.5、81-102、1978。Belsley、Kuh＆Welsch、 'Regression diagnostics&amp;hellip;'、Wiley、1980で使用されています。NB後者の244-261ページの表では、さまざまな変換が使用されています。</target>
        </trans-unit>
        <trans-unit id="0e6c056d908bee991f648998f41ff72e8a7301c2" translate="yes" xml:space="preserve">
          <source>The CF Subclusters hold the necessary information for clustering which prevents the need to hold the entire input data in memory. This information includes:</source>
          <target state="translated">CF サブクラスターは,クラスタリングに必要な情報を保持しているので,入力データ全体をメモリに保持する必要がありません.この情報には以下のものが含まれます。</target>
        </trans-unit>
        <trans-unit id="b96c855c122e21633007587f3600e092339a5383" translate="yes" xml:space="preserve">
          <source>The Calinski-Harabaz index is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN.</source>
          <target state="translated">Calinski-Harabaz指数は、DBSCANで得られたような密度ベースのクラスターのような他のクラスターの概念よりも、一般的に凸型クラスターの方が高い。</target>
        </trans-unit>
        <trans-unit id="6cdbc3a888667e8344981c2a8742122994627087" translate="yes" xml:space="preserve">
          <source>The Complement Naive Bayes classifier described in Rennie et al.</source>
          <target state="translated">Rennieらに記載されているComplement Naive Bayes分類器。</target>
        </trans-unit>
        <trans-unit id="2b0b8b0a4dd5523203a6ee4b5d195c2e6a35c0fe" translate="yes" xml:space="preserve">
          <source>The Complement Naive Bayes classifier described in Rennie et al. (2003).</source>
          <target state="translated">Rennieら(2003)に記載されている補完ナイーブベイズ分類器。</target>
        </trans-unit>
        <trans-unit id="3398d87a3aceb1ea8375c52fcc6a67cec5c63df9" translate="yes" xml:space="preserve">
          <source>The Complement Naive Bayes classifier was designed to correct the &amp;ldquo;severe assumptions&amp;rdquo; made by the standard Multinomial Naive Bayes classifier. It is particularly suited for imbalanced data sets.</source>
          <target state="translated">Complement Naive Bayes分類器は、標準のMultinomial Naive Bayes分類器によって作成された「厳しい仮定」を修正するように設計されました。これは、不均衡なデータセットに特に適しています。</target>
        </trans-unit>
        <trans-unit id="7c176cfd9628fae51161daa03c193aeae01d49ea" translate="yes" xml:space="preserve">
          <source>The Contrastive Divergence method suggests to stop the chain after a small number of iterations, \(k\), usually even 1. This method is fast and has low variance, but the samples are far from the model distribution.</source>
          <target state="translated">Contrastive Divergence法は、小さな反復回数、\(k\)の後に連鎖を止めることを提案します。 この方法は高速で分散が低いのですが、サンプルはモデル分布から離れています。</target>
        </trans-unit>
        <trans-unit id="1d09474bd461f8756796df6e9a77749a8489b6d3" translate="yes" xml:space="preserve">
          <source>The DBSCAN algorithm is deterministic, always generating the same clusters when given the same data in the same order. However, the results can differ when data is provided in a different order. First, even though the core samples will always be assigned to the same clusters, the labels of those clusters will depend on the order in which those samples are encountered in the data. Second and more importantly, the clusters to which non-core samples are assigned can differ depending on the data order. This would happen when a non-core sample has a distance lower than &lt;code&gt;eps&lt;/code&gt; to two core samples in different clusters. By the triangular inequality, those two core samples must be more distant than &lt;code&gt;eps&lt;/code&gt; from each other, or they would be in the same cluster. The non-core sample is assigned to whichever cluster is generated first in a pass through the data, and so the results will depend on the data ordering.</source>
          <target state="translated">DBSCANアルゴリズムは確定的であり、同じデータを同じ順序で指定すると、常に同じクラスターを生成します。ただし、データが異なる順序で提供されると、結果が異なる場合があります。まず、コアサンプルは常に同じクラスターに割り当てられますが、それらのクラスターのラベルは、データ内でこれらのサンプルが出現する順序によって異なります。 2番目に重要なこととして、非コアサンプルが割り当てられるクラスターは、データの順序によって異なる場合があります。これは、非コアサンプルの距離が、異なるクラスター内の2つのコアサンプルまで &lt;code&gt;eps&lt;/code&gt; 未満の場合に発生します。三角形の不等式により、これらの2つのコアサンプルは &lt;code&gt;eps&lt;/code&gt; よりも離れている必要があります。お互いから、またはそれらは同じクラスターにあります。非コアサンプルは、データのパスで最初に生成されたクラスターに割り当てられるため、結果はデー​​タの順序に依存します。</target>
        </trans-unit>
        <trans-unit id="b5783662e991f62ab3cc63e2843b11c10be0af6e" translate="yes" xml:space="preserve">
          <source>The Davies-Boulding index is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained from DBSCAN.</source>
          <target state="translated">Davies-Boulding指数は、DBSCANから得られたような密度ベースのクラスターのような他のクラスターの概念よりも、凸型クラスターの方が一般的に高い。</target>
        </trans-unit>
        <trans-unit id="9dae6187a2e4264ce3764c47d6ced56e22112bc3" translate="yes" xml:space="preserve">
          <source>The Digit Dataset</source>
          <target state="translated">デジットデータセット</target>
        </trans-unit>
        <trans-unit id="15949e73904f01f7a29e0206a3232b388b3af43d" translate="yes" xml:space="preserve">
          <source>The Dirichlet process prior allows to define an infinite number of components and automatically selects the correct number of components: it activates a component only if it is necessary.</source>
          <target state="translated">ディリクレ処理の前処理では、無限の数のコンポーネントを定義することができ、正しい数のコンポーネントを自動的に選択することができます:必要な場合にのみコンポーネントを起動します。</target>
        </trans-unit>
        <trans-unit id="d2e49448333a2cd92baf05825e927765f1835316" translate="yes" xml:space="preserve">
          <source>The DotProduct kernel is commonly combined with exponentiation.</source>
          <target state="translated">DotProductカーネルは、一般的に指数と組み合わせて使用されています。</target>
        </trans-unit>
        <trans-unit id="842c0c72bca462c8ce3e9ff0cead821c33430a8a" translate="yes" xml:space="preserve">
          <source>The DotProduct kernel is non-stationary and can be obtained from linear regression by putting N(0, 1) priors on the coefficients of x_d (d = 1, . . . , D) and a prior of N(0, sigma_0^2) on the bias. The DotProduct kernel is invariant to a rotation of the coordinates about the origin, but not translations. It is parameterized by a parameter sigma_0^2. For sigma_0^2 =0, the kernel is called the homogeneous linear kernel, otherwise it is inhomogeneous. The kernel is given by</source>
          <target state="translated">DotProductカーネルは非定常であり、線形回帰からx_d (d=1,...,D)の係数にN(0,1)のプライアを置き、バイアスにN(0,sigma_0^2)のプライアを置くことで得ることができます。DotProduct カーネルは、原点に関する座標の回転には不変ですが、平行移動には不変です。これはパラメータ sigma_0^2 によってパラメータ化されます。sigma_0^2 =0 の場合、カーネルは均質な線形カーネルと呼ばれ、そうでない場合は不均質です。カーネルは次式で与えられます</target>
        </trans-unit>
        <trans-unit id="c219b28ff4dd0afb8b865b96896a279315780153" translate="yes" xml:space="preserve">
          <source>The Elastic Net mixing parameter, with 0 &amp;lt;= l1_ratio &amp;lt;= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1. Defaults to 0.15.</source>
          <target state="translated">Elastic Netミキシングパラメーター。0&amp;lt;= l1_ratio &amp;lt;= 1です。l1_ratio= 0はL2ペナルティに対応し、l1_ratio = 1はL1に対応します。デフォルトは0.15です。</target>
        </trans-unit>
        <trans-unit id="71317f65b5997839fb7f2d86b8c73dc4399ad254" translate="yes" xml:space="preserve">
          <source>The ElasticNet mixing parameter, with 0 &amp;lt; l1_ratio &amp;lt;= 1. For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it is an L2 penalty. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt;, the penalty is a combination of L1/L2 and L2.</source>
          <target state="translated">ElasticNet混合パラメーター、0 &amp;lt;l1_ratio &amp;lt;=1。l1_ratio= 1の場合、ペナルティはL1 / L2ペナルティです。l1_ratio = 0の場合、L2ペナルティです。以下のために &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt; 、ペナルティは、L1 / L2とL2の組み合わせです。</target>
        </trans-unit>
        <trans-unit id="f913d5ab29d41b1d0b8510f8960f88320058e481" translate="yes" xml:space="preserve">
          <source>The ElasticNet mixing parameter, with 0 &amp;lt; l1_ratio &amp;lt;= 1. For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it is an L2 penalty. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt;, the penalty is a combination of L1/L2 and L2. This parameter can be a list, in which case the different values are tested by cross-validation and the one giving the best prediction score is used. Note that a good choice of list of values for l1_ratio is often to put more values close to 1 (i.e. Lasso) and less close to 0 (i.e. Ridge), as in &lt;code&gt;[.1, .5, .7,
.9, .95, .99, 1]&lt;/code&gt;</source>
          <target state="translated">ElasticNet混合パラメーター、0 &amp;lt;l1_ratio &amp;lt;=1。l1_ratio= 1の場合、ペナルティはL1 / L2ペナルティです。 l1_ratio = 0の場合、L2ペナルティです。以下のために &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt; 、ペナルティは、L1 / L2とL2の組み合わせです。このパラメーターはリストにすることができます。その場合、さまざまな値が交差検証によってテストされ、最高の予測スコアを与えるものが使用されます。 l._ratioの値のリストの適切な選択は、 &lt;code&gt;[.1, .5, .7, .9, .95, .99, 1]&lt;/code&gt; 、。のように、多くの値を1（つまりLasso）に近づけ、0（つまりRidge）に近づけないことです。 95、.99、1]</target>
        </trans-unit>
        <trans-unit id="62ded71b403044434993092527c854b8f53e5c17" translate="yes" xml:space="preserve">
          <source>The ElasticNet mixing parameter, with &lt;code&gt;0 &amp;lt;= l1_ratio &amp;lt;= 1&lt;/code&gt;. For &lt;code&gt;l1_ratio = 0&lt;/code&gt; the penalty is an L2 penalty. &lt;code&gt;For l1_ratio = 1&lt;/code&gt; it is an L1 penalty. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt;, the penalty is a combination of L1 and L2.</source>
          <target state="translated">ElasticNetミキシングパラメーター &lt;code&gt;0 &amp;lt;= l1_ratio &amp;lt;= 1&lt;/code&gt; です。ため &lt;code&gt;l1_ratio = 0&lt;/code&gt; ペナルティL2ペナルティです。 &lt;code&gt;For l1_ratio = 1&lt;/code&gt; 場合、L1ペナルティです。以下のために &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt; 、ペナルティは、L1とL2の組み合わせです。</target>
        </trans-unit>
        <trans-unit id="706ede4c7ae02c53bd29139976a5c9ab95013fca" translate="yes" xml:space="preserve">
          <source>The ExpSineSquared kernel allows modeling periodic functions. It is parameterized by a length-scale parameter length_scale&amp;gt;0 and a periodicity parameter periodicity&amp;gt;0. Only the isotropic variant where l is a scalar is supported at the moment. The kernel given by:</source>
          <target state="translated">ExpSineSquaredカーネルでは、周期的な関数をモデル化できます。これは、長さスケールパラメーターlength_scale&amp;gt; 0および周期性パラメーターperiodity&amp;gt; 0によってパラメーター化されます。現時点では、lがスカラーである等方性バリアントのみがサポートされています。以下によって与えられるカーネル：</target>
        </trans-unit>
        <trans-unit id="bd1aecbb19e2286cfafdebb8292f1071a3eb508c" translate="yes" xml:space="preserve">
          <source>The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.</source>
          <target state="translated">F-betaスコアは、精度とリコールの加重調和平均として解釈することができ、F-betaスコアは1で最高値に達し、0で最悪値に達する。</target>
        </trans-unit>
        <trans-unit id="fcc8a075bfea5f42d0e38256200386182d33cc42" translate="yes" xml:space="preserve">
          <source>The F-beta score is the weighted harmonic mean of precision and recall, reaching its optimal value at 1 and its worst value at 0.</source>
          <target state="translated">F-βスコアは、精度とリコールの加重調和平均であり、最適値は1、最悪値は0に達します。</target>
        </trans-unit>
        <trans-unit id="553773b1d0f7903e424f857dbba13f4b2343a5a1" translate="yes" xml:space="preserve">
          <source>The F-beta score weights recall more than precision by a factor of &lt;code&gt;beta&lt;/code&gt;. &lt;code&gt;beta == 1.0&lt;/code&gt; means recall and precision are equally important.</source>
          <target state="translated">Fベータスコアの重みは、 &lt;code&gt;beta&lt;/code&gt; 係数により精度以上に再現されます。 &lt;code&gt;beta == 1.0&lt;/code&gt; は、再現率と精度が等しく重要であることを意味します。</target>
        </trans-unit>
        <trans-unit id="3f53f44feaa854c0fdf5365ef5e43bc7fa440b01" translate="yes" xml:space="preserve">
          <source>The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:</source>
          <target state="translated">F1スコアは、精度とリコールの加重平均として解釈することができ、F1スコアの最高値は1で、最悪値は0である。 F1スコアに対する精度とリコールの相対的な寄与は等しい。F1スコアの式は次のようになります。</target>
        </trans-unit>
        <trans-unit id="518509d08bd98ece386f11a6583f35b4f559fdac" translate="yes" xml:space="preserve">
          <source>The Figure below shows four one-way and one two-way partial dependence plots for the California housing dataset:</source>
          <target state="translated">下の図は、カリフォルニア州の住宅データセットについて、4つの一方向と1つの二方向の部分依存性プロットを示しています。</target>
        </trans-unit>
        <trans-unit id="2280ec0b9d5ba5eef024fd7f6041defdab8c9d48" translate="yes" xml:space="preserve">
          <source>The Figure below shows the contours of the different regularization terms in the parameter space when \(R(w) = 1\).</source>
          <target state="translated">下の図は、\(R(w)=1\)の時のパラメータ空間の異なる正則化項の輪郭を示しています。</target>
        </trans-unit>
        <trans-unit id="af21e4c771514d1aca6cd1c14dcff1ce67a0e477" translate="yes" xml:space="preserve">
          <source>The Fowlkes-Mallows index (&lt;a href=&quot;generated/sklearn.metrics.fowlkes_mallows_score#sklearn.metrics.fowlkes_mallows_score&quot;&gt;&lt;code&gt;sklearn.metrics.fowlkes_mallows_score&lt;/code&gt;&lt;/a&gt;) can be used when the ground truth class assignments of the samples is known. The Fowlkes-Mallows score FMI is defined as the geometric mean of the pairwise precision and recall:</source>
          <target state="translated">Fowlkes-Mallowsインデックス（&lt;a href=&quot;generated/sklearn.metrics.fowlkes_mallows_score#sklearn.metrics.fowlkes_mallows_score&quot;&gt; &lt;code&gt;sklearn.metrics.fowlkes_mallows_score&lt;/code&gt; &lt;/a&gt;）は、サンプルのグラウンドトゥルースクラスの割り当てがわかっている場合に使用できます。 Fowlkes-MallowsスコアFMIは、ペアワイズ精度と再現率の幾何平均として定義されます。</target>
        </trans-unit>
        <trans-unit id="bfdc8ed29be1f90cbfb4dc9576ce17a736f93294" translate="yes" xml:space="preserve">
          <source>The Fowlkes-Mallows index (FMI) is defined as the geometric mean between of the precision and recall:</source>
          <target state="translated">Fowlkes-Mallows指数(FMI)は、精度とリコールの幾何学的平均値として定義されています。</target>
        </trans-unit>
        <trans-unit id="90c804ef552ac504772d87a0c90e2454ea50931f" translate="yes" xml:space="preserve">
          <source>The GP prior mean is assumed to be zero. The prior&amp;rsquo;s covariance is specified by a passing a &lt;a href=&quot;#gp-kernels&quot;&gt;kernel&lt;/a&gt; object. The hyperparameters of the kernel are optimized during fitting of GaussianProcessRegressor by maximizing the log-marginal-likelihood (LML) based on the passed &lt;code&gt;optimizer&lt;/code&gt;. As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying &lt;code&gt;n_restarts_optimizer&lt;/code&gt;. The first run is always conducted starting from the initial hyperparameter values of the kernel; subsequent runs are conducted from hyperparameter values that have been chosen randomly from the range of allowed values. If the initial hyperparameters should be kept fixed, &lt;code&gt;None&lt;/code&gt; can be passed as optimizer.</source>
          <target state="translated">GP事前平均はゼロと見なされます。事前分布の共分散は、&lt;a href=&quot;#gp-kernels&quot;&gt;カーネル&lt;/a&gt;オブジェクトを渡すことによって指定されます。カーネルのハイパーパラメーターは、GaussianProcessRegressorのフィッティング中に、渡さ &lt;code&gt;optimizer&lt;/code&gt; 基づいてlog-marginal-likelihood（LML）を最大化することにより最適化されます。 LMLには複数のローカルオプティマがある可能性があるため、 &lt;code&gt;n_restarts_optimizer&lt;/code&gt; を指定することにより、オプティマイザーを繰り返し起動できます。最初の実行は常に、カーネルの初期ハイパーパラメーター値から開始されます。後続の実行は、許容値の範囲からランダムに選択されたハイパーパラメーター値から行われます。初期ハイパーパラメータを固定したままにする必要がある場合は、 &lt;code&gt;None&lt;/code&gt; をオプティマイザとして渡すことができます。</target>
        </trans-unit>
        <trans-unit id="01a974069deaad9f4a696951139fa6f180c4610d" translate="yes" xml:space="preserve">
          <source>The HLLE algorithm comprises three stages:</source>
          <target state="translated">HLLEアルゴリズムは3つの段階から構成されています。</target>
        </trans-unit>
        <trans-unit id="eaec5120030b979edc1dcb8e844874e0db8c76f1" translate="yes" xml:space="preserve">
          <source>The Hamming loss is the fraction of labels that are incorrectly predicted.</source>
          <target state="translated">ハミング損失とは、誤って予測されたラベルの割合のことです。</target>
        </trans-unit>
        <trans-unit id="ee89b1587a7c7b8d189cccf628a41e55c5d7ebe4" translate="yes" xml:space="preserve">
          <source>The Hamming loss is upperbounded by the subset zero-one loss. When normalized over samples, the Hamming loss is always between 0 and 1.</source>
          <target state="translated">ハミング損失は、0-1損失のサブセットによって上限が設定されています。サンプルで正規化すると、ハミング損失は常に0と1の間になります。</target>
        </trans-unit>
        <trans-unit id="2f575d5541e123fdf35ce0dd5c6fb4373f14dde3" translate="yes" xml:space="preserve">
          <source>The Huber Regressor optimizes the squared loss for the samples where &lt;code&gt;|(y - X'w) / sigma| &amp;lt; epsilon&lt;/code&gt; and the absolute loss for the samples where &lt;code&gt;|(y - X'w) / sigma| &amp;gt; epsilon&lt;/code&gt;, where w and sigma are parameters to be optimized. The parameter sigma makes sure that if y is scaled up or down by a certain factor, one does not need to rescale epsilon to achieve the same robustness. Note that this does not take into account the fact that the different features of X may be of different scales.</source>
          <target state="translated">Huber Regressorは、 &lt;code&gt;|(y - X'w) / sigma| &amp;lt; epsilon&lt;/code&gt; であるサンプルの損失の2乗を最適化します。&amp;lt;イプシロンおよびサンプルの絶対損失 &lt;code&gt;|(y - X'w) / sigma| &amp;gt; epsilon&lt;/code&gt; 、ここでwとsigmaは最適化されるパラメーターです。パラメータsigmaは、yが特定の係数で拡大または縮小された場合、同じロバスト性を実現するためにイプシロンを再スケーリングする必要がないことを確認します。これは、Xのさまざまな機能のスケールが異なる可能性があるという事実を考慮していないことに注意してください。</target>
        </trans-unit>
        <trans-unit id="a428acf86562c566ebfc0a4a1d3e42c700a1a75f" translate="yes" xml:space="preserve">
          <source>The Huber and epsilon-insensitive loss functions can be used for robust regression. The width of the insensitive region has to be specified via the parameter &lt;code&gt;epsilon&lt;/code&gt;. This parameter depends on the scale of the target variables.</source>
          <target state="translated">Huberおよびイプシロンに依存しない損失関数は、ロバスト回帰に使用できます。不感領域の幅は、パラメータ &lt;code&gt;epsilon&lt;/code&gt; で指定する必要があります。このパラメーターは、ターゲット変数のスケールに依存します。</target>
        </trans-unit>
        <trans-unit id="0a2150cee6da11610a14f68e745e5d0639a39459" translate="yes" xml:space="preserve">
          <source>The Iris Dataset</source>
          <target state="translated">アイリスデータセット</target>
        </trans-unit>
        <trans-unit id="7c0ef25371cb6aecb434ff0290ad88905a1c2367" translate="yes" xml:space="preserve">
          <source>The Iris dataset represents 3 kind of Iris flowers (Setosa, Versicolour and Virginica) with 4 attributes: sepal length, sepal width, petal length and petal width.</source>
          <target state="translated">アイリスのデータセットは、3種類のアイリス(セトサ、バーシカラー、ヴァージニカ)の花を、萼の長さ、萼の幅、花びらの長さ、花びらの幅の4つの属性で表現しています。</target>
        </trans-unit>
        <trans-unit id="3877561d1fbee6aff8708ef4ec5fcabe6115833e" translate="yes" xml:space="preserve">
          <source>The IsolationForest &amp;lsquo;isolates&amp;rsquo; observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.</source>
          <target state="translated">IsolationForestは、ランダムに機能を選択し、選択した機能の最大値と最小値の間の分割値をランダムに選択することにより、観測を「分離」します。</target>
        </trans-unit>
        <trans-unit id="9e47d533732129c7b308a9673031913c04e17afd" translate="yes" xml:space="preserve">
          <source>The Isomap algorithm comprises three stages:</source>
          <target state="translated">Isomapアルゴリズムは3つの段階から構成されています。</target>
        </trans-unit>
        <trans-unit id="43f0627e56441244b0d44b83d4abbf8510d9eac0" translate="yes" xml:space="preserve">
          <source>The Jaccard index [1], or Jaccard similarity coefficient, defined as the size of the intersection divided by the size of the union of two label sets, is used to compare set of predicted labels for a sample to the corresponding set of labels in &lt;code&gt;y_true&lt;/code&gt;.</source>
          <target state="translated">Jaccardインデックス[1]、またはJaccard類似度係数は、交差のサイズを2つのラベルセットの和集合のサイズで割ったものとして定義され、サンプルの予測ラベルのセットを &lt;code&gt;y_true&lt;/code&gt; の対応するラベルのセットと比較するために使用されます。</target>
        </trans-unit>
        <trans-unit id="54b0fb2f21ad6f516f16dd3562af3d3b867660fe" translate="yes" xml:space="preserve">
          <source>The Jaccard similarity coefficient of the \(i\)-th samples, with a ground truth label set \(y_i\) and predicted label set \(\hat{y}_i\), is defined as</source>
          <target state="translated">第1のサンプルのJaccard類似度係数は、真実のラベルと予測されたラベルのセットを用いて、次のように定義されます。</target>
        </trans-unit>
        <trans-unit id="1b6ab133b67b3354e1eaee98fb0609deafb4a4e0" translate="yes" xml:space="preserve">
          <source>The Johnson-Lindenstrauss bound for embedding with random projections</source>
          <target state="translated">ランダム射影を用いた埋め込みのジョンソン-リンデンストラウス境界</target>
        </trans-unit>
        <trans-unit id="2e420c9d276fc6531e2d5a1fd3367ac4465f247c" translate="yes" xml:space="preserve">
          <source>The KDD Cup &amp;lsquo;99 dataset was created by processing the tcpdump portions of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset, created by MIT Lincoln Lab [1]. The artificial data (described on the &lt;a href=&quot;http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html&quot;&gt;dataset&amp;rsquo;s homepage&lt;/a&gt;) was generated using a closed network and hand-injected attacks to produce a large number of different types of attack with normal activity in the background. As the initial goal was to produce a large training set for supervised learning algorithms, there is a large proportion (80.1%) of abnormal data which is unrealistic in real world, and inappropriate for unsupervised anomaly detection which aims at detecting &amp;lsquo;abnormal&amp;rsquo; data, ie</source>
          <target state="translated">KDD Cup '99データセットは、MITリンカーンラボ[1]によって作成された1998 DARPA侵入検知システム（IDS）評価データセットのtcpdump部分を処理することによって作成されました。人工的なデータ（&lt;a href=&quot;http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html&quot;&gt;データセットのホームページで&lt;/a&gt;説明）は、クローズドネットワークと手動注入攻撃を使用して生成され、バックグラウンドで通常のアクティビティを伴うさまざまな種類の攻撃を多数生成しました。最初の目標は教師あり学習アルゴリズム用の大規模なトレーニングセットを作成することであったため、現実世界では非現実的であり、「異常な」データの検出を目的とする教師なしの異常検出には不適切な異常データの割合が大きく（80.1％）、すなわち</target>
        </trans-unit>
        <trans-unit id="42488af2a88df53e7b3301c7d5a125afdc9cd2ce" translate="yes" xml:space="preserve">
          <source>The Kullback-Leibler (KL) divergence of the joint probabilities in the original space and the embedded space will be minimized by gradient descent. Note that the KL divergence is not convex, i.e. multiple restarts with different initializations will end up in local minima of the KL divergence. Hence, it is sometimes useful to try different seeds and select the embedding with the lowest KL divergence.</source>
          <target state="translated">元の空間と埋め込まれた空間の共同確率の Kullback-Leibler (KL)発散は勾配降下法によって最小化される.KL発散は凸ではないことに注意してください。つまり、異なる初期化で複数回の再起動を行うと、KL発散の局所的な最小値になってしまいます。そのため,異なるシードを試してみて,KL発散が最も小さい埋め込みを選択することが有用な場合もあります.</target>
        </trans-unit>
        <trans-unit id="5c5e824f9a2e5664a81c2abf8d8de696419cc018" translate="yes" xml:space="preserve">
          <source>The LARS model can be used using estimator &lt;a href=&quot;generated/sklearn.linear_model.lars#sklearn.linear_model.Lars&quot;&gt;&lt;code&gt;Lars&lt;/code&gt;&lt;/a&gt;, or its low-level implementation &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt;&lt;code&gt;lars_path&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">LARSモデルは、推定器&lt;a href=&quot;generated/sklearn.linear_model.lars#sklearn.linear_model.Lars&quot;&gt; &lt;code&gt;Lars&lt;/code&gt; &lt;/a&gt;またはその低レベルの実装&lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt; &lt;code&gt;lars_path&lt;/code&gt; を&lt;/a&gt;使用して使用できます。</target>
        </trans-unit>
        <trans-unit id="5604e6549ba6e538c77a361942c228433b92a65e" translate="yes" xml:space="preserve">
          <source>The LTSA algorithm comprises three stages:</source>
          <target state="translated">LTSAアルゴリズムは3つのステージから構成されています。</target>
        </trans-unit>
        <trans-unit id="5921a49032d4468242b223b9e118c35472eff0fa" translate="yes" xml:space="preserve">
          <source>The Lars algorithm provides the full path of the coefficients along the regularization parameter almost for free, thus a common operation consist of retrieving the path with function &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt;&lt;code&gt;lars_path&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">Larsアルゴリズムは、ほぼ無料で正則化パラメーターに沿った係数の完全なパスを提供します。したがって、一般的な操作は、関数&lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt; &lt;code&gt;lars_path&lt;/code&gt; を使用し&lt;/a&gt;てパスを取得することで構成されます</target>
        </trans-unit>
        <trans-unit id="0f5aa687d48724082c940128def6bdb3f6066405" translate="yes" xml:space="preserve">
          <source>The Lasso optimization function varies for mono and multi-outputs.</source>
          <target state="translated">ラッソ最適化機能は、モノ出力とマルチ出力で異なります。</target>
        </trans-unit>
        <trans-unit id="3c1b2c3b1551762e6f1da2d8a29e9acf6404c123" translate="yes" xml:space="preserve">
          <source>The Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse underlying graphs, where number of features is greater than number of samples. Elsewhere prefer cd which is more numerically stable.</source>
          <target state="translated">使用するLassoソルバー:座標降下法またはLARS。特徴量の数がサンプル数よりも多い、非常に疎なグラフにはLARSを使用します。その他の場合は、数値的に安定しているcdを使用します。</target>
        </trans-unit>
        <trans-unit id="2de8b3228aef7c3b031e79cb56d858268d66f64e" translate="yes" xml:space="preserve">
          <source>The Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse underlying graphs, where p &amp;gt; n. Elsewhere prefer cd which is more numerically stable.</source>
          <target state="translated">使用する投げ縄ソルバー：座標降下またはLARS。LARSは、p&amp;gt; nの非常にスパースな基礎グラフに使用します。他の場所では、数値的に安定したcdを好みます。</target>
        </trans-unit>
        <trans-unit id="6c4c48b93f6a66f7991382ba54fcfe5fb18a6ebb" translate="yes" xml:space="preserve">
          <source>The Ledoit-Wolf estimator of the covariance matrix can be computed on a sample with the &lt;a href=&quot;generated/sklearn.covariance.ledoit_wolf#sklearn.covariance.ledoit_wolf&quot;&gt;&lt;code&gt;ledoit_wolf&lt;/code&gt;&lt;/a&gt; function of the &lt;code&gt;sklearn.covariance&lt;/code&gt; package, or it can be otherwise obtained by fitting a &lt;a href=&quot;generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf&quot;&gt;&lt;code&gt;LedoitWolf&lt;/code&gt;&lt;/a&gt; object to the same sample.</source>
          <target state="translated">共分散行列のLedoit-Wolf推定量は、 &lt;code&gt;sklearn.covariance&lt;/code&gt; パッケージの&lt;a href=&quot;generated/sklearn.covariance.ledoit_wolf#sklearn.covariance.ledoit_wolf&quot;&gt; &lt;code&gt;ledoit_wolf&lt;/code&gt; &lt;/a&gt;関数を使用してサンプルで計算できます。または、同じサンプルに&lt;a href=&quot;generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf&quot;&gt; &lt;code&gt;LedoitWolf&lt;/code&gt; &lt;/a&gt;オブジェクトを当てはめることで取得できます。</target>
        </trans-unit>
        <trans-unit id="aec2f6a4d0fb4e3ecc2ba56ea33e122b851a535b" translate="yes" xml:space="preserve">
          <source>The Linnerud dataset constains two small dataset:</source>
          <target state="translated">Linnerudデータセットは、2つの小さなデータセットを保持しています。</target>
        </trans-unit>
        <trans-unit id="0460202c91e1eee28482597354a8366af4e5eb02" translate="yes" xml:space="preserve">
          <source>The Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors. This example shows how to use LOF for novelty detection. Note that when LOF is used for novelty detection you MUST not use predict, decision_function and score_samples on the training set as this would lead to wrong results. You must only use these methods on new unseen data (which are not in the training set). See &lt;a href=&quot;../../modules/outlier_detection#outlier-detection&quot;&gt;User Guide&lt;/a&gt;: for details on the difference between outlier detection and novelty detection and how to use LOF for outlier detection.</source>
          <target state="translated">Local Outlier Factor（LOF）アルゴリズムは、監視されていない異常検出方法であり、特定のデータポイントの近傍に対する局所密度偏差を計算します。隣接するサンプルよりも密度が大幅に低いサンプルを異常値と見なします。この例は、LOFをノベルティ検出に使用する方法を示しています。 LOFが目新しさの検出に使用されている場合、誤った結果につながる可能性があるため、トレーニングセットでpredict、decision_function、score_samplesを使用しないでください。これらのメソッドは、新しく表示されないデータ（トレーニングセットに含まれていない）に対してのみ使用する必要があります。参照してください。&lt;a href=&quot;../../modules/outlier_detection#outlier-detection&quot;&gt;ユーザーガイド&lt;/a&gt;：外れ値検出とノベルティの検出方法と、外れ値の検出のためのLOFを使用する方法の違いの詳細については。</target>
        </trans-unit>
        <trans-unit id="d0e3831e71a419f2c6e3df80ad462189c4208703" translate="yes" xml:space="preserve">
          <source>The Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors. This example shows how to use LOF for outlier detection which is the default use case of this estimator in scikit-learn. Note that when LOF is used for outlier detection it has no predict, decision_function and score_samples methods. See &lt;a href=&quot;../../modules/outlier_detection#outlier-detection&quot;&gt;User Guide&lt;/a&gt;: for details on the difference between outlier detection and novelty detection and how to use LOF for novelty detection.</source>
          <target state="translated">Local Outlier Factor（LOF）アルゴリズムは、監視されていない異常検出方法であり、特定のデータポイントの近傍に対する局所密度偏差を計算します。隣接するサンプルよりも密度が大幅に低いサンプルを異常値と見なします。この例は、外れ値の検出にLOFを使用する方法を示しています。これは、scikit-learnでのこの推定器のデフォルトの使用例です。LOFが外れ値の検出に使用される場合、LODにはpredict、decision_function、score_samplesメソッドがないことに注意してください。参照してください。&lt;a href=&quot;../../modules/outlier_detection#outlier-detection&quot;&gt;ユーザーガイド&lt;/a&gt;：外れ値検出とノベルティの検出方法と、新規性検出のためのLOFを使用する方法の違いの詳細については。</target>
        </trans-unit>
        <trans-unit id="e334ea68d0fb9bc258e189ffac7524671f8600d5" translate="yes" xml:space="preserve">
          <source>The MLLE algorithm comprises three stages:</source>
          <target state="translated">MLLEのアルゴリズムは3つの段階から構成されています。</target>
        </trans-unit>
        <trans-unit id="514baa8efb372e193dd4209109d33c4d6c755f88" translate="yes" xml:space="preserve">
          <source>The Matplotlib Figure object.</source>
          <target state="translated">Matplotlib 図形オブジェクト。</target>
        </trans-unit>
        <trans-unit id="5adf2471ff49dbf27acc1a8b4862b530e5b52366" translate="yes" xml:space="preserve">
          <source>The Matthews correlation coefficient (+1 represents a perfect prediction, 0 an average random prediction and -1 and inverse prediction).</source>
          <target state="translated">マシューズ相関係数(+1は完全予測、0は平均ランダム予測、-1と逆予測を表す)。</target>
        </trans-unit>
        <trans-unit id="21efcbf7188af02e8a17818fd892c631c3cc436c" translate="yes" xml:space="preserve">
          <source>The Matthews correlation coefficient is used in machine learning as a measure of the quality of binary and multiclass classifications. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient value between -1 and +1. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction. The statistic is also known as the phi coefficient. [source: Wikipedia]</source>
          <target state="translated">マシューズ相関係数は,バイナリおよびマルチクラス分類の品質の尺度として機械学習で使用される.これは,真正と偽正と偽負を考慮に入れており,一般に,クラスのサイズが非常に異なる場合でも使用できるバランスのとれた尺度とみなされている.MCCは,本質的には-1と+1の間の相関係数の値である.係数が+1の場合は完全な予測、0の場合は平均的なランダム予測、-1の場合は逆予測を表します。この統計量はファイ係数としても知られている。出典:ウィキペディア</target>
        </trans-unit>
        <trans-unit id="d3bca24cc7fb644916020d52b90a1ddb7136c5be" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant covariance estimator is to be applied on Gaussian-distributed data, but could still be relevant on data drawn from a unimodal, symmetric distribution. It is not meant to be used with multi-modal data (the algorithm used to fit a MinCovDet object is likely to fail in such a case). One should consider projection pursuit methods to deal with multi-modal datasets.</source>
          <target state="translated">最小共分散決定子共分散推定量は、ガウス分布のデータに適用されますが、単モーダルで対称な分布から引き出されたデータにも適用できます。これは、マルチモーダルデータでの使用を意図したものではありません(MinCovDetオブジェクトを適合させるために使用されるアルゴリズムは、このような場合に失敗する可能性が高いです)。マルチモーダルデータを扱うためには,射影追跡法を検討する必要があります.</target>
        </trans-unit>
        <trans-unit id="451133ac20cdcd95892f17968e1c348a4b0f4b8f" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator (MCD) has been introduced by P.J.Rousseuw in &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="translated">最小共分散行列式推定量（MCD）は、PJRousseuwによって&lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;で導入されました。</target>
        </trans-unit>
        <trans-unit id="d811e159b509e69af06fa8b1a6b421d8a9ea40ca" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator (MCD) has been introduced by P.J.Rousseuw in [1].</source>
          <target state="translated">最小共分散決定量推定器(MCD)は、P.J.Rousseuwによって[1]で紹介されています。</target>
        </trans-unit>
        <trans-unit id="92f2af418b12a7a58727045874e736340d33efbd" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator is a robust estimator of a data set&amp;rsquo;s covariance introduced by P.J. Rousseeuw in &lt;a href=&quot;#id11&quot; id=&quot;id9&quot;&gt;[3]&lt;/a&gt;. The idea is to find a given proportion (h) of &amp;ldquo;good&amp;rdquo; observations which are not outliers and compute their empirical covariance matrix. This empirical covariance matrix is then rescaled to compensate the performed selection of observations (&amp;ldquo;consistency step&amp;rdquo;). Having computed the Minimum Covariance Determinant estimator, one can give weights to observations according to their Mahalanobis distance, leading to a reweighted estimate of the covariance matrix of the data set (&amp;ldquo;reweighting step&amp;rdquo;).</source>
          <target state="translated">最小共分散行列式推定量は、PJ Rousseeuwによって導入されたデータセットの共分散のロバスト推定量です&lt;a href=&quot;#id11&quot; id=&quot;id9&quot;&gt;[3]&lt;/a&gt;。アイデアは、外れ値ではない「良い」観測値の所定の割合（h）を見つけ、その経験的共分散行列を計算することです。次に、この経験的共分散行列は、実行された観測の選択を補正するために再スケーリングされます（「整合性ステップ」）。最小共分散行列式推定量を計算したら、マハラノビス距離に従って観測値に重みを付けることができ、データセットの共分散行列の再重み付けされた推定につながります（「再重み付けステップ」）。</target>
        </trans-unit>
        <trans-unit id="26a5ec87ffc45e2d236a64ca891cc8c307910b23" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator is a robust, high-breakdown point (i.e. it can be used to estimate the covariance matrix of highly contaminated datasets, up to \(\frac{n_\text{samples} - n_\text{features}-1}{2}\) outliers) estimator of covariance. The idea is to find \(\frac{n_\text{samples} + n_\text{features}+1}{2}\) observations whose empirical covariance has the smallest determinant, yielding a &amp;ldquo;pure&amp;rdquo; subset of observations from which to compute standards estimates of location and covariance. After a correction step aiming at compensating the fact that the estimates were learned from only a portion of the initial data, we end up with robust estimates of the data set location and covariance.</source>
          <target state="translated">最小共分散行列式推定量は、堅牢で高分解のポイントです（つまり、非常に汚染されたデータセットの共分散行列を\（\ frac {n_ \ text {samples}-n_ \ text {features}-まで推定するために使用できます） 1} {2} \）外れ値）共分散の推定量。アイデアは、経験的共分散が最小の行列式を持つ\（\ frac {n_ \ text {samples} + n_ \ text {features} +1} {2} \）観測を見つけ、観測の「純粋な」サブセットを生成することです。場所と共分散の標準推定値を計算します。推定値が初期データの一部のみから学習されたという事実を補償することを目的とした修正ステップの後、データセットの場所と共分散のロバストな推定値が得られます。</target>
        </trans-unit>
        <trans-unit id="868c6b821132444289ad5893f70525225594997c" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator is a robust, high-breakdown point (i.e. it can be used to estimate the covariance matrix of highly contaminated datasets, up to \(\frac{n_\text{samples}-n_\text{features}-1}{2}\) outliers) estimator of covariance. The idea is to find \(\frac{n_\text{samples}+n_\text{features}+1}{2}\) observations whose empirical covariance has the smallest determinant, yielding a &amp;ldquo;pure&amp;rdquo; subset of observations from which to compute standards estimates of location and covariance.</source>
          <target state="translated">最小共分散行列式推定量は、堅牢で高分解のポイントです（つまり、非常に汚染されたデータセットの共分散行列を\（\ frac {n_ \ text {samples} -n_ \ text {features}-まで推定するために使用できます） 1} {2} \）外れ値）共分散の推定量。アイデアは、経験的共分散が最小の行列式を持つ\（\ frac {n_ \ text {samples} + n_ \ text {features} +1} {2} \）観測を見つけて、観測の「純粋な」サブセットを生成することです。場所と共分散の標準推定値を計算します。</target>
        </trans-unit>
        <trans-unit id="b2ce758bd900de6631654a32b9b8962161ffb45f" translate="yes" xml:space="preserve">
          <source>The Mutual Information is a measure of the similarity between two labels of the same data. Where \(|U_i|\) is the number of the samples in cluster \(U_i\) and \(|V_j|\) is the number of the samples in cluster \(V_j\), the Mutual Information between clusterings \(U\) and \(V\) is given as:</source>
          <target state="translated">相互情報とは、同じデータのラベル間の類似度を測定したものです。ここで、\(|U_i|\)はクラスター内のサンプル数、\(|V_j|\)はクラスター内のサンプル数であり、クラスター間の相互情報は次のように与えられます。</target>
        </trans-unit>
        <trans-unit id="4607a569600ef039f2071e80730b55efccd8dd8d" translate="yes" xml:space="preserve">
          <source>The Nystroem method, as implemented in &lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; is a general method for low-rank approximations of kernels. It achieves this by essentially subsampling the data on which the kernel is evaluated. By default &lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; uses the &lt;code&gt;rbf&lt;/code&gt; kernel, but it can use any kernel function or a precomputed kernel matrix. The number of samples used - which is also the dimensionality of the features computed - is given by the parameter &lt;code&gt;n_components&lt;/code&gt;.</source>
          <target state="translated">Nystroemで実装されている&lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt;メソッドは、カーネルの低ランク近似の一般的なメソッドです。これは、カーネルが評価されるデータを本質的にサブサンプリングすることによってこれを実現します。デフォルトでは、&lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt;は &lt;code&gt;rbf&lt;/code&gt; カーネルを使用しますが、任意のカーネル関数または事前計算されたカーネルマトリックスを使用できます。使用されるサンプルの数-計算された特徴の次元でもある-は、パラメーター &lt;code&gt;n_components&lt;/code&gt; によって指定されます。</target>
        </trans-unit>
        <trans-unit id="ad47de32445041d6ef99f77621d867a03cd9751f" translate="yes" xml:space="preserve">
          <source>The OAS estimator of the covariance matrix can be computed on a sample with the &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.oas&quot;&gt;&lt;code&gt;oas&lt;/code&gt;&lt;/a&gt; function of the &lt;code&gt;sklearn.covariance&lt;/code&gt; package, or it can be otherwise obtained by fitting an &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt;&lt;code&gt;OAS&lt;/code&gt;&lt;/a&gt; object to the same sample.</source>
          <target state="translated">共分散行列のOAS推定量は、 &lt;code&gt;sklearn.covariance&lt;/code&gt; パッケージの&lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.oas&quot;&gt; &lt;code&gt;oas&lt;/code&gt; &lt;/a&gt;関数を使用してサンプルで計算できます。または、同じ方法で&lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt; &lt;code&gt;OAS&lt;/code&gt; &lt;/a&gt;オブジェクトを近似することで取得できます。</target>
        </trans-unit>
        <trans-unit id="6f45a4e82bc4c1b489f40318ecca4028acd0f46e" translate="yes" xml:space="preserve">
          <source>The One-Class SVM has been introduced by Sch&amp;ouml;lkopf et al. for that purpose and implemented in the &lt;a href=&quot;svm#svm&quot;&gt;Support Vector Machines&lt;/a&gt; module in the &lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt;&lt;code&gt;svm.OneClassSVM&lt;/code&gt;&lt;/a&gt; object. It requires the choice of a kernel and a scalar parameter to define a frontier. The RBF kernel is usually chosen although there exists no exact formula or algorithm to set its bandwidth parameter. This is the default in the scikit-learn implementation. The \(\nu\) parameter, also known as the margin of the One-Class SVM, corresponds to the probability of finding a new, but regular, observation outside the frontier.</source>
          <target state="translated">One-Class SVMはSch&amp;ouml;lkopfらによって導入されました。その目的のために、&lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt; &lt;code&gt;svm.OneClassSVM&lt;/code&gt; &lt;/a&gt;オブジェクトの&lt;a href=&quot;svm#svm&quot;&gt;サポートベクターマシン&lt;/a&gt;モジュールに実装されています。フロンティアを定義するには、カーネルとスカラーパラメーターの選択が必要です。帯域幅パラメータを設定するための正確な公式やアルゴリズムは存在しませんが、RBFカーネルが通常選択されます。これは、scikit-learn実装のデフォルトです。 \（\ nu \）パラメータは、One-Class SVMのマージンとも呼ばれ、境界線の外側にある新しい、ただし規則的な観測値を見つける確率に対応しています。</target>
        </trans-unit>
        <trans-unit id="ce00ae4f245475e7026810e89464783863843edd" translate="yes" xml:space="preserve">
          <source>The PCA does an unsupervised dimensionality reduction, while the logistic regression does the prediction.</source>
          <target state="translated">PCAは教師なしの次元削減を行い、ロジスティック回帰は予測を行う。</target>
        </trans-unit>
        <trans-unit id="33c279bdcb922f55f225b113a682e1a5cf28cc6d" translate="yes" xml:space="preserve">
          <source>The RBF kernel is a stationary kernel. It is also known as the &amp;ldquo;squared exponential&amp;rdquo; kernel. It is parameterized by a length-scale parameter length_scale&amp;gt;0, which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs X (anisotropic variant of the kernel). The kernel is given by:</source>
          <target state="translated">RBFカーネルは固定カーネルです。「二乗指数」カーネルとしても知られています。これは、長さスケールパラメーターlength_scale&amp;gt; 0によってパラメーター化されます。これは、スカラー（カーネルの等方性バリアント）または入力Xと同じ次元数のベクトル（カーネルの異方性バリアント）のいずれかです。カーネルは以下によって与えられます：</target>
        </trans-unit>
        <trans-unit id="f47997c4010f2e20accd04690970d6547a71bfc2" translate="yes" xml:space="preserve">
          <source>The RBF kernel will produce a fully connected graph which is represented in memory by a dense matrix. This matrix may be very large and combined with the cost of performing a full matrix multiplication calculation for each iteration of the algorithm can lead to prohibitively long running times. On the other hand, the KNN kernel will produce a much more memory-friendly sparse matrix which can drastically reduce running times.</source>
          <target state="translated">RBF カーネルは,メモリ上で密な行列で表現される完全に接続されたグラフを生成します.この行列は非常に大きくなる可能性があり,アルゴリズムの各繰り返しに対して完全な行列の乗算計算を実行するコストと相まって,法外に長い実行時間をもたらす可能性があります.一方,KNN カーネルは,よりメモリに優しい疎な行列を生成し,実行時間を大幅に短縮することができます.</target>
        </trans-unit>
        <trans-unit id="512de356729b7d551b43d5d76bd8681dea941e19" translate="yes" xml:space="preserve">
          <source>The RBM tries to maximize the likelihood of the data using a particular graphical model. The parameter learning algorithm used (&lt;a href=&quot;#sml&quot;&gt;Stochastic Maximum Likelihood&lt;/a&gt;) prevents the representations from straying far from the input data, which makes them capture interesting regularities, but makes the model less useful for small datasets, and usually not useful for density estimation.</source>
          <target state="translated">RBMは、特定のグラフィカルモデルを使用してデータの可能性を最大化しようとします。使用されているパラメーター学習アルゴリズム（&lt;a href=&quot;#sml&quot;&gt;Stochastic Maximum Likelihood&lt;/a&gt;）は、表現が入力データから遠く離れないようにします。これにより、興味深い規則性を捉えることができますが、モデルが小規模なデータセットにはあまり役に立たなくなり、通常は密度推定に役立ちません。</target>
        </trans-unit>
        <trans-unit id="95d48012cfd5cdfda48a80ad45de8292db32665c" translate="yes" xml:space="preserve">
          <source>The R^2 score or ndarray of scores if &amp;lsquo;multioutput&amp;rsquo; is &amp;lsquo;raw_values&amp;rsquo;.</source>
          <target state="translated">'multioutput'が 'raw_values'の場合​​、R ^ 2スコアまたはスコアのndarray。</target>
        </trans-unit>
        <trans-unit id="57cf85e67b74d4b36ce4e00a60cc2afb37409d42" translate="yes" xml:space="preserve">
          <source>The Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.</source>
          <target state="translated">Rand Indexは、すべてのサンプルのペアを考慮し、予測されたクラスタリングと真のクラスタリングで同じクラスタまたは異なるクラスタに割り当てられたペアをカウントすることで、2つのクラスタリング間の類似性の尺度を計算します。</target>
        </trans-unit>
        <trans-unit id="21d3f8892ca41bf337636e90759152be22d9788c" translate="yes" xml:space="preserve">
          <source>The RandomTreesEmbedding, from the &lt;a href=&quot;../../modules/classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; module, is not technically a manifold embedding method, as it learn a high-dimensional representation on which we apply a dimensionality reduction method. However, it is often useful to cast a dataset into a representation in which the classes are linearly-separable.</source>
          <target state="translated">&lt;a href=&quot;../../modules/classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt;モジュールのRandomTreesEmbeddingは、次元削減法を適用する高次元表現を学習するため、技術的には多様体埋め込み法ではありません。ただし、データセットをクラスが線形分離可能な表現にキャストすると便利な場合があります。</target>
        </trans-unit>
        <trans-unit id="ddaea903844387569c2d558e504bb67163af0e53" translate="yes" xml:space="preserve">
          <source>The RationalQuadratic kernel can be seen as a scale mixture (an infinite sum) of RBF kernels with different characteristic length-scales. It is parameterized by a length-scale parameter length_scale&amp;gt;0 and a scale mixture parameter alpha&amp;gt;0. Only the isotropic variant where length_scale is a scalar is supported at the moment. The kernel given by:</source>
          <target state="translated">RationalQuadraticカーネルは、異なる特性の長さスケールを持つRBFカーネルのスケール混合（無限合計）と見なすことができます。これは、長さスケールパラメーターlength_scale&amp;gt; 0およびスケール混合パラメーターalpha&amp;gt; 0によってパラメーター化されます。現在、length_scaleがスカラーである等方性バリアントのみがサポートされています。以下によって与えられるカーネル：</target>
        </trans-unit>
        <trans-unit id="66cebb865fc92b18216814d7cca47981b6cb6b41" translate="yes" xml:space="preserve">
          <source>The SMACOF (Scaling by MAjorizing a COmplicated Function) algorithm is a multidimensional scaling algorithm which minimizes an objective function (the &lt;em&gt;stress&lt;/em&gt;) using a majorization technique. Stress majorization, also known as the Guttman Transform, guarantees a monotone convergence of stress, and is more powerful than traditional techniques such as gradient descent.</source>
          <target state="translated">SMACOF（COmplicated Function をメジャー化することによるスケーリング）アルゴリズムは、多次元化アルゴリズムであり、メジャー化手法を使用して目的関数（&lt;em&gt;ストレス&lt;/em&gt;）を最小化します。ガットマン変換とも呼ばれるストレスメジャー化は、ストレスの単調な収束を保証し、勾配降下法などの従来の手法よりも強力です。</target>
        </trans-unit>
        <trans-unit id="e2c4971923aff3f7fe2f8678fb678a8a1fe12716" translate="yes" xml:space="preserve">
          <source>The SMACOF algorithm for metric MDS can summarized by the following steps:</source>
          <target state="translated">メトリックMDSのためのSMACOFアルゴリズムは、以下のステップで要約することができます。</target>
        </trans-unit>
        <trans-unit id="9991accadfafce42fd0097ac5bf1452a92166503" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient &lt;em&gt;s&lt;/em&gt; for a single sample is then given as:</source>
          <target state="translated">単一のサンプルのシルエット係数&lt;em&gt;s&lt;/em&gt;は、次のように与えられます。</target>
        </trans-unit>
        <trans-unit id="977018d27e576be441891586d29895a67820e9bb" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient for a set of samples is given as the mean of the Silhouette Coefficient for each sample.</source>
          <target state="translated">サンプルの集合に対するシルエット係数は、各サンプルのシルエット係数の平均値として与えられる。</target>
        </trans-unit>
        <trans-unit id="a8aa62253fcb64807b1a0f2a014811a76f4c09cc" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient is a measure of how well samples are clustered with samples that are similar to themselves. Clustering models with a high Silhouette Coefficient are said to be dense, where samples in the same cluster are similar to each other, and well separated, where samples in different clusters are not very similar to each other.</source>
          <target state="translated">シルエット係数は、サンプルがそれ自身に似たサンプルでどれだけよくクラスタリングされているかを示す指標である。シルエット係数が高いクラスタリングモデルは、同じクラスタ内のサンプルが互いに類似している場合は密であり、異なるクラスタ内のサンプルが互いにあまり類似していない場合はよく分離されていると言われています。</target>
        </trans-unit>
        <trans-unit id="5da8604230381cdf66d3aec27ea08ace466e606c" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient is calculated using the mean intra-cluster distance (&lt;code&gt;a&lt;/code&gt;) and the mean nearest-cluster distance (&lt;code&gt;b&lt;/code&gt;) for each sample. The Silhouette Coefficient for a sample is &lt;code&gt;(b - a) / max(a,
b)&lt;/code&gt;. Note that Silhouette Coefficient is only defined if number of labels is 2 &amp;lt;= n_labels &amp;lt;= n_samples - 1.</source>
          <target state="translated">シルエット係数は、各サンプルの平均クラスター内距離（ &lt;code&gt;a&lt;/code&gt; ）と平均最近傍クラスター距離（ &lt;code&gt;b&lt;/code&gt; ）を使用して計算されます。サンプルのシルエット係数は &lt;code&gt;(b - a) / max(a, b)&lt;/code&gt; です。シルエット係数は、ラベルの数が2 &amp;lt;= n_labels &amp;lt;= n_samples-1の場合にのみ定義されることに注意してください。</target>
        </trans-unit>
        <trans-unit id="4273a368cac49099b8caa7c819b214a9d85c0ff4" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient is calculated using the mean intra-cluster distance (&lt;code&gt;a&lt;/code&gt;) and the mean nearest-cluster distance (&lt;code&gt;b&lt;/code&gt;) for each sample. The Silhouette Coefficient for a sample is &lt;code&gt;(b - a) / max(a,
b)&lt;/code&gt;. To clarify, &lt;code&gt;b&lt;/code&gt; is the distance between a sample and the nearest cluster that the sample is not a part of. Note that Silhouette Coefficient is only defined if number of labels is 2 &amp;lt;= n_labels &amp;lt;= n_samples - 1.</source>
          <target state="translated">シルエット係数は、各サンプルの平均クラスター内距離（ &lt;code&gt;a&lt;/code&gt; ）と平均最近傍クラスター距離（ &lt;code&gt;b&lt;/code&gt; ）を使用して計算されます。サンプルのシルエット係数は &lt;code&gt;(b - a) / max(a, b)&lt;/code&gt; です。明確にするために、 &lt;code&gt;b&lt;/code&gt; は、サンプルと、そのサンプルが含まれていない最も近いクラスターとの間の距離です。シルエット係数は、ラベルの数が2 &amp;lt;= n_labels &amp;lt;= n_samples-1の場合にのみ定義されることに注意してください。</target>
        </trans-unit>
        <trans-unit id="74a577cf3f9facf0caa36a3cd46ce91a25197ef1" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN.</source>
          <target state="translated">シルエット係数は、DBSCANで得られるような密度ベースのクラスターのような他のクラスターの概念に比べて、一般的に凸型クラスターの方が高い。</target>
        </trans-unit>
        <trans-unit id="b08373a116f8a28b8f59b4a939f8e86bc18a285c" translate="yes" xml:space="preserve">
          <source>The Spearman correlation coefficient is estimated from the data, and the sign of the resulting estimate is used as the result.</source>
          <target state="translated">データからスピアマン相関係数を推定し、その結果の符号を結果とします。</target>
        </trans-unit>
        <trans-unit id="1aca461bec942a3ea49c28ca350e80c03610ed92" translate="yes" xml:space="preserve">
          <source>The Spectral Embedding (Laplacian Eigenmaps) algorithm comprises three stages:</source>
          <target state="translated">スペクトル埋め込み(ラプラシアン固有マップ)アルゴリズムは、3つの段階から構成されています。</target>
        </trans-unit>
        <trans-unit id="530048073ab2f0fd4beae1a41150629fce3bbc04" translate="yes" xml:space="preserve">
          <source>The TF-IDF vectorized posts form a word frequency matrix, which is then biclustered using Dhillon&amp;rsquo;s Spectral Co-Clustering algorithm. The resulting document-word biclusters indicate subsets words used more often in those subsets documents.</source>
          <target state="translated">TF-IDFのベクトル化されたポストは単語頻度行列を形成し、その後DhillonのSpectral Co-Clusteringアルゴリズムを使用してバイクラスター化されます。結果として得られるドキュメント単語バイクラスタは、これらのサブセットドキュメントでより頻繁に使用されるサブセット単語を示します。</target>
        </trans-unit>
        <trans-unit id="a51b4bd7337a8a43bf76bb00c70355d636e98cf2" translate="yes" xml:space="preserve">
          <source>The V-measure is actually equivalent to the mutual information (NMI) discussed above, with the aggregation function being the arithmetic mean &lt;a href=&quot;#b2011&quot; id=&quot;id15&quot;&gt;[B2011]&lt;/a&gt;.</source>
          <target state="translated">Vメジャーは実際には上記の相互情報量（NMI）と同等であり、集計関数は算術平均です&lt;a href=&quot;#b2011&quot; id=&quot;id15&quot;&gt;[B2011]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="28e22a992327910c0281c7997fd0381a055b2da5" translate="yes" xml:space="preserve">
          <source>The V-measure is the harmonic mean between homogeneity and completeness:</source>
          <target state="translated">Vメジャーは、均質性と完全性の間の調和平均です。</target>
        </trans-unit>
        <trans-unit id="9d831d6fbe54cab7c78889ea027eda02af846327" translate="yes" xml:space="preserve">
          <source>The Yeo-Johnson transform is given by:</source>
          <target state="translated">Yeo-Johnson変換は次式で与えられる。</target>
        </trans-unit>
        <trans-unit id="dd310b3ef8cade9b7b44463cf24d9960d1a07902" translate="yes" xml:space="preserve">
          <source>The \(\ell = \lceil \log_2 k \rceil\) singular vectors, starting from the second, provide the desired partitioning information. They are used to form the matrix \(Z\):</source>
          <target state="translated">2 番目から始まる単数ベクトルは、所望のパーティショニング情報を提供する。それらは、行列を形成するために使用されます。</target>
        </trans-unit>
        <trans-unit id="6bae7eee8e57958cb7a015c09e85982ab959fe35" translate="yes" xml:space="preserve">
          <source>The \(k\)-neighbors classification in &lt;a href=&quot;generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt;&lt;code&gt;KNeighborsClassifier&lt;/code&gt;&lt;/a&gt; is the most commonly used technique. The optimal choice of the value \(k\) is highly data-dependent: in general a larger \(k\) suppresses the effects of noise, but makes the classification boundaries less distinct.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt; &lt;code&gt;KNeighborsClassifier&lt;/code&gt; &lt;/a&gt;の\（k \）-neighbors分類は、最も一般的に使用される手法です。値\（k \）の最適な選択は、データに大きく依存します。一般に、\（k \）を大きくすると、ノイズの影響が抑制されますが、分類の境界が目立たなくなります。</target>
        </trans-unit>
        <trans-unit id="7ea48a47287b56e3f66c9ec81d51c291e8fedf59" translate="yes" xml:space="preserve">
          <source>The above vectorization scheme is simple but the fact that it holds an &lt;strong&gt;in- memory mapping from the string tokens to the integer feature indices&lt;/strong&gt; (the &lt;code&gt;vocabulary_&lt;/code&gt; attribute) causes several &lt;strong&gt;problems when dealing with large datasets&lt;/strong&gt;:</source>
          <target state="translated">上記のベクトル化スキームは単純ですが、&lt;strong&gt;文字列トークンから整数機能インデックス&lt;/strong&gt;（ &lt;code&gt;vocabulary_&lt;/code&gt; 属性）&lt;strong&gt;へのメモリ内マッピングを&lt;/strong&gt;保持しているため&lt;strong&gt;、大きなデータセットを処理するときに&lt;/strong&gt;いくつかの&lt;strong&gt;問題が発生し&lt;/strong&gt;ます。</target>
        </trans-unit>
        <trans-unit id="58279b870becff96bab0fc34e17d7f045af03b34" translate="yes" xml:space="preserve">
          <source>The abstract base class for all kernels is &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt;&lt;code&gt;Kernel&lt;/code&gt;&lt;/a&gt;. Kernel implements a similar interface as &lt;code&gt;Estimator&lt;/code&gt;, providing the methods &lt;code&gt;get_params()&lt;/code&gt;, &lt;code&gt;set_params()&lt;/code&gt;, and &lt;code&gt;clone()&lt;/code&gt;. This allows setting kernel values also via meta-estimators such as &lt;code&gt;Pipeline&lt;/code&gt; or &lt;code&gt;GridSearch&lt;/code&gt;. Note that due to the nested structure of kernels (by applying kernel operators, see below), the names of kernel parameters might become relatively complicated. In general, for a binary kernel operator, parameters of the left operand are prefixed with &lt;code&gt;k1__&lt;/code&gt; and parameters of the right operand with &lt;code&gt;k2__&lt;/code&gt;. An additional convenience method is &lt;code&gt;clone_with_theta(theta)&lt;/code&gt;, which returns a cloned version of the kernel but with the hyperparameters set to &lt;code&gt;theta&lt;/code&gt;. An illustrative example:</source>
          <target state="translated">すべてのカーネルの抽象基本クラスは&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt; &lt;code&gt;Kernel&lt;/code&gt; &lt;/a&gt;です。カーネルは、 &lt;code&gt;Estimator&lt;/code&gt; と同様のインターフェースを実装し、メソッド &lt;code&gt;get_params()&lt;/code&gt; 、 &lt;code&gt;set_params()&lt;/code&gt; 、および &lt;code&gt;clone()&lt;/code&gt; を提供します。これにより、 &lt;code&gt;Pipeline&lt;/code&gt; や &lt;code&gt;GridSearch&lt;/code&gt; などのメタ推定器を介してカーネル値を設定することもできます。カーネルのネスト構造（カーネルオペレーターを適用することにより、以下を参照）により、カーネルパラメーターの名前が比較的複雑になる可能性があることに注意してください。一般的に、バイナリカーネルオペレータのため、左のオペランドのパラメータが付いさ &lt;code&gt;k1__&lt;/code&gt; 及び右側オペランドのパラメータ &lt;code&gt;k2__&lt;/code&gt; 。追加の便利な方法は &lt;code&gt;clone_with_theta(theta)&lt;/code&gt; は、カーネルのクローンバージョンを返しますが、ハイパーパラメータは &lt;code&gt;theta&lt;/code&gt; に設定されています。説明的な例：</target>
        </trans-unit>
        <trans-unit id="d6dfb2a865fc7561c9990f60831354c8c2b2acf1" translate="yes" xml:space="preserve">
          <source>The actual linear program used to obtain the separating plane in the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: &amp;ldquo;Robust Linear Programming Discrimination of Two Linearly Inseparable Sets&amp;rdquo;, Optimization Methods and Software 1, 1992, 23-34].</source>
          <target state="translated">3次元空間で分離平面を取得するために使用される実際の線形プログラムは、[KPベネットとOLマンガリアン：「2つの線形分離不能セットのロバスト線形プログラミング識別」、最適化方法とソフトウェア1、1992、23- 34]。</target>
        </trans-unit>
        <trans-unit id="5c16c787f9e7f87c21ce4f86533b13082a6022ce" translate="yes" xml:space="preserve">
          <source>The actual number of iteration performed by the solver. Only returned if &lt;code&gt;return_n_iter&lt;/code&gt; is True.</source>
          <target state="translated">ソルバーによって実行された実際の反復回数。 &lt;code&gt;return_n_iter&lt;/code&gt; がTrueの場合にのみ返されます。</target>
        </trans-unit>
        <trans-unit id="c6a597b70b1c3dba7fc91a33b2b458a1718ae376" translate="yes" xml:space="preserve">
          <source>The actual number of iterations to reach the stopping criterion.</source>
          <target state="translated">停止基準に到達するまでの実際の反復回数。</target>
        </trans-unit>
        <trans-unit id="be3e6ed927427ff958a3dc691ec6f2ca38b153e8" translate="yes" xml:space="preserve">
          <source>The actual number of iterations to reach the stopping criterion. For multiclass fits, it is the maximum over every binary fit.</source>
          <target state="translated">停止基準に到達するための実際の反復回数。マルチクラスフィットの場合は、すべてのバイナリフィットの最大値です。</target>
        </trans-unit>
        <trans-unit id="905122fa4ae495ec172d69201d288d9cbd8eb107" translate="yes" xml:space="preserve">
          <source>The actual number of neighbors used for &lt;a href=&quot;#sklearn.neighbors.LocalOutlierFactor.kneighbors&quot;&gt;&lt;code&gt;kneighbors&lt;/code&gt;&lt;/a&gt; queries.</source>
          <target state="translated">&lt;a href=&quot;#sklearn.neighbors.LocalOutlierFactor.kneighbors&quot;&gt; &lt;code&gt;kneighbors&lt;/code&gt; &lt;/a&gt;クエリに使用されるネイバーの実際の数。</target>
        </trans-unit>
        <trans-unit id="99f948dd8ff8ebdf8ccc1268f27f992c4d54a1e0" translate="yes" xml:space="preserve">
          <source>The actual number of samples</source>
          <target state="translated">実際のサンプル数</target>
        </trans-unit>
        <trans-unit id="257683b273bf01e4ad40af7f18f19c426c837478" translate="yes" xml:space="preserve">
          <source>The additive chi squared kernel as used here is given by</source>
          <target state="translated">ここで使用される加法カイ二乗カーネルは次式で与えられます。</target>
        </trans-unit>
        <trans-unit id="721e722946fa16cdc1d4e80a122a59df383d3b35" translate="yes" xml:space="preserve">
          <source>The additive chi squared kernel is a kernel on histograms, often used in computer vision.</source>
          <target state="translated">加法カイ2乗カーネルは、コンピュータビジョンでよく使われるヒストグラム上のカーネルです。</target>
        </trans-unit>
        <trans-unit id="509930cd9616089e31b59f87837fde9a2850682a" translate="yes" xml:space="preserve">
          <source>The additive version of this kernel</source>
          <target state="translated">このカーネルの加算版</target>
        </trans-unit>
        <trans-unit id="479259eedb54024948ef963b0d114b00fbb05f1b" translate="yes" xml:space="preserve">
          <source>The adjacency matrix is used to compute a normalized graph Laplacian whose spectrum (especially the eigenvectors associated to the smallest eigenvalues) has an interpretation in terms of minimal number of cuts necessary to split the graph into comparably sized components.</source>
          <target state="translated">隣接行列は,正規化されたグラフのラプラシアンを計算するために使用され,そのスペクトル(特に最小の固有値に関連する固有ベクトル)は,グラフを比較可能な大きさの成分に分割するのに必要な最小のカット数という観点から解釈されます.</target>
        </trans-unit>
        <trans-unit id="b4f6e52ff52334d95b6f16934aecd26800758ca5" translate="yes" xml:space="preserve">
          <source>The adjacency matrix of the graph to embed.</source>
          <target state="translated">埋め込むグラフの隣接行列。</target>
        </trans-unit>
        <trans-unit id="f8c06d6c9bebb104301c962acd25687eac4074b4" translate="yes" xml:space="preserve">
          <source>The adjusted Rand index is thus ensured to have a value close to 0.0 for random labeling independently of the number of clusters and samples and exactly 1.0 when the clusterings are identical (up to a permutation).</source>
          <target state="translated">このようにして、調整されたRand指数は、クラスタ数とサンプル数に依存しないランダムなラベリングの場合は0.0に近い値を持ち、クラスタリングが同一の場合(順列まで)は正確に1.0に近い値を持つことが保証されています。</target>
        </trans-unit>
        <trans-unit id="83e2303d70450b875b04241e83f4a4153932e0a9" translate="yes" xml:space="preserve">
          <source>The advantage of using approximate explicit feature maps compared to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_trick&quot;&gt;kernel trick&lt;/a&gt;, which makes use of feature maps implicitly, is that explicit mappings can be better suited for online learning and can significantly reduce the cost of learning with very large datasets. Standard kernelized SVMs do not scale well to large datasets, but using an approximate kernel map it is possible to use much more efficient linear SVMs. In particular, the combination of kernel map approximations with &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; can make non-linear learning on large datasets possible.</source>
          <target state="translated">&lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_trick&quot;&gt;カーネルトリック&lt;/a&gt;と比較して近似の明示的な機能マップを使用する利点は、暗黙的に機能マップを使用するため、明示的なマッピングがオンライン学習に適し、非常に大きなデータセットでの学習コストを大幅に削減できることです。標準のカーネル化されたSVMは、大規模なデータセットにうまく対応できませんが、おおよそのカーネルマップを使用すると、はるかに効率的な線形SVMを使用できます。特に、カーネルマップ近似と&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;の組み合わせにより、大規模なデータセットでの非線形学習が可能になります。</target>
        </trans-unit>
        <trans-unit id="e8cf8d3d0c8dfd7b6a3c8351c56803f2fdf0d2d8" translate="yes" xml:space="preserve">
          <source>The advantages of Bayesian Regression are:</source>
          <target state="translated">ベイズ回帰のメリットは</target>
        </trans-unit>
        <trans-unit id="d7269fefe21b18a1d9b1e8a11da2f35edf3e36ec" translate="yes" xml:space="preserve">
          <source>The advantages of GBRT are:</source>
          <target state="translated">GBRTのメリットは</target>
        </trans-unit>
        <trans-unit id="6bdcff89c23af609c3f964058bbddc38e45558f8" translate="yes" xml:space="preserve">
          <source>The advantages of Gaussian processes are:</source>
          <target state="translated">ガウス過程の利点は</target>
        </trans-unit>
        <trans-unit id="c632bf8abb99b63a04554183dcc32e66bb0cf004" translate="yes" xml:space="preserve">
          <source>The advantages of LARS are:</source>
          <target state="translated">LARSのメリットは</target>
        </trans-unit>
        <trans-unit id="54923c815d40d3138e9fbe2ad92c97dd60ef1b2e" translate="yes" xml:space="preserve">
          <source>The advantages of Multi-layer Perceptron are:</source>
          <target state="translated">多層パーセプトロンの利点は、以下の通りです。</target>
        </trans-unit>
        <trans-unit id="1dabd14761114fadc83f1114e989744a341117db" translate="yes" xml:space="preserve">
          <source>The advantages of Stochastic Gradient Descent are:</source>
          <target state="translated">確率勾配降下法の利点は、以下の通りです。</target>
        </trans-unit>
        <trans-unit id="a12cea148a9927c20c87185a7ad428a4bd953760" translate="yes" xml:space="preserve">
          <source>The advantages of support vector machines are:</source>
          <target state="translated">サポートベクターマシンの利点は</target>
        </trans-unit>
        <trans-unit id="b3ce019dea8f1fa7178760b3ed7e1bed715f9894" translate="yes" xml:space="preserve">
          <source>The affinity matrix describing the relationship of the samples to embed. &lt;strong&gt;Must be symmetric&lt;/strong&gt;.</source>
          <target state="translated">埋め込むサンプルの関係を表す親和性マトリックス。&lt;strong&gt;対称でなければなりません&lt;/strong&gt;。</target>
        </trans-unit>
        <trans-unit id="f8f51936a34abb4d8304c65310991fbeb781a2b1" translate="yes" xml:space="preserve">
          <source>The algorithm automatically sets the number of clusters, instead of relying on a parameter &lt;code&gt;bandwidth&lt;/code&gt;, which dictates the size of the region to search through. This parameter can be set manually, but can be estimated using the provided &lt;code&gt;estimate_bandwidth&lt;/code&gt; function, which is called if the bandwidth is not set.</source>
          <target state="translated">アルゴリズムは、検索する領域のサイズを指定するパラメーターの &lt;code&gt;bandwidth&lt;/code&gt; に依存するのではなく、クラスターの数を自動的に設定します。このパラメーターは手動で設定できますが、帯域幅が設定されていない場合に呼び出される、提供 &lt;code&gt;estimate_bandwidth&lt;/code&gt; 帯域幅関数を使用して推定できます。</target>
        </trans-unit>
        <trans-unit id="999ae100a463cc0ff60234ca1a1687522770fa40" translate="yes" xml:space="preserve">
          <source>The algorithm calculates least square solutions on subsets with size n_subsamples of the samples in X. Any value of n_subsamples between the number of features and samples leads to an estimator with a compromise between robustness and efficiency. Since the number of least square solutions is &amp;ldquo;n_samples choose n_subsamples&amp;rdquo;, it can be extremely large and can therefore be limited with max_subpopulation. If this limit is reached, the subsets are chosen randomly. In a final step, the spatial median (or L1 median) is calculated of all least square solutions.</source>
          <target state="translated">アルゴリズムは、Xのサンプルのサイズがn_subsamplesであるサブセットの最小二乗解を計算します。特徴とサンプルの数の間のn_subsamplesの値は、ロバスト性と効率の間で妥協した推定器になります。最小二乗解の数は「n_samplesはn_subsamplesを選択」であるため、非常に大きくなる可能性があり、したがってmax_subpopulationで制限できます。この制限に達すると、サブセットはランダムに選択されます。最後のステップでは、すべての最小二乗解の空間中央値（またはL1中央値）が計算されます。</target>
        </trans-unit>
        <trans-unit id="19207c781cd295df6565aab588711bf1a9323bef" translate="yes" xml:space="preserve">
          <source>The algorithm can also be understood through the concept of &lt;a href=&quot;https://en.wikipedia.org/wiki/Voronoi_diagram&quot;&gt;Voronoi diagrams&lt;/a&gt;. First the Voronoi diagram of the points is calculated using the current centroids. Each segment in the Voronoi diagram becomes a separate cluster. Secondly, the centroids are updated to the mean of each segment. The algorithm then repeats this until a stopping criterion is fulfilled. Usually, the algorithm stops when the relative decrease in the objective function between iterations is less than the given tolerance value. This is not the case in this implementation: iteration stops when centroids move less than the tolerance.</source>
          <target state="translated">アルゴリズムは、&lt;a href=&quot;https://en.wikipedia.org/wiki/Voronoi_diagram&quot;&gt;ボロノイ図の&lt;/a&gt;概念を通じて理解することもできます。まず、ポイントのボロノイ図が現在の重心を使用して計算されます。ボロノイ図の各セグメントは、個別のクラスターになります。次に、重心は各セグメントの平均に更新されます。アルゴリズムは、停止基準が満たされるまでこれを繰り返します。通常、アルゴリズムは、反復間の目的関数の相対的な減少が指定された許容値を下回ると停止します。これは、この実装では当てはまりません。重心が許容値未満で移動すると、反復が停止します。</target>
        </trans-unit>
        <trans-unit id="ac387733191797f4111e67a021bb9641f7899ac2" translate="yes" xml:space="preserve">
          <source>The algorithm employed to solve this problem is the GLasso algorithm, from the Friedman 2008 Biostatistics paper. It is the same algorithm as in the R &lt;code&gt;glasso&lt;/code&gt; package.</source>
          <target state="translated">この問題を解決するために採用されたアルゴリズムは、Friedman 2008 BiostatisticsペーパーのGLassoアルゴリズムです。これは、R &lt;code&gt;glasso&lt;/code&gt; パッケージと同じアルゴリズムです。</target>
        </trans-unit>
        <trans-unit id="2d7b95845283c4d98cc4167d4e0748ee6c69e84f" translate="yes" xml:space="preserve">
          <source>The algorithm for incremental mean and std is given in Equation 1.5a,b in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. &amp;ldquo;Algorithms for computing the sample variance: Analysis and recommendations.&amp;rdquo; The American Statistician 37.3 (1983): 242-247:</source>
          <target state="translated">増分平均とstdのアルゴリズムは、Chan、Tony F.、Gene H. Golub、およびRandall J. LeVequeの式1.5a、bに示されています。「サンプル分散を計算するためのアルゴリズム：分析と推奨事項。」アメリカ統計学者37.3（1983）：242-247：</target>
        </trans-unit>
        <trans-unit id="c4e655f6aa7cf22a58a123dc60b12c815d9f7df3" translate="yes" xml:space="preserve">
          <source>The algorithm is adapted from Guyon [1] and was designed to generate the &amp;ldquo;Madelon&amp;rdquo; dataset.</source>
          <target state="translated">このアルゴリズムはGuyon [1]から採用されており、「Madelon」データセットを生成するように設計されています。</target>
        </trans-unit>
        <trans-unit id="da28c971693f926f3030184039a925bcc2b1ca76" translate="yes" xml:space="preserve">
          <source>The algorithm is from Marsland [1].</source>
          <target state="translated">アルゴリズムはMarsland [1]からのものです。</target>
        </trans-unit>
        <trans-unit id="94c105f8aebba6a360eae1c93878348f10f58fca" translate="yes" xml:space="preserve">
          <source>The algorithm is not highly scalable, as it requires multiple nearest neighbor searches during the execution of the algorithm. The algorithm is guaranteed to converge, however the algorithm will stop iterating when the change in centroids is small.</source>
          <target state="translated">このアルゴリズムは、アルゴリズムの実行中に複数の最近傍探索を必要とするため、非常にスケーラブルではありません。このアルゴリズムは収束することが保証されていますが、セントロイドの変化が小さい場合、アルゴリズムは反復処理を停止します。</target>
        </trans-unit>
        <trans-unit id="3dc6012ab672dce7fe920dc60add16368612e345" translate="yes" xml:space="preserve">
          <source>The algorithm is similar to forward stepwise regression, but instead of including variables at each step, the estimated parameters are increased in a direction equiangular to each one&amp;rsquo;s correlations with the residual.</source>
          <target state="translated">アルゴリズムは順方向ステップワイズ回帰に​​似ていますが、各ステップで変数を含めるのではなく、推定パラメーターは、残差との各相関と等角の方向に増加します。</target>
        </trans-unit>
        <trans-unit id="0335c4d6784d824f2c45b2464a3517f723de00c5" translate="yes" xml:space="preserve">
          <source>The algorithm is stochastic and multiple restarts with different seeds can yield different embeddings. However, it is perfectly legitimate to pick the embedding with the least error.</source>
          <target state="translated">このアルゴリズムは確率的なものであり、異なるシードを用いて複数回の再起動を行うと、異なる埋め込みが得られる可能性があります。しかし、誤差が最も少ない埋め込みを選ぶことは完全に正当なことです。</target>
        </trans-unit>
        <trans-unit id="7c7de93467b285ca3f0b63a2764ebf7e5ab511ed" translate="yes" xml:space="preserve">
          <source>The algorithm iterates between two major steps, similar to vanilla k-means. In the first step, \(b\) samples are drawn randomly from the dataset, to form a mini-batch. These are then assigned to the nearest centroid. In the second step, the centroids are updated. In contrast to k-means, this is done on a per-sample basis. For each sample in the mini-batch, the assigned centroid is updated by taking the streaming average of the sample and all previous samples assigned to that centroid. This has the effect of decreasing the rate of change for a centroid over time. These steps are performed until convergence or a predetermined number of iterations is reached.</source>
          <target state="translated">このアルゴリズムは、バニラのk-meansと同様に、2つの主要なステップの間で反復します。最初のステップでは、ミニバッチを形成するために、データセットからランダムに \(b\)サンプルが引き出されます。これらは、次に、最も近いセントロイドに割り当てられます。第2のステップでは,セントロイドが更新される.k平均とは対照的に,これはサンプルごとに行われる.ミニバッチ内の各サンプルについて、割り当てられたセントロイドは、サンプルのストリーミング平均と、そのセントロイドに割り当てられた以前のすべてのサンプルを取ることによって更新されます。これは、時間の経過とともにセントロイドの変化率を減少させる効果があります。これらのステップは、収束または所定の反復回数に達するまで実行されます。</target>
        </trans-unit>
        <trans-unit id="eac8adf7ae77ba9768138371c6c71edd5a3a76a7" translate="yes" xml:space="preserve">
          <source>The algorithm partitions the rows and columns of a matrix so that a corresponding blockwise-constant checkerboard matrix provides a good approximation to the original matrix.</source>
          <target state="translated">このアルゴリズムは,対応するブロック間定数チェッカーボード行列が元の行列に対して良好な近似を提供するように,行列の行と列を分割します.</target>
        </trans-unit>
        <trans-unit id="36533938d0e94afa89a69e7b9609c61646ab71f5" translate="yes" xml:space="preserve">
          <source>The algorithm splits the complete input sample data into a set of inliers, which may be subject to noise, and outliers, which are e.g. caused by erroneous measurements or invalid hypotheses about the data. The resulting model is then estimated only from the determined inliers.</source>
          <target state="translated">アルゴリズムは,完全な入力サンプル・データを,ノイズの影響を受ける可能性のあるインライアと,データについての誤った測定や無効な仮説などによって引き起こされるアウトライアのセットに分割する.結果として得られるモデルは,決定されたインライアからのみ推定される.</target>
        </trans-unit>
        <trans-unit id="76e87cc3ec2713663d8449fef17b4e0c4101c305" translate="yes" xml:space="preserve">
          <source>The algorithm stops when it reaches a preset maximum number of iterations; or when the improvement in loss is below a certain, small number.</source>
          <target state="translated">アルゴリズムは、あらかじめ設定された最大反復回数に達したとき、または損失の改善が特定の小さな数以下になったときに停止します。</target>
        </trans-unit>
        <trans-unit id="841b170fb7ba2429816cb7c51af4dbae57b471af" translate="yes" xml:space="preserve">
          <source>The algorithm supports sample weights, which can be given by a parameter &lt;code&gt;sample_weight&lt;/code&gt;. This allows to assign more weight to some samples when computing cluster centers and values of inertia. For example, assigning a weight of 2 to a sample is equivalent to adding a duplicate of that sample to the dataset \(X\).</source>
          <target state="translated">アルゴリズムは、サンプルの重みをサポートしています。これは、パラメーター &lt;code&gt;sample_weight&lt;/code&gt; で指定できます。これにより、クラスターの中心と慣性の値を計算するときに、いくつかのサンプルにより多くの重みを割り当てることができます。たとえば、サンプルに重み2を割り当てることは、そのサンプルの複製をデータセット\（X \）に追加することと同じです。</target>
        </trans-unit>
        <trans-unit id="a8238e48cf484817aaf23aa2adc5e4a70681d78f" translate="yes" xml:space="preserve">
          <source>The algorithm to be used by the NearestNeighbors module to compute pointwise distances and find nearest neighbors. See NearestNeighbors module documentation for details.</source>
          <target state="translated">NearestNeighbors モジュールが使用するアルゴリズム。詳細は NearestNeighbors モジュールのドキュメントを参照してください。</target>
        </trans-unit>
        <trans-unit id="3602b334476c395c578aab7a7d09d75f92100f28" translate="yes" xml:space="preserve">
          <source>The algorithm treats the input data matrix as a bipartite graph: the rows and columns of the matrix correspond to the two sets of vertices, and each entry corresponds to an edge between a row and a column. The algorithm approximates the normalized cut of this graph to find heavy subgraphs.</source>
          <target state="translated">このアルゴリズムは、入力データ行列を二部グラフとして扱います:行列の行と列は、2つの頂点のセットに対応し、各エントリは、行と列の間の辺に対応します。このアルゴリズムは,重い部分グラフを見つけるために,このグラフの正規化カットを近似します.</target>
        </trans-unit>
        <trans-unit id="e4086364ea17b4442e0265a08361358aa304a247" translate="yes" xml:space="preserve">
          <source>The algorithm used to estimate the weights. It will be called n_components times, i.e. once for each iteration of the outer loop.</source>
          <target state="translated">重みの推定に用いられるアルゴリズム.このアルゴリズムは n_components を何度も呼び出します。</target>
        </trans-unit>
        <trans-unit id="e4e3e486ab0114bca02d02635f9410b77ca99a25" translate="yes" xml:space="preserve">
          <source>The algorithm used to fit the model is coordinate descent.</source>
          <target state="translated">モデルのフィットに使用されるアルゴリズムは、座標降下です。</target>
        </trans-unit>
        <trans-unit id="17e35eb9c6004e2d60b73104c93585aacf4a2e98" translate="yes" xml:space="preserve">
          <source>The algorithmic complexity of affinity propagation is quadratic in the number of points.</source>
          <target state="translated">親和性伝播のアルゴリズム的複雑さは、点の数で二次的である。</target>
        </trans-unit>
        <trans-unit id="4c657d81ec38ba678d89f7b7de187e376e50c0f3" translate="yes" xml:space="preserve">
          <source>The algorithms for regression and classification only differ in the concrete loss function used.</source>
          <target state="translated">回帰と分類のアルゴリズムは、使用する具体的な損失関数が異なるだけです。</target>
        </trans-unit>
        <trans-unit id="50542b2bc530bb15fc1bcd67d7b073d61bea03ab" translate="yes" xml:space="preserve">
          <source>The alpha parameter in the stability selection article used to randomly scale the features. Should be between 0 and 1.</source>
          <target state="translated">特徴をランダムにスケールするために使用される,安定性選択項目のアルファ・パラメータ.0から1の間でなければなりません。</target>
        </trans-unit>
        <trans-unit id="35d70c04237d10e5da496ff67ac57fce44665490" translate="yes" xml:space="preserve">
          <source>The alpha parameter of the GraphicalLasso setting the sparsity of the model is set by internal cross-validation in the GraphicalLassoCV. As can be seen on figure 2, the grid to compute the cross-validation score is iteratively refined in the neighborhood of the maximum.</source>
          <target state="translated">モデルのスパース度を設定するGraphicalLassoのαパラメータは、GraphicalLassoCVの内部クロスバリデーションによって設定される。図2に示すように、交差検証スコアを計算するグリッドは、最大値付近で反復的に精緻化されている。</target>
        </trans-unit>
        <trans-unit id="2075fb3135d145905cc5a41db16871a4bf5168da" translate="yes" xml:space="preserve">
          <source>The alpha-quantile of the huber loss function and the quantile loss function. Only if &lt;code&gt;loss='huber'&lt;/code&gt; or &lt;code&gt;loss='quantile'&lt;/code&gt;.</source>
          <target state="translated">フーバー損失関数と分位点損失関数のアルファ分位数。 &lt;code&gt;loss='huber'&lt;/code&gt; または &lt;code&gt;loss='quantile'&lt;/code&gt; の場合のみ。</target>
        </trans-unit>
        <trans-unit id="28a2d4f18d224d5f96bb30d3faa50cdca5ce2935" translate="yes" xml:space="preserve">
          <source>The alphas along the path where models are computed.</source>
          <target state="translated">モデルが計算されるパスに沿ったアルファ。</target>
        </trans-unit>
        <trans-unit id="4390acc7061cc013783802ff89b6303810b0044d" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set.</source>
          <target state="translated">データセットの汚染量、すなわちデータセット内の外れ値の割合。</target>
        </trans-unit>
        <trans-unit id="db48eba23a1b19c738dbbe1eb3626efb11bd507f" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. If &amp;lsquo;auto&amp;rsquo;, the decision function threshold is determined as in the original paper.</source>
          <target state="translated">データセットの汚染の量、つまりデータセット内の外れ値の比率。決定関数のしきい値を定義するためにフィッティングするときに使用されます。'auto'の場合、決定関数のしきい値は元の論文と同様に決定されます。</target>
        </trans-unit>
        <trans-unit id="0b69accc98489279e23dd931886f63d4aafd913d" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set. When fitting this is used to define the threshold on the decision function. If &amp;ldquo;auto&amp;rdquo;, the decision function threshold is determined as in the original paper.</source>
          <target state="translated">データセットの汚染の量、つまりデータセット内の外れ値の比率。フィッティングするとき、これは決定関数のしきい値を定義するために使用されます。「自動」の場合、決定関数のしきい値は元の論文と同様に決定されます。</target>
        </trans-unit>
        <trans-unit id="057670e523b801c82d76306d2019fbc0b1e55fa7" translate="yes" xml:space="preserve">
          <source>The amount of penalization chosen by cross validation</source>
          <target state="translated">クロスバリデーションによって選択されたペナルティの量</target>
        </trans-unit>
        <trans-unit id="0a8889ae5955aac97641248ca40c9031408985ae" translate="yes" xml:space="preserve">
          <source>The amount of variance explained by each of the selected components.</source>
          <target state="translated">選択された各成分によって説明される分散の量。</target>
        </trans-unit>
        <trans-unit id="0c35119665a5a383bf2a2ea2491d6548297b77b9" translate="yes" xml:space="preserve">
          <source>The anomaly score of an input sample is computed as the mean anomaly score of the trees in the forest.</source>
          <target state="translated">入力サンプルの異常スコアは、森林内の樹木の平均異常スコアとして計算されます。</target>
        </trans-unit>
        <trans-unit id="f5bff141bb39bd3c0bc3c7fd23ee875e735e27a8" translate="yes" xml:space="preserve">
          <source>The anomaly score of each sample is called Local Outlier Factor. It measures the local deviation of density of a given sample with respect to its neighbors. It is local in that the anomaly score depends on how isolated the object is with respect to the surrounding neighborhood. More precisely, locality is given by k-nearest neighbors, whose distance is used to estimate the local density. By comparing the local density of a sample to the local densities of its neighbors, one can identify samples that have a substantially lower density than their neighbors. These are considered outliers.</source>
          <target state="translated">各標本の異常値スコアは,局所的外れ値因子と呼ばれる.これは、隣人に対する与えられたサンプルの密度の局所的な偏差を測定します。これは、異常スコアが、オブジェクトが周囲の近隣に対してどれだけ孤立しているかに依存するという点で局所的です。より正確には、局所性はk-最も近い隣人によって与えられ、その距離は局所密度を推定するために使用されます。サンプルの局所密度を隣人の局所密度と比較することで、隣人よりもかなり低い密度を持つサンプルを識別することができます。これらは外れ値と考えられます。</target>
        </trans-unit>
        <trans-unit id="9d2b05e02bf58b122225781451ec470ef3bbad43" translate="yes" xml:space="preserve">
          <source>The anomaly score of the input samples. The lower, the more abnormal.</source>
          <target state="translated">入力サンプルの異常スコア。低いほど異常です。</target>
        </trans-unit>
        <trans-unit id="7ea9f6456fff678e2acc40131f68202326a3c878" translate="yes" xml:space="preserve">
          <source>The anomaly score of the input samples. The lower, the more abnormal. Negative scores represent outliers, positive scores represent inliers.</source>
          <target state="translated">入力サンプルの異常スコア。低いほど異常である.負のスコアは外れ値を表し,正のスコアは外れ値を表す.</target>
        </trans-unit>
        <trans-unit id="f8f5cba472ebac3a5205adfc98746f94cfb55c44" translate="yes" xml:space="preserve">
          <source>The approximate feature map provided by &lt;a href=&quot;generated/sklearn.kernel_approximation.additivechi2sampler#sklearn.kernel_approximation.AdditiveChi2Sampler&quot;&gt;&lt;code&gt;AdditiveChi2Sampler&lt;/code&gt;&lt;/a&gt; can be combined with the approximate feature map provided by &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; to yield an approximate feature map for the exponentiated chi squared kernel. See the &lt;a href=&quot;#vz2010&quot; id=&quot;id5&quot;&gt;[VZ2010]&lt;/a&gt; for details and &lt;a href=&quot;#vvz2010&quot; id=&quot;id6&quot;&gt;[VVZ2010]&lt;/a&gt; for combination with the &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.kernel_approximation.additivechi2sampler#sklearn.kernel_approximation.AdditiveChi2Sampler&quot;&gt; &lt;code&gt;AdditiveChi2Sampler&lt;/code&gt; &lt;/a&gt;によって提供される近似機能マップを&lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt;によって提供される近似機能マップと組み合わせて、指数カイ2乗カーネルの近似機能マップを生成できます。参照&lt;a href=&quot;#vz2010&quot; id=&quot;id5&quot;&gt;[VZ2010]&lt;/a&gt;詳細とについて&lt;a href=&quot;#vvz2010&quot; id=&quot;id6&quot;&gt;[VVZ2010]&lt;/a&gt;との組み合わせのため&lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="98f8783bf76339c13180f5a0c21989e5d9843d3d" translate="yes" xml:space="preserve">
          <source>The approximate number of singular vectors required to explain most of the data by linear combinations.</source>
          <target state="translated">ほとんどのデータを線形の組み合わせで説明するのに必要な特異ベクトルのおおよその数。</target>
        </trans-unit>
        <trans-unit id="01540c9a0c0d75da953bc96aafe62b761a8754d9" translate="yes" xml:space="preserve">
          <source>The approximate number of singular vectors required to explain most of the input data by linear combinations. Using this kind of singular spectrum in the input allows the generator to reproduce the correlations often observed in practice.</source>
          <target state="translated">入力データの大部分を線形の組み合わせで説明するのに必要な特異ベクトルのおおよその数。この種の特異スペクトルを入力に使用することで、生成器は実際によく観察される相関を再現することができる。</target>
        </trans-unit>
        <trans-unit id="9b1212b40ba5f9b8205d7b64e2e9c20d35b415b3" translate="yes" xml:space="preserve">
          <source>The arithmetic mean is the sum of the elements along the axis divided by the number of elements.</source>
          <target state="translated">算術平均は、軸に沿った要素の総和を要素数で割ったものです。</target>
        </trans-unit>
        <trans-unit id="35a07a5d9cec4bce6db2b0fe073857a7979c5b16" translate="yes" xml:space="preserve">
          <source>The array has 0.16% of non zero values.</source>
          <target state="translated">配列には0.16%の非ゼロ値があります。</target>
        </trans-unit>
        <trans-unit id="449fd4c0217076d90d22633d6cb32b7300c30336" translate="yes" xml:space="preserve">
          <source>The array of (log)-density evaluations, shape = X.shape[:-1]</source>
          <target state="translated">(log)密度評価の配列,shape=X.shape[:-1].</target>
        </trans-unit>
        <trans-unit id="d00a652e1f004fd3ddfd653b6794ad2aba8108c7" translate="yes" xml:space="preserve">
          <source>The array of log(density) evaluations.</source>
          <target state="translated">log(密度)評価の配列。</target>
        </trans-unit>
        <trans-unit id="531360783ebbdd4b65c83458a2d186f114e4086b" translate="yes" xml:space="preserve">
          <source>The automatic estimation from Automatic Choice of Dimensionality for PCA. NIPS 2000: 598-604 by Thomas P. Minka is also compared.</source>
          <target state="translated">PCAのための次元性の自動選択からの自動推定.Thomas P.MinkaによるNIPS 2000:598-604も比較しています。</target>
        </trans-unit>
        <trans-unit id="37bf2d4736a8a0a6781e876ebdb656796aca8913" translate="yes" xml:space="preserve">
          <source>The available cross validation iterators are introduced in the following section.</source>
          <target state="translated">利用可能なクロスバリデーションイテレータについては、次のセクションで紹介します。</target>
        </trans-unit>
        <trans-unit id="52d681893d146fe7dad6c4424a94a534bab69431" translate="yes" xml:space="preserve">
          <source>The average complexity is given by O(k n T), were n is the number of samples and T is the number of iteration.</source>
          <target state="translated">平均的な複雑さはO(k n T)で与えられ、nはサンプル数、Tは反復回数である。</target>
        </trans-unit>
        <trans-unit id="d9092bb0da47977d9b43f41f12fd7dcc75d26801" translate="yes" xml:space="preserve">
          <source>The average number of labels per instance. More precisely, the number of labels per sample is drawn from a Poisson distribution with &lt;code&gt;n_labels&lt;/code&gt; as its expected value, but samples are bounded (using rejection sampling) by &lt;code&gt;n_classes&lt;/code&gt;, and must be nonzero if &lt;code&gt;allow_unlabeled&lt;/code&gt; is False.</source>
          <target state="translated">インスタンスあたりのラベルの平均数。より正確には、サンプルあたりのラベル数は、 &lt;code&gt;n_labels&lt;/code&gt; を期待値として持つポアソン分布から導き出されますが、サンプルは（拒否サンプリングを使用して） &lt;code&gt;n_classes&lt;/code&gt; によって制限され、 &lt;code&gt;allow_unlabeled&lt;/code&gt; がFalseの場合はゼロ以外でなければなりません。</target>
        </trans-unit>
        <trans-unit id="50521765649b7ccece3cb452f18896c14c55c5d8" translate="yes" xml:space="preserve">
          <source>The average precision score in multi-label settings</source>
          <target state="translated">マルチラベル設定での平均精度スコア</target>
        </trans-unit>
        <trans-unit id="7f192c1d1db49dfae3574eb70a5374ae8608eb71" translate="yes" xml:space="preserve">
          <source>The averaged intercept term.</source>
          <target state="translated">平均化された切片項。</target>
        </trans-unit>
        <trans-unit id="5f1e3570b35bda8ded1b9651eb310bcefe0589f6" translate="yes" xml:space="preserve">
          <source>The axes with which the grid has been created or None if the grid has been given.</source>
          <target state="translated">グリッドが作成された軸、またはグリッドが指定されている場合はなし。</target>
        </trans-unit>
        <trans-unit id="2af1a0461163ef9a917dd17f9a6c7032c07c79bb" translate="yes" xml:space="preserve">
          <source>The axis along which to impute.</source>
          <target state="translated">帰属させる軸。</target>
        </trans-unit>
        <trans-unit id="efae1008b127cbb418b23469c7f222e2a6660b67" translate="yes" xml:space="preserve">
          <source>The bag of words representation is quite simplistic but surprisingly useful in practice.</source>
          <target state="translated">単語の袋の表現はかなり単純ですが、実際には驚くほど便利です。</target>
        </trans-unit>
        <trans-unit id="a41734ff5b2ce49bfc6f83ebb1bb324e76d37d70" translate="yes" xml:space="preserve">
          <source>The bags of words representation implies that &lt;code&gt;n_features&lt;/code&gt; is the number of distinct words in the corpus: this number is typically larger than 100,000.</source>
          <target state="translated">単語のバッグ表現は、 &lt;code&gt;n_features&lt;/code&gt; がコーパス内の個別の単語の数であることを意味します。この数は通常、100,000を超えます。</target>
        </trans-unit>
        <trans-unit id="eac13f6898f72e0eaad5c85bd1b456a5882d1a72" translate="yes" xml:space="preserve">
          <source>The balanced accuracy in binary and multiclass classification problems to deal with imbalanced datasets. It is defined as the average of recall obtained on each class.</source>
          <target state="translated">不均衡なデータセットに対処するための二値分類問題や多クラス分類問題における均衡精度。各クラスで得られたリコールの平均値として定義される。</target>
        </trans-unit>
        <trans-unit id="fd188197709d9753bf6d2d7d8ebbb1b211377cd3" translate="yes" xml:space="preserve">
          <source>The bandwidth here acts as a smoothing parameter, controlling the tradeoff between bias and variance in the result. A large bandwidth leads to a very smooth (i.e. high-bias) density distribution. A small bandwidth leads to an unsmooth (i.e. high-variance) density distribution.</source>
          <target state="translated">ここでの帯域幅は平滑化パラメータとして機能し、結果のバイアスと分散の間のトレードオフを制御します。帯域幅が大きいと、非常に滑らかな(つまり高バイアスの)密度分布になります。帯域幅が小さいと、平滑でない(すなわち、高分散)密度分布になります。</target>
        </trans-unit>
        <trans-unit id="220a3cae014ca1f6f44493020405f4e460f4038d" translate="yes" xml:space="preserve">
          <source>The bandwidth of the kernel.</source>
          <target state="translated">カーネルの帯域幅。</target>
        </trans-unit>
        <trans-unit id="eaa1089b1fdc51cb43f528072a19cd3f3ec50c61" translate="yes" xml:space="preserve">
          <source>The bandwidth parameter.</source>
          <target state="translated">帯域幅パラメータ。</target>
        </trans-unit>
        <trans-unit id="ca9efc037882c6303cbf300aed185a1d9a7dd7ae" translate="yes" xml:space="preserve">
          <source>The bar plot indicates the accuracy, training time (normalized) and test time (normalized) of each classifier.</source>
          <target state="translated">棒グラフは、各分類器の精度、訓練時間(正規化)、試験時間(正規化)を示す。</target>
        </trans-unit>
        <trans-unit id="6ef0fbe42e5306306c086ebee574632386d4e293" translate="yes" xml:space="preserve">
          <source>The base classifier is a random forest classifier with 25 base estimators (trees). If this classifier is trained on all 800 training datapoints, it is overly confident in its predictions and thus incurs a large log-loss. Calibrating an identical classifier, which was trained on 600 datapoints, with method=&amp;rsquo;sigmoid&amp;rsquo; on the remaining 200 datapoints reduces the confidence of the predictions, i.e., moves the probability vectors from the edges of the simplex towards the center. This calibration results in a lower log-loss. Note that an alternative would have been to increase the number of base estimators which would have resulted in a similar decrease in log-loss.</source>
          <target state="translated">基本分類器は、25の基本推定量（ツリー）を持つランダムフォレスト分類器です。この分類器が800のトレーニングデータポイントすべてでトレーニングされると、予測に過度の自信があり、大きなログ損失が発生します。残りの200データポイントでmethod = 'sigmoid'を使用して600データポイントでトレーニングされた同一の分類子をキャリブレーションすると、予測の信頼性が低下します。つまり、確率ベクトルがシンプレックスのエッジから中心に向かって移動します。この調整により、対数損失が減少します。代替案は、対数損失の同様の減少をもたらすであろうベース推定量の数を増やすことであったことに注意してください。</target>
        </trans-unit>
        <trans-unit id="3f89ef1e6cb2a54f74eb0a982beea8ffd3333592" translate="yes" xml:space="preserve">
          <source>The base classifier is a random forest classifier with 25 base estimators (trees). If this classifier is trained on all 800 training datapoints, it is overly confident in its predictions and thus incurs a large log-loss. Calibrating an identical classifier, which was trained on 600 datapoints, with method=&amp;rsquo;sigmoid&amp;rsquo; on the remaining 200 datapoints reduces the confidence of the predictions, i.e., moves the probability vectors from the edges of the simplex towards the center:</source>
          <target state="translated">基本分類器は、25の基本推定量（ツリー）を持つランダムフォレスト分類器です。この分類器が800のトレーニングデータポイントすべてでトレーニングされると、予測に過度の自信があり、大きなログ損失が発生します。残りの200データポイントでmethod = 'sigmoid'を使用して600データポイントでトレーニングされた同一の分類子をキャリブレーションすると、予測の信頼度が低下します。つまり、確率ベクトルがシンプレックスのエッジから中心に向かって移動します。</target>
        </trans-unit>
        <trans-unit id="cdab13e5017fd96224833fdcfcd75615e152e1af" translate="yes" xml:space="preserve">
          <source>The base estimator from which the boosted ensemble is built. Support for sample weighting is required, as well as proper &lt;code&gt;classes_&lt;/code&gt; and &lt;code&gt;n_classes_&lt;/code&gt; attributes. If &lt;code&gt;None&lt;/code&gt;, then the base estimator is &lt;code&gt;DecisionTreeClassifier(max_depth=1)&lt;/code&gt;</source>
          <target state="translated">ブーストされたアンサンブルが作成されるベース推定量。サンプルの重み付けのサポートと、適切な &lt;code&gt;classes_&lt;/code&gt; および &lt;code&gt;n_classes_&lt;/code&gt; 属性が必要です。 &lt;code&gt;None&lt;/code&gt; の場合、基本推定量は &lt;code&gt;DecisionTreeClassifier(max_depth=1)&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="39ba9d176a83a208ed96d60f33bec321ac051ac5" translate="yes" xml:space="preserve">
          <source>The base estimator from which the boosted ensemble is built. Support for sample weighting is required. If &lt;code&gt;None&lt;/code&gt;, then the base estimator is &lt;code&gt;DecisionTreeRegressor(max_depth=3)&lt;/code&gt;</source>
          <target state="translated">ブーストされたアンサンブルが作成されるベース推定量。サンプルの重み付けのサポートが必要です。 &lt;code&gt;None&lt;/code&gt; の場合、基本推定量は &lt;code&gt;DecisionTreeRegressor(max_depth=3)&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="09e53f3d87743d060ad05fb04ae38587d00307b0" translate="yes" xml:space="preserve">
          <source>The base estimator from which the classifier chain is built.</source>
          <target state="translated">分類器チェーンが構築されるベース推定器。</target>
        </trans-unit>
        <trans-unit id="c229b061f41bb16a20d56916c466508eaa778c17" translate="yes" xml:space="preserve">
          <source>The base estimator from which the ensemble is grown.</source>
          <target state="translated">アンサンブルを成長させるベースとなる推定器。</target>
        </trans-unit>
        <trans-unit id="5066de8ea8cb5c40493cb3ffbb0e912b0ff83ff9" translate="yes" xml:space="preserve">
          <source>The base estimator from which the transformer is built. This can be both a fitted (if &lt;code&gt;prefit&lt;/code&gt; is set to True) or a non-fitted estimator. The estimator must have either a &lt;code&gt;feature_importances_&lt;/code&gt; or &lt;code&gt;coef_&lt;/code&gt; attribute after fitting.</source>
          <target state="translated">トランスが作成されるベース推定量。これは、適合（ &lt;code&gt;prefit&lt;/code&gt; がTrueに設定されている場合）または適合されていない推定量の両方になります。推定器は、フィッティング後に &lt;code&gt;feature_importances_&lt;/code&gt; または &lt;code&gt;coef_&lt;/code&gt; 属性のいずれかを持つ必要があります。</target>
        </trans-unit>
        <trans-unit id="8935e197daa0e6c85ff9f9b0e235723b5ff13893" translate="yes" xml:space="preserve">
          <source>The base estimator from which the transformer is built. This is stored only when a non-fitted estimator is passed to the &lt;code&gt;SelectFromModel&lt;/code&gt;, i.e when prefit is False.</source>
          <target state="translated">トランスが作成されるベース推定量。これは、フィットされていない推定量が &lt;code&gt;SelectFromModel&lt;/code&gt; に渡された場合、つまりprefitがFalseの場合にのみ保存されます。</target>
        </trans-unit>
        <trans-unit id="8062156aa44a8a655e18edffbba0a99e0e098c8a" translate="yes" xml:space="preserve">
          <source>The base estimator to fit on random subsets of the dataset. If None, then the base estimator is a decision tree.</source>
          <target state="translated">データセットのランダムな部分集合に適合する基底推定量。Noneの場合、基底推定器は決定木である。</target>
        </trans-unit>
        <trans-unit id="d80c39c018abc387662fda22711c8b7bb4707717" translate="yes" xml:space="preserve">
          <source>The base kernel</source>
          <target state="translated">ベースカーネル</target>
        </trans-unit>
        <trans-unit id="1b7bc12e1d1eb0bb8e39e15748d7f9b27d93ef1a" translate="yes" xml:space="preserve">
          <source>The basic nearest neighbors classification uses uniform weights: that is, the value assigned to a query point is computed from a simple majority vote of the nearest neighbors. Under some circumstances, it is better to weight the neighbors such that nearer neighbors contribute more to the fit. This can be accomplished through the &lt;code&gt;weights&lt;/code&gt; keyword. The default value, &lt;code&gt;weights = 'uniform'&lt;/code&gt;, assigns uniform weights to each neighbor. &lt;code&gt;weights = 'distance'&lt;/code&gt; assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied to compute the weights.</source>
          <target state="translated">基本的な最近傍分類は均一の重みを使用します。つまり、クエリポイントに割り当てられる値は、最近傍の単純多数決投票から計算されます。状況によっては、近傍により近いものが適合により寄与するように近傍に重みを付ける方が適切です。これは、 &lt;code&gt;weights&lt;/code&gt; キーワードを使用して実行できます。デフォルト値の &lt;code&gt;weights = 'uniform'&lt;/code&gt; 、各近傍に均一の重みを割り当てます。 &lt;code&gt;weights = 'distance'&lt;/code&gt; は、クエリポイントからの距離の逆数に比例する重みを割り当てます。または、距離のユーザー定義関数を指定して、重みを計算することもできます。</target>
        </trans-unit>
        <trans-unit id="650ceb2a1485e5f890a3dcff86c2748037640c3b" translate="yes" xml:space="preserve">
          <source>The basic nearest neighbors regression uses uniform weights: that is, each point in the local neighborhood contributes uniformly to the classification of a query point. Under some circumstances, it can be advantageous to weight points such that nearby points contribute more to the regression than faraway points. This can be accomplished through the &lt;code&gt;weights&lt;/code&gt; keyword. The default value, &lt;code&gt;weights = 'uniform'&lt;/code&gt;, assigns equal weights to all points. &lt;code&gt;weights = 'distance'&lt;/code&gt; assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied, which will be used to compute the weights.</source>
          <target state="translated">基本的な最近傍回帰は、均一の重みを使用します。つまり、ローカル近傍内の各ポイントは、クエリポイントの分類に均一に寄与します。状況によっては、遠くのポイントよりも近くのポイントの方が回帰に寄与するように、ポイントに重みを付けると効果的です。これは、 &lt;code&gt;weights&lt;/code&gt; キーワードを使用して実行できます。デフォルト値の &lt;code&gt;weights = 'uniform'&lt;/code&gt; 、すべてのポイントに等しい重みを割り当てます。 &lt;code&gt;weights = 'distance'&lt;/code&gt; は、クエリポイントからの距離の逆数に比例する重みを割り当てます。または、距離のユーザー定義関数を指定して、重みを計算することもできます。</target>
        </trans-unit>
        <trans-unit id="1cb5ab68855135ab83ff44b066a7bba092358a07" translate="yes" xml:space="preserve">
          <source>The behavior of &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt;&lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt;&lt;/a&gt; is summarized in the following table.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt; &lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt; &lt;/a&gt;の動作を次の表にまとめます。</target>
        </trans-unit>
        <trans-unit id="9c1d95adae9acea07102a8bfe1db513459ebeaee" translate="yes" xml:space="preserve">
          <source>The behavior of the model is very sensitive to the &lt;code&gt;gamma&lt;/code&gt; parameter. If &lt;code&gt;gamma&lt;/code&gt; is too large, the radius of the area of influence of the support vectors only includes the support vector itself and no amount of regularization with &lt;code&gt;C&lt;/code&gt; will be able to prevent overfitting.</source>
          <target state="translated">モデルの動作は、 &lt;code&gt;gamma&lt;/code&gt; パラメーターに非常に敏感です。 &lt;code&gt;gamma&lt;/code&gt; が大きすぎる場合、サポートベクトルの影響範囲の半径にはサポートベクトル自体のみが含まれ、 &lt;code&gt;C&lt;/code&gt; による正則化ではオーバーフィットを防ぐことができません。</target>
        </trans-unit>
        <trans-unit id="2ab99fa4c73a04aa03f4ac31b15851c1ddbe51a1" translate="yes" xml:space="preserve">
          <source>The below plot uses the first two features. See &lt;a href=&quot;https://en.wikipedia.org/wiki/Iris_flower_data_set&quot;&gt;here&lt;/a&gt; for more information on this dataset.</source>
          <target state="translated">以下のプロットは、最初の2つの機能を使用しています。このデータセットの詳細については、&lt;a href=&quot;https://en.wikipedia.org/wiki/Iris_flower_data_set&quot;&gt;こちら&lt;/a&gt;をご覧ください。</target>
        </trans-unit>
        <trans-unit id="4992d67c6e1aff2322ba4c5bb3c9e0c82a7a36cf" translate="yes" xml:space="preserve">
          <source>The best model is selected by cross-validation.</source>
          <target state="translated">クロスバリデーションにより最適なモデルが選択されます。</target>
        </trans-unit>
        <trans-unit id="1a3f42d6835c4f6686d54e99d9f9cd0e3a597fec" translate="yes" xml:space="preserve">
          <source>The best performance is 1 with &lt;code&gt;normalize == True&lt;/code&gt; and the number of samples with &lt;code&gt;normalize == False&lt;/code&gt;.</source>
          <target state="translated">最高のパフォーマンスは、 &lt;code&gt;normalize == True&lt;/code&gt; は1であり、 &lt;code&gt;normalize == False&lt;/code&gt; はサンプル数です。</target>
        </trans-unit>
        <trans-unit id="4b2ab3d16cd4ed6ed82ae137ea7b0ef6d79ff832" translate="yes" xml:space="preserve">
          <source>The best possible p-value is 1/(n_permutations + 1), the worst is 1.0.</source>
          <target state="translated">最高のp値は1/(n_permutations+1)で、最悪は1.0です。</target>
        </trans-unit>
        <trans-unit id="ed9ac623a873633695e926c14de64904b54167f4" translate="yes" xml:space="preserve">
          <source>The best possible score is 1.0, lower values are worse.</source>
          <target state="translated">最高のスコアは1.0で、低い値の方が悪いです。</target>
        </trans-unit>
        <trans-unit id="9230aaef9f0a1fa5e57a82e7f44b36cb5d04d36e" translate="yes" xml:space="preserve">
          <source>The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters.</source>
          <target state="translated">最良値は1、最悪値は-1です。0に近い値は、クラスタが重複していることを示します。</target>
        </trans-unit>
        <trans-unit id="6e2eac60da3011cdc30f86be85be8ffa7e5f1da5" translate="yes" xml:space="preserve">
          <source>The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar.</source>
          <target state="translated">最良値は1、最悪値は-1です。0に近い値は、クラスタが重複していることを示します。負の値は一般的に、サンプルが間違ったクラスタに割り当てられていることを示します。</target>
        </trans-unit>
        <trans-unit id="b4a689b86b5cda51ef6628ef622a0daf756424bd" translate="yes" xml:space="preserve">
          <source>The best value is 1 and the worst value is 0 when &lt;code&gt;adjusted=False&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;adjusted=False&lt;/code&gt; 場合、最適な値は1で、最悪の値は0 です。</target>
        </trans-unit>
        <trans-unit id="488558d8da77f986fd351608404cb6a07bd4fe58" translate="yes" xml:space="preserve">
          <source>The best value is 1 and the worst value is 0.</source>
          <target state="translated">最良値は1、最悪値は0です。</target>
        </trans-unit>
        <trans-unit id="b82813aa45a878f8df07ac95972e2e5116e63bed" translate="yes" xml:space="preserve">
          <source>The bias term in the underlying linear model.</source>
          <target state="translated">基礎となる線形モデルのバイアス項。</target>
        </trans-unit>
        <trans-unit id="d5675ace96b0ccc75bff7a0b67255343283bf5b9" translate="yes" xml:space="preserve">
          <source>The bicluster label of each column.</source>
          <target state="translated">各列の二重クラスタラベル。</target>
        </trans-unit>
        <trans-unit id="d135838b65ddeda2dac8521941211167551d33d9" translate="yes" xml:space="preserve">
          <source>The bicluster label of each row.</source>
          <target state="translated">各行の双曲線ラベル。</target>
        </trans-unit>
        <trans-unit id="4f153639172a63c8dba2ed27e16715aaa100936d" translate="yes" xml:space="preserve">
          <source>The bipartite structure allows for the use of efficient block Gibbs sampling for inference.</source>
          <target state="translated">二分割構造により、効率的なブロックギブスサンプリングを推論に利用することができます。</target>
        </trans-unit>
        <trans-unit id="b104bf424ac624d7987a6846ffafb73fcb6a3323" translate="yes" xml:space="preserve">
          <source>The bounding box for each cluster center when centers are generated at random.</source>
          <target state="translated">中心がランダムに生成されたときの各クラスタ中心の外接箱。</target>
        </trans-unit>
        <trans-unit id="de87510aaa8a4634fe7b670444f1141d0b934e04" translate="yes" xml:space="preserve">
          <source>The breast cancer dataset is a classic and very easy binary classification dataset.</source>
          <target state="translated">乳がんデータセットは、古典的で非常に簡単な二値分類データセットです。</target>
        </trans-unit>
        <trans-unit id="2045b20286b76d60e5fa1f74a1df8fc02a45c66b" translate="yes" xml:space="preserve">
          <source>The brier score loss is also between 0 to 1 and the lower the score (the mean square difference is smaller), the more accurate the prediction is. It can be thought of as a measure of the &amp;ldquo;calibration&amp;rdquo; of a set of probabilistic predictions.</source>
          <target state="translated">ブライアスコアの損失も0から1の間であり、スコアが低い（平均二乗差が小さい）ほど、予測の精度が高くなります。これは、一連の確率的予測の「キャリブレーション」の尺度と考えることができます。</target>
        </trans-unit>
        <trans-unit id="74481417cca6e6adb9f25dfbb8e31b8fc201eb01" translate="yes" xml:space="preserve">
          <source>The callable to use for the inverse transformation. This will be passed the same arguments as inverse transform, with args and kwargs forwarded. If inverse_func is None, then inverse_func will be the identity function.</source>
          <target state="translated">逆変換に使用するコールアブル。これは inverse transform と同じ引数が渡され、args と kwargs が転送されます。inverse_func が None の場合は、inverse_func が ID 関数となります。</target>
        </trans-unit>
        <trans-unit id="81ea3433c521d90153f147dfae64d0df290c9757" translate="yes" xml:space="preserve">
          <source>The callable to use for the transformation. This will be passed the same arguments as transform, with args and kwargs forwarded. If func is None, then func will be the identity function.</source>
          <target state="translated">変換に使用する callable。これはtransformと同じ引数が渡され、argsとkwargsが転送されます。func が None の場合、func は identity 関数になります。</target>
        </trans-unit>
        <trans-unit id="1f38539ea641dd8b50f2f7d85a7f466ba670ba50" translate="yes" xml:space="preserve">
          <source>The categories of each feature determined during fitting (in order of the features in X and corresponding with the output of &lt;code&gt;transform&lt;/code&gt;).</source>
          <target state="translated">フィッティング中に決定された各特徴のカテゴリ（Xの特徴の順序で、 &lt;code&gt;transform&lt;/code&gt; の出力に対応）。</target>
        </trans-unit>
        <trans-unit id="9d4177cfd25fe2d42f6a09b10ba8f700ec03cc45" translate="yes" xml:space="preserve">
          <source>The chi squared kernel is given by</source>
          <target state="translated">カイ二乗カーネルは次式で与えられます。</target>
        </trans-unit>
        <trans-unit id="e654b311ef425d553f0f6705f2dd43fc95968d41" translate="yes" xml:space="preserve">
          <source>The chi squared kernel is most commonly used on histograms (bags) of visual words.</source>
          <target state="translated">カイ二乗カーネルは、視覚的な単語のヒストグラム(袋)で最もよく使われます。</target>
        </trans-unit>
        <trans-unit id="085e153138bf3768d8fc53856c1f1238c6d3caff" translate="yes" xml:space="preserve">
          <source>The chi-squared kernel is a very popular choice for training non-linear SVMs in computer vision applications. It can be computed using &lt;a href=&quot;generated/sklearn.metrics.pairwise.chi2_kernel#sklearn.metrics.pairwise.chi2_kernel&quot;&gt;&lt;code&gt;chi2_kernel&lt;/code&gt;&lt;/a&gt; and then passed to an &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;sklearn.svm.SVC&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;kernel=&quot;precomputed&quot;&lt;/code&gt;:</source>
          <target state="translated">カイ2乗カーネルは、コンピュータービジョンアプリケーションで非線形SVMをトレーニングするための非常に人気のある選択肢です。これは、使用して計算することができ&lt;a href=&quot;generated/sklearn.metrics.pairwise.chi2_kernel#sklearn.metrics.pairwise.chi2_kernel&quot;&gt; &lt;code&gt;chi2_kernel&lt;/code&gt; を&lt;/a&gt;し、その後に渡さ&lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;sklearn.svm.SVC&lt;/code&gt; &lt;/a&gt;で &lt;code&gt;kernel=&quot;precomputed&quot;&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="9233e345f2318b5fc92b12dd745f5adf2df3c33c" translate="yes" xml:space="preserve">
          <source>The chi-squared kernel is computed between each pair of rows in X and Y. X and Y have to be non-negative. This kernel is most commonly applied to histograms.</source>
          <target state="translated">カイ2乗カーネルは,X と Y の各行のペア間で計算されます.このカーネルは,ヒストグラムに最も一般的に適用される.</target>
        </trans-unit>
        <trans-unit id="a8ba729b106653acd0bc97ad998e8dac7b2b635e" translate="yes" xml:space="preserve">
          <source>The chi-squared kernel is given by:</source>
          <target state="translated">カイ二乗カーネルは次式で与えられる。</target>
        </trans-unit>
        <trans-unit id="5ac1548c51d0c4529f9b6250f5d16db6b392dc40" translate="yes" xml:space="preserve">
          <source>The children of each non-leaf node. Values less than &lt;code&gt;n_features&lt;/code&gt; correspond to leaves of the tree which are the original samples. A node &lt;code&gt;i&lt;/code&gt; greater than or equal to &lt;code&gt;n_features&lt;/code&gt; is a non-leaf node and has children &lt;code&gt;children_[i - n_features]&lt;/code&gt;. Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node &lt;code&gt;n_features + i&lt;/code&gt;</source>
          <target state="translated">各非リーフノードの子。 &lt;code&gt;n_features&lt;/code&gt; より小さい値は、元のサンプルであるツリーの葉に対応します。 &lt;code&gt;n_features&lt;/code&gt; 以上のノード &lt;code&gt;i&lt;/code&gt; は非リーフノードであり、子 &lt;code&gt;children_[i - n_features]&lt;/code&gt; ます。あるいは、i番目の反復で、children [i] [0]とchildren [i] [1]がマージされ、ノード &lt;code&gt;n_features + i&lt;/code&gt; 形成されます。</target>
        </trans-unit>
        <trans-unit id="c8989abb78a441b49758b99ec48d8b090a3ce248" translate="yes" xml:space="preserve">
          <source>The children of each non-leaf node. Values less than &lt;code&gt;n_samples&lt;/code&gt; correspond to leaves of the tree which are the original samples. A node &lt;code&gt;i&lt;/code&gt; greater than or equal to &lt;code&gt;n_samples&lt;/code&gt; is a non-leaf node and has children &lt;code&gt;children_[i - n_samples]&lt;/code&gt;. Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node &lt;code&gt;n_samples + i&lt;/code&gt;</source>
          <target state="translated">各非リーフノードの子。 &lt;code&gt;n_samples&lt;/code&gt; より小さい値は、元のサンプルであるツリーの葉に対応します。 &lt;code&gt;n_samples&lt;/code&gt; 以上のノード &lt;code&gt;i&lt;/code&gt; は非リーフノードであり、子 &lt;code&gt;children_[i - n_samples]&lt;/code&gt; ます。あるいは、i番目の反復で、children [i] [0]とchildren [i] [1]がマージされ、ノード &lt;code&gt;n_samples + i&lt;/code&gt; 形成されます。</target>
        </trans-unit>
        <trans-unit id="f3473fe6f2b500963d93290672a7a3ef3f212da2" translate="yes" xml:space="preserve">
          <source>The choice of features is not particularly helpful, but serves to illustrate the technique.</source>
          <target state="translated">特徴の選択は特に参考になりませんが、テクニックを説明するのに役立ちます。</target>
        </trans-unit>
        <trans-unit id="1f460fe2e0e34556a3c6a8a6d017a5f63eeb9ea9" translate="yes" xml:space="preserve">
          <source>The cholesky decomposition of the precision matrices of each mixture component. A precision matrix is the inverse of a covariance matrix. A covariance matrix is symmetric positive definite so the mixture of Gaussian can be equivalently parameterized by the precision matrices. Storing the precision matrices instead of the covariance matrices makes it more efficient to compute the log-likelihood of new samples at test time. The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">各混合成分の精度行列のコレスキー分解。精度行列は、共分散行列の逆です。共分散行列は対称正定行列であるため、ガウス分布の混合は、精度行列によって同等にパラメーター化できます。共分散行列の代わりに精度行列を格納すると、テスト時に新しいサンプルの対数尤度を計算するのがより効率的になります。形状は &lt;code&gt;covariance_type&lt;/code&gt; によって異なります。</target>
        </trans-unit>
        <trans-unit id="54b709e28c0bc302f278f75cf2f9281781fc40ea" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; used with the optional parameter &lt;code&gt;svd_solver='randomized'&lt;/code&gt; is very useful in that case: since we are going to drop most of the singular vectors it is much more efficient to limit the computation to an approximated estimate of the singular vectors we will keep to actually perform the transform.</source>
          <target state="translated">オプションのパラメーター &lt;code&gt;svd_solver='randomized'&lt;/code&gt; で使用されるクラス&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt;は、その場合に非常に役立ちます。特異ベクトルのほとんどをドロップするため、計算を特異ベクトルの近似推定に制限する方がはるかに効率的です。実際に変換を実行し続けます。</target>
        </trans-unit>
        <trans-unit id="9afc238719eb2575519db8e476eb36c39ec071a2" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.feature_extraction.dictvectorizer#sklearn.feature_extraction.DictVectorizer&quot;&gt;&lt;code&gt;DictVectorizer&lt;/code&gt;&lt;/a&gt; can be used to convert feature arrays represented as lists of standard Python &lt;code&gt;dict&lt;/code&gt; objects to the NumPy/SciPy representation used by scikit-learn estimators.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.feature_extraction.dictvectorizer#sklearn.feature_extraction.DictVectorizer&quot;&gt; &lt;code&gt;DictVectorizer&lt;/code&gt; &lt;/a&gt;クラスを使用して、標準のPython &lt;code&gt;dict&lt;/code&gt; オブジェクトのリストとして表される機能配列を、scikit-learn推定器で使用されるNumPy / SciPy表現に変換できます。</target>
        </trans-unit>
        <trans-unit id="0b3d7d58279a2952a288a94db8a135a534d78ec7" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt;&lt;code&gt;FeatureHasher&lt;/code&gt;&lt;/a&gt; is a high-speed, low-memory vectorizer that uses a technique known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_hashing&quot;&gt;feature hashing&lt;/a&gt;, or the &amp;ldquo;hashing trick&amp;rdquo;. Instead of building a hash table of the features encountered in training, as the vectorizers do, instances of &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt;&lt;code&gt;FeatureHasher&lt;/code&gt;&lt;/a&gt; apply a hash function to the features to determine their column index in sample matrices directly. The result is increased speed and reduced memory usage, at the expense of inspectability; the hasher does not remember what the input features looked like and has no &lt;code&gt;inverse_transform&lt;/code&gt; method.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt; &lt;code&gt;FeatureHasher&lt;/code&gt; &lt;/a&gt;クラスは、高速機能、低メモリのベクトライザーであり、&lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_hashing&quot;&gt;特徴ハッシュ&lt;/a&gt;または「ハッシュトリック」と呼ばれる手法を使用します。&lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt; &lt;code&gt;FeatureHasher&lt;/code&gt; &lt;/a&gt;行うように、トレーニングで遭遇した特徴のハッシュテーブルを構築する代わりに、FeatureHasherのインスタンスはハッシュ関数を特徴に適用して、サンプルマトリックスの列インデックスを直接決定します。その結果、検査可能性を犠牲にして、速度が向上し、メモリ使用量が減少します。調理人は、入力機能は同じように見えたし、何も持っていないものを覚えていない &lt;code&gt;inverse_transform&lt;/code&gt; の方法を。</target>
        </trans-unit>
        <trans-unit id="46148e4d160f9510def06597f1921af425f74fcb" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.isotonic.isotonicregression#sklearn.isotonic.IsotonicRegression&quot;&gt;&lt;code&gt;IsotonicRegression&lt;/code&gt;&lt;/a&gt; fits a non-decreasing function to data. It solves the following problem:</source>
          <target state="translated">クラス&lt;a href=&quot;generated/sklearn.isotonic.isotonicregression#sklearn.isotonic.IsotonicRegression&quot;&gt; &lt;code&gt;IsotonicRegression&lt;/code&gt; は&lt;/a&gt;、減少しない関数をデータに適合させます。次の問題を解決します。</target>
        </trans-unit>
        <trans-unit id="04846022f8485d75461ddcb301a4717897728672" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.kernel_approximation.additivechi2sampler#sklearn.kernel_approximation.AdditiveChi2Sampler&quot;&gt;&lt;code&gt;AdditiveChi2Sampler&lt;/code&gt;&lt;/a&gt; implements this component wise deterministic sampling. Each component is sampled \(n\) times, yielding \(2n+1\) dimensions per input dimension (the multiple of two stems from the real and complex part of the Fourier transform). In the literature, \(n\) is usually chosen to be 1 or 2, transforming the dataset to size &lt;code&gt;n_samples * 5 * n_features&lt;/code&gt; (in the case of \(n=2\)).</source>
          <target state="translated">クラス&lt;a href=&quot;generated/sklearn.kernel_approximation.additivechi2sampler#sklearn.kernel_approximation.AdditiveChi2Sampler&quot;&gt; &lt;code&gt;AdditiveChi2Sampler&lt;/code&gt; &lt;/a&gt;は、このコンポーネントごとの決定論的サンプリングを実装します。各コンポーネントは\（n \）回サンプリングされ、入力次元ごとに\（2n + 1 \）次元になります（フーリエ変換の実部と複素部からの2つの倍数）。文献では、\（n \）は通常1または2に選択され、データセットをサイズ &lt;code&gt;n_samples * 5 * n_features&lt;/code&gt; （\（n = 2 \）の場合）。</target>
        </trans-unit>
        <trans-unit id="9fdc0ed638cb0889fa5320ee5ada72b80a16402f" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.elasticnetcv#sklearn.linear_model.ElasticNetCV&quot;&gt;&lt;code&gt;ElasticNetCV&lt;/code&gt;&lt;/a&gt; can be used to set the parameters &lt;code&gt;alpha&lt;/code&gt; (\(\alpha\)) and &lt;code&gt;l1_ratio&lt;/code&gt; (\(\rho\)) by cross-validation.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.elasticnetcv#sklearn.linear_model.ElasticNetCV&quot;&gt; &lt;code&gt;ElasticNetCV&lt;/code&gt; &lt;/a&gt;クラスを使用して、交差検定によりパラメーター &lt;code&gt;alpha&lt;/code&gt; （\（\ alpha \））および &lt;code&gt;l1_ratio&lt;/code&gt; （\（\ rho \））を設定できます。</target>
        </trans-unit>
        <trans-unit id="c89182d0633b09742e745213024a53ccec4fee0a" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnetcv#sklearn.linear_model.MultiTaskElasticNetCV&quot;&gt;&lt;code&gt;MultiTaskElasticNetCV&lt;/code&gt;&lt;/a&gt; can be used to set the parameters &lt;code&gt;alpha&lt;/code&gt; (\(\alpha\)) and &lt;code&gt;l1_ratio&lt;/code&gt; (\(\rho\)) by cross-validation.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnetcv#sklearn.linear_model.MultiTaskElasticNetCV&quot;&gt; &lt;code&gt;MultiTaskElasticNetCV&lt;/code&gt; &lt;/a&gt;クラスを使用して、交差検証によりパラメーター &lt;code&gt;alpha&lt;/code&gt; （\（\ alpha \））および &lt;code&gt;l1_ratio&lt;/code&gt; （\（\ rho \））を設定できます。</target>
        </trans-unit>
        <trans-unit id="6c66ee2c7a53fbda2b2b2da602d1e782a1fcc1b3" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; implements a first-order SGD learning routine. The algorithm iterates over the training examples and for each example updates the model parameters according to the update rule given by</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;クラスは、1次SGD学習ルーチンを実装します。アルゴリズムはトレーニングの例を反復し、各例について、次によって与えられる更新規則に従ってモデルパラメーターを更新します。</target>
        </trans-unit>
        <trans-unit id="f128e57d09cf8b511f002f70c40429b250044323" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties for classification.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;クラスは、分類のためのさまざまな損失関数とペナルティをサポートする単純な確率的勾配降下学習ルーチンを実装します。</target>
        </trans-unit>
        <trans-unit id="9be5b4c1c13de0290ee1bddf8fed519138b844e9" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties to fit linear regression models. &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; is well suited for regression problems with a large number of training samples (&amp;gt; 10.000), for other problems we recommend &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt;&lt;code&gt;Lasso&lt;/code&gt;&lt;/a&gt;, or &lt;a href=&quot;generated/sklearn.linear_model.elasticnet#sklearn.linear_model.ElasticNet&quot;&gt;&lt;code&gt;ElasticNet&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt;クラスは、線形回帰モデルに適合するためのさまざまな損失関数とペナルティをサポートする単純な確率的勾配降下学習ルーチンを実装しています。&lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; は、&lt;/a&gt;他の問題のために私たちがお勧め、よく訓練多数のサンプル（&amp;gt; 10.000）との回帰問題に適している&lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt; &lt;code&gt;Lasso&lt;/code&gt; &lt;/a&gt;、または&lt;a href=&quot;generated/sklearn.linear_model.elasticnet#sklearn.linear_model.ElasticNet&quot;&gt; &lt;code&gt;ElasticNet&lt;/code&gt; を&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="5ca16e4c08c0574f508b6c526beeb2111eade54a" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt;&lt;code&gt;OneClassSVM&lt;/code&gt;&lt;/a&gt; implements a One-Class SVM which is used in outlier detection.</source>
          <target state="translated">クラス&lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt; &lt;code&gt;OneClassSVM&lt;/code&gt; &lt;/a&gt;は、外れ値の検出に使用されるOne-Class SVMを実装します。</target>
        </trans-unit>
        <trans-unit id="5eaffd69df806c5b8353c88358cfd0a49b275168" translate="yes" xml:space="preserve">
          <source>The class label for which the PDPs should be computed. Only if gbrt is a multi-class model. Must be in &lt;code&gt;gbrt.classes_&lt;/code&gt;.</source>
          <target state="translated">PDPを計算する必要があるクラスラベル。gbrtがマルチクラスモデルの場合のみ。 &lt;code&gt;gbrt.classes_&lt;/code&gt; にある必要があります。</target>
        </trans-unit>
        <trans-unit id="2a31ab08188f24bd31e99f218cab5467eddf2ef0" translate="yes" xml:space="preserve">
          <source>The class labels.</source>
          <target state="translated">クラスのラベルです。</target>
        </trans-unit>
        <trans-unit id="075fd1f474c95d9f3895b75ead4b1f9c99be54a8" translate="yes" xml:space="preserve">
          <source>The class log-probabilities of the input samples. The order of the classes corresponds to that in the attribute &lt;code&gt;classes_&lt;/code&gt;.</source>
          <target state="translated">入力サンプルのクラス対数確率。クラスの順序は、属性 &lt;code&gt;classes_&lt;/code&gt; の順序に対応しています。</target>
        </trans-unit>
        <trans-unit id="f9e49144e04860d9a1f4a8512901a503e5d41c6f" translate="yes" xml:space="preserve">
          <source>The class of Matern kernels is a generalization of the RBF and the absolute exponential kernel parameterized by an additional parameter nu. The smaller nu, the less smooth the approximated function is. For nu=inf, the kernel becomes equivalent to the RBF kernel and for nu=0.5 to the absolute exponential kernel. Important intermediate values are nu=1.5 (once differentiable functions) and nu=2.5 (twice differentiable functions).</source>
          <target state="translated">Matern カーネルのクラスは,RBF と絶対指数カーネルの一般化であり,追加のパラメータ nu でパラメータ化されています.nu が小さいほど,近似関数は滑らかではありません.nu=inf の場合,カーネルは RBF カーネルと等価になり,nu=0.5 の場合は絶対指数カーネルと等価になります.重要な中間値は、nu=1.5 (1回微分可能な関数)とnu=2.5 (2回微分可能な関数)です。</target>
        </trans-unit>
        <trans-unit id="e48205f573bda857c27ec3937eb80960daed3556" translate="yes" xml:space="preserve">
          <source>The class ordering is preserved:</source>
          <target state="translated">クラスの並び順は保存されます。</target>
        </trans-unit>
        <trans-unit id="81478ef84d2d645ae9c4e632247f7f3bc9052a92" translate="yes" xml:space="preserve">
          <source>The class probabilities of the input samples. The order of outputs is the same of that of the &lt;code&gt;classes_&lt;/code&gt; attribute.</source>
          <target state="translated">入力サンプルのクラス確率。出力の順序は、 &lt;code&gt;classes_&lt;/code&gt; 属性の順序と同じです。</target>
        </trans-unit>
        <trans-unit id="8ea2403e3ddde25cdaa7213df112f25928f5232d" translate="yes" xml:space="preserve">
          <source>The class probabilities of the input samples. The order of the classes corresponds to that in the attribute &lt;code&gt;classes_&lt;/code&gt;.</source>
          <target state="translated">入力サンプルのクラス確率。クラスの順序は、属性 &lt;code&gt;classes_&lt;/code&gt; の順序に対応しています。</target>
        </trans-unit>
        <trans-unit id="64b22dfb54911895a611d3b9cfbaf3a3f14d97f5" translate="yes" xml:space="preserve">
          <source>The class to report if &lt;code&gt;average='binary'&lt;/code&gt; and the data is binary. If the data are multiclass or multilabel, this will be ignored; setting &lt;code&gt;labels=[pos_label]&lt;/code&gt; and &lt;code&gt;average != 'binary'&lt;/code&gt; will report scores for that label only.</source>
          <target state="translated">&lt;code&gt;average='binary'&lt;/code&gt; でデータがバイナリの場合に報告するクラス。データがマルチクラスまたはマルチラベルの場合、これは無視されます。設定 &lt;code&gt;labels=[pos_label]&lt;/code&gt; と &lt;code&gt;average != 'binary'&lt;/code&gt; だけでラベルのスコアを報告します。</target>
        </trans-unit>
        <trans-unit id="b02b6e7e5cca0ff24c2691f6e7d2bc9c247d17d1" translate="yes" xml:space="preserve">
          <source>The class to use to build the returned adjacency matrix.</source>
          <target state="translated">返された隣接関係行列を構築するために利用されるクラス.</target>
        </trans-unit>
        <trans-unit id="08bf02b9f7a917eed20c9c94c384791c0ed4578f" translate="yes" xml:space="preserve">
          <source>The class with respect to which we perform a one-vs-all fit. If None, then it is assumed that the given problem is binary.</source>
          <target state="translated">一対一の適合を実行するクラス。Noneの場合は、与えられた問題が2値であると仮定します。</target>
        </trans-unit>
        <trans-unit id="26dc8660d1112d77620f09027ce7b9fbee6027a3" translate="yes" xml:space="preserve">
          <source>The classes &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; provide functionality to fit linear models for classification and regression using different (convex) loss functions and different penalties. E.g., with &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt;, &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; fits a logistic regression model, while with &lt;code&gt;loss=&quot;hinge&quot;&lt;/code&gt; it fits a linear support vector machine (SVM).</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;クラスと&lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt;クラスは、さまざまな（凸）損失関数とさまざまなペナルティを使用して、分類と回帰の線形モデルに適合する機能を提供します。たとえば、 &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt; の場合、&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;はロジスティック回帰モデルに適合し、 &lt;code&gt;loss=&quot;hinge&quot;&lt;/code&gt; の場合、線形サポートベクターマシン（SVM）に適合します。</target>
        </trans-unit>
        <trans-unit id="d29e2c71b01d97c9c93dc5aad937ec38e91b7ca9" translate="yes" xml:space="preserve">
          <source>The classes &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; provide two criteria to stop the algorithm when a given level of convergence is reached:</source>
          <target state="translated">クラス&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt;は、特定の収束レベルに達したときにアルゴリズムを停止するための2つの基準を提供します。</target>
        </trans-unit>
        <trans-unit id="d7ef8ada32aed500913b0cf053368e18ea306f33" translate="yes" xml:space="preserve">
          <source>The classes in &lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt;&lt;code&gt;sklearn.neighbors&lt;/code&gt;&lt;/a&gt; can handle either NumPy arrays or &lt;code&gt;scipy.sparse&lt;/code&gt; matrices as input. For dense matrices, a large number of possible distance metrics are supported. For sparse matrices, arbitrary Minkowski metrics are supported for searches.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt; &lt;code&gt;sklearn.neighbors&lt;/code&gt; &lt;/a&gt;のクラスは、NumPy配列または &lt;code&gt;scipy.sparse&lt;/code&gt; 行列を入力として処理できます。密な行列の場合、多数の可能な距離メトリックがサポートされます。スパース行列の場合、任意のミンコフスキーメトリックが検索でサポートされます。</target>
        </trans-unit>
        <trans-unit id="f39d71815c55889dd46cb64855cc1d40f837d2d9" translate="yes" xml:space="preserve">
          <source>The classes in the &lt;a href=&quot;classes#module-sklearn.feature_selection&quot;&gt;&lt;code&gt;sklearn.feature_selection&lt;/code&gt;&lt;/a&gt; module can be used for feature selection/dimensionality reduction on sample sets, either to improve estimators&amp;rsquo; accuracy scores or to boost their performance on very high-dimensional datasets.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.feature_selection&quot;&gt; &lt;code&gt;sklearn.feature_selection&lt;/code&gt; &lt;/a&gt;モジュールのクラスは、推定器の精度スコアを改善するため、または非常に高次元のデータセットでのパフォーマンスを向上させるために、サンプルセットの特徴選択/次元削減に使用できます。</target>
        </trans-unit>
        <trans-unit id="df672af473b055efb339763c1ea56e27edeec8c9" translate="yes" xml:space="preserve">
          <source>The classes in this submodule allow to approximate the embedding \(\phi\), thereby working explicitly with the representations \(\phi(x_i)\), which obviates the need to apply the kernel or store training examples.</source>
          <target state="translated">このサブモジュールのクラスでは、埋め込みを近似することで、表現を明示的に扱うことができ、カーネルの適用や学習例の保存が不要になります。</target>
        </trans-unit>
        <trans-unit id="41647bba96e968e0bd58965ae65f2bc0365d42c6" translate="yes" xml:space="preserve">
          <source>The classes labels (single output problem), or a list of arrays of class labels (multi-output problem).</source>
          <target state="translated">クラスのラベル(単一出力問題)、またはクラスのラベルの配列のリスト(複数出力問題)。</target>
        </trans-unit>
        <trans-unit id="772daeb9b0eaa408b264b0fa548b1cbd24779805" translate="yes" xml:space="preserve">
          <source>The classes labels.</source>
          <target state="translated">クラスのラベル。</target>
        </trans-unit>
        <trans-unit id="13f18ce429d97546a8b16cf132e8e6c03147cb50" translate="yes" xml:space="preserve">
          <source>The classic implementation of the clustering method based on the Lloyd&amp;rsquo;s algorithm. It consumes the whole set of input data at each iteration.</source>
          <target state="translated">ロイドのアルゴリズムに基づくクラスタリング手法の古典的な実装。各反復で入力データのセット全体を消費します。</target>
        </trans-unit>
        <trans-unit id="4b0c76ea0ae326d19f535ab9520b9dd024cf9124" translate="yes" xml:space="preserve">
          <source>The classification is performed by projecting to the first two principal components found by PCA and CCA for visualisation purposes, followed by using the &lt;a href=&quot;../modules/generated/sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt;&lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt;&lt;/a&gt; metaclassifier using two SVCs with linear kernels to learn a discriminative model for each class. Note that PCA is used to perform an unsupervised dimensionality reduction, while CCA is used to perform a supervised one.</source>
          <target state="translated">分類は、PCAとCCAが見つけた最初の2つの主成分に投影して視覚化し、その後、&lt;a href=&quot;../modules/generated/sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt; &lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt; メタ分類子&lt;/a&gt;を使用して、線形カーネルを持つ2つのSVCを使用し、各クラスの判別モデルを学習します。PCAは教師なし次元削減を実行するために使用され、CCAは教師付き次元削減を実行するために使用されることに注意してください。</target>
        </trans-unit>
        <trans-unit id="d9bcc8debd316bfd1c4b9e73630f6ba7215b9e3d" translate="yes" xml:space="preserve">
          <source>The classifier whose output decision function needs to be calibrated to offer more accurate predict_proba outputs. If cv=prefit, the classifier must have been fit already on data.</source>
          <target state="translated">より正確な predict_proba 出力を提供するために,出力決定関数を校正する必要がある分類器.cv=prefit の場合,その分類器はすでにデータに適合していなければいけません.</target>
        </trans-unit>
        <trans-unit id="88e39a3dfd7087282361f3b39109354f7ff32ede" translate="yes" xml:space="preserve">
          <source>The code below also illustrates how the construction and the computation of the predictions can be parallelized within multiple jobs.</source>
          <target state="translated">以下のコードは、複数のジョブ内で予測の構築と計算を並列化する方法を示しています。</target>
        </trans-unit>
        <trans-unit id="6a1cc4b5491e9b42fd445850086762fcb248e836" translate="yes" xml:space="preserve">
          <source>The code below plots the dependency of y against individual x_i and normalized values of univariate F-tests statistics and mutual information.</source>
          <target state="translated">以下のコードは、個々のx_iに対するyの依存性と、一変量F検定統計量と相互情報の正規化された値をプロットしています。</target>
        </trans-unit>
        <trans-unit id="4b0a65eb61a5277b5e28aef5c3b0e7ea577bdc07" translate="yes" xml:space="preserve">
          <source>The code-examples in the above tutorials are written in a &lt;em&gt;python-console&lt;/em&gt; format. If you wish to easily execute these examples in &lt;strong&gt;IPython&lt;/strong&gt;, use:</source>
          <target state="translated">上記のチュートリアルのコード例は、&lt;em&gt;python-console&lt;/em&gt;形式で記述されています。これらの例を&lt;strong&gt;IPython&lt;/strong&gt;で簡単に実行したい場合は、以下を使用します。</target>
        </trans-unit>
        <trans-unit id="ce298b790cf6df954f5ce5058b04debe04deb707" translate="yes" xml:space="preserve">
          <source>The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the regression sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.</source>
          <target state="translated">係数 R^2 は,(1-u/v)として定義され,ここで u は残差平方和((y_true-y_pred)**2).sum()であり,v は回帰平方和((y_true-y_true.mean())である.**2).sum()。可能な最高スコアは1.0で,負の値にすることができる(モデルが任意に悪くなる可能性があるため).入力特徴量を無視して常にyの期待値を予測する定数モデルは,R^2スコア0.0を得ます.</target>
        </trans-unit>
        <trans-unit id="48121e6567bc65fc6ea38acd7d461ac73e2ea472" translate="yes" xml:space="preserve">
          <source>The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.</source>
          <target state="translated">係数R^2は,(1-u/v)として定義され,ここで u は2乗の残差総和((y_true-y_pred)**2).sum()であり,v は2乗の総和((y_true-y_true.mean())である.**2).sum()となります。可能な最高のスコアは1.0で,負の値になることがある(モデルが任意に悪くなることがあるため).入力特徴量を無視して常にyの期待値を予測する定数モデルは,R^2スコア0.0を得ます.</target>
        </trans-unit>
        <trans-unit id="f0dabbd241b37afcd99edc97579580c22f2703f9" translate="yes" xml:space="preserve">
          <source>The coefficient of the underlying linear model. It is returned only if coef is True.</source>
          <target state="translated">基礎となる線形モデルの係数。coefがTrueの場合のみ返されます。</target>
        </trans-unit>
        <trans-unit id="b2933da174c8fa332cc9ad841c0c25a1ab1996d2" translate="yes" xml:space="preserve">
          <source>The coefficients can be forced to be positive.</source>
          <target state="translated">係数は強制的に正の値にすることができる。</target>
        </trans-unit>
        <trans-unit id="3c89d0ba57b5481659a50cc73b873694c355fd9b" translate="yes" xml:space="preserve">
          <source>The coefficients of the linear model: &lt;code&gt;Y = X coef_ + Err&lt;/code&gt;</source>
          <target state="translated">線形モデルの係数： &lt;code&gt;Y = X coef_ + Err&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="a7757730afe97cb8593dea4757d82727872c7529" translate="yes" xml:space="preserve">
          <source>The coefficients, the residual sum of squares and the variance score are also calculated.</source>
          <target state="translated">係数、二乗の残差和、分散スコアも計算されます。</target>
        </trans-unit>
        <trans-unit id="f15e1f2c7fb96cd43c32056a6c407c9c1c570883" translate="yes" xml:space="preserve">
          <source>The collection of fitted base estimators.</source>
          <target state="translated">フィットした基底推定器のコレクション。</target>
        </trans-unit>
        <trans-unit id="e97d4f2642ef3372648a38fef3350e771647c3d6" translate="yes" xml:space="preserve">
          <source>The collection of fitted sub-estimators as defined in &lt;code&gt;estimators&lt;/code&gt; that are not &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;None&lt;/code&gt; ではない、 &lt;code&gt;estimators&lt;/code&gt; で定義されている、当てはめられたサブ推定器のコレクション。</target>
        </trans-unit>
        <trans-unit id="0b6e390487ac628e8895818624f2b32fd0c95d36" translate="yes" xml:space="preserve">
          <source>The collection of fitted sub-estimators.</source>
          <target state="translated">フィットしたサブエスティメーターのコレクション。</target>
        </trans-unit>
        <trans-unit id="929cef17b33a8aadbb4cff8f87d4813d9e794dca" translate="yes" xml:space="preserve">
          <source>The collection of fitted sub-estimators. &lt;code&gt;loss_.K&lt;/code&gt; is 1 for binary classification, otherwise n_classes.</source>
          <target state="translated">適合したサブ推定量のコレクション。 &lt;code&gt;loss_.K&lt;/code&gt; は、バイナリ分類の場合は1、それ以外の場合はn_classesです。</target>
        </trans-unit>
        <trans-unit id="34213cac4f6d6096a4a72655d4c46d8da28f907c" translate="yes" xml:space="preserve">
          <source>The collection of fitted transformers as tuples of (name, fitted_transformer, column). &lt;code&gt;fitted_transformer&lt;/code&gt; can be an estimator, &amp;lsquo;drop&amp;rsquo;, or &amp;lsquo;passthrough&amp;rsquo;. In case there were no columns selected, this will be the unfitted transformer. If there are remaining columns, the final element is a tuple of the form: (&amp;lsquo;remainder&amp;rsquo;, transformer, remaining_columns) corresponding to the &lt;code&gt;remainder&lt;/code&gt; parameter. If there are remaining columns, then &lt;code&gt;len(transformers_)==len(transformers)+1&lt;/code&gt;, otherwise &lt;code&gt;len(transformers_)==len(transformers)&lt;/code&gt;.</source>
          <target state="translated">（name、fitted_transformer、column）のタプルとしてのフィットしたトランスフォーマーのコレクション。 &lt;code&gt;fitted_transformer&lt;/code&gt; は、推定器、 'drop'、または 'passthrough'にすることができます。カラムが選択されていない場合、これは未適合のトランスになります。残りの列がある場合、最後の要素は、 &lt;code&gt;remainder&lt;/code&gt; パラメーターに対応する次の形式のタプルです：（ 'remainder'、トランスフォーマー、maining_columns）。列が残っている場合は &lt;code&gt;len(transformers_)==len(transformers)+1&lt;/code&gt; 、それ以外の場合は &lt;code&gt;len(transformers_)==len(transformers)&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="d6d471d12278a874c4225d83bbd25d7f04a0a976" translate="yes" xml:space="preserve">
          <source>The color map illustrates the decision function learned by the SVC.</source>
          <target state="translated">カラーマップは、SVCが学習した決定関数を示しています。</target>
        </trans-unit>
        <trans-unit id="093f5986f8d055d5b47f11ad021ce6ecfcef91e9" translate="yes" xml:space="preserve">
          <source>The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]] gives the indicator value for the i-th estimator.</source>
          <target state="translated">indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]の列は、i番目の推定量の指標値を与える。</target>
        </trans-unit>
        <trans-unit id="f0cca3e664aa52c0e892615c12365c0a6af40452" translate="yes" xml:space="preserve">
          <source>The combination used in this example is not particularly helpful on this dataset and is only used to illustrate the usage of FeatureUnion.</source>
          <target state="translated">この例で使用されている組み合わせは、このデータセットでは特に有用ではなく、FeatureUnionの使用法を説明するためだけに使用されています。</target>
        </trans-unit>
        <trans-unit id="4524a6f67fb21e2d5fb712c219044b7460dbad0e" translate="yes" xml:space="preserve">
          <source>The components of the random matrix are drawn from N(0, 1 / n_components).</source>
          <target state="translated">ランダム行列の成分は N(0,1/n_components)から引き出されます。</target>
        </trans-unit>
        <trans-unit id="4d179dd17ec2e9e8ac79ee8fb26a8b8727033655" translate="yes" xml:space="preserve">
          <source>The compromise between l1 and l2 penalization chosen by cross validation</source>
          <target state="translated">クロスバリデーションによって選択されたl1とl2のペナルティの間の妥協点</target>
        </trans-unit>
        <trans-unit id="37c8e8aa42af430070b2b87be4c5db5db294cc7b" translate="yes" xml:space="preserve">
          <source>The computation during &lt;code&gt;fit&lt;/code&gt; is:</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; 中の計算は次のとおりです。</target>
        </trans-unit>
        <trans-unit id="cc728704a5b1deaa221b61ada3dd42ad8e661fbd" translate="yes" xml:space="preserve">
          <source>The computation during &lt;code&gt;predict&lt;/code&gt; is:</source>
          <target state="translated">&lt;code&gt;predict&lt;/code&gt; 中の計算は次のとおりです。</target>
        </trans-unit>
        <trans-unit id="cc5659751ed96e6a9a511b247760eba7bd9fab63" translate="yes" xml:space="preserve">
          <source>The computation of Davies-Bouldin is simpler than that of Silhouette scores.</source>
          <target state="translated">ダヴィース=ボルダンの計算は、シルエットのスコアよりも簡単です。</target>
        </trans-unit>
        <trans-unit id="d526bd934b93818cdf8a3fccada8e6b2b34d84f2" translate="yes" xml:space="preserve">
          <source>The computational overhead of each SVD is &lt;code&gt;O(batch_size * n_features ** 2)&lt;/code&gt;, but only 2 * batch_size samples remain in memory at a time. There will be &lt;code&gt;n_samples / batch_size&lt;/code&gt; SVD computations to get the principal components, versus 1 large SVD of complexity &lt;code&gt;O(n_samples * n_features ** 2)&lt;/code&gt; for PCA.</source>
          <target state="translated">各SVDの計算オーバーヘッドは &lt;code&gt;O(batch_size * n_features ** 2)&lt;/code&gt; ですが、一度にメモリに残るのは2 * batch_sizeサンプルのみです。PCAの場合、複雑さ &lt;code&gt;O(n_samples * n_features ** 2)&lt;/code&gt; 1つの大きなSVDに対して、主成分を取得するための &lt;code&gt;n_samples / batch_size&lt;/code&gt; SVD計算があります。</target>
        </trans-unit>
        <trans-unit id="2fa163515de812b4d78a2bbcef87e239a36ae4d1" translate="yes" xml:space="preserve">
          <source>The concept of early stopping is simple. We specify a &lt;code&gt;validation_fraction&lt;/code&gt; which denotes the fraction of the whole dataset that will be kept aside from training to assess the validation loss of the model. The gradient boosting model is trained using the training set and evaluated using the validation set. When each additional stage of regression tree is added, the validation set is used to score the model. This is continued until the scores of the model in the last &lt;code&gt;n_iter_no_change&lt;/code&gt; stages do not improve by atleast &lt;code&gt;tol&lt;/code&gt;. After that the model is considered to have converged and further addition of stages is &amp;ldquo;stopped early&amp;rdquo;.</source>
          <target state="translated">早期停止のコンセプトは簡単です。モデルの検証損失を評価するためのトレーニングは別として、データセット全体の割合を示す &lt;code&gt;validation_fraction&lt;/code&gt; を指定します。勾配ブースティングモデルは、トレーニングセットを使用してトレーニングされ、検証セットを使用して評価されます。回帰ツリーの各ステージが追加されると、検証セットを使用してモデルにスコアが付けられます。これは、最後の &lt;code&gt;n_iter_no_change&lt;/code&gt; ステージでのモデルのスコアが少なくとも &lt;code&gt;tol&lt;/code&gt; によって改善されなくなるまで継続されます。その後、モデルは収束したと見なされ、ステージの追加は「早期に停止」されます。</target>
        </trans-unit>
        <trans-unit id="feb60f640c62fd0856cb50e8401267d4f55d4774" translate="yes" xml:space="preserve">
          <source>The concrete &lt;code&gt;LossFunction&lt;/code&gt; object.</source>
          <target state="translated">具体的な &lt;code&gt;LossFunction&lt;/code&gt; オブジェクト。</target>
        </trans-unit>
        <trans-unit id="4f21e1340272b60cf06a69153cc19d0140d625cf" translate="yes" xml:space="preserve">
          <source>The concrete loss function can be set via the &lt;code&gt;loss&lt;/code&gt; parameter. &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; supports the following loss functions:</source>
          <target state="translated">具体的な損失関数は、 &lt;code&gt;loss&lt;/code&gt; パラメーターを介して設定できます。&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;は、次の損失関数をサポートしています。</target>
        </trans-unit>
        <trans-unit id="c24a50928ec0729629e45e8b53b6d8b9f0112f79" translate="yes" xml:space="preserve">
          <source>The concrete loss function can be set via the &lt;code&gt;loss&lt;/code&gt; parameter. &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; supports the following loss functions:</source>
          <target state="translated">具体的な損失関数は、 &lt;code&gt;loss&lt;/code&gt; パラメーターを介して設定できます。&lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt;は、次の損失関数をサポートしています。</target>
        </trans-unit>
        <trans-unit id="a5b98bb304bc3ec6a422167859a2f5da5580fa47" translate="yes" xml:space="preserve">
          <source>The concrete penalty can be set via the &lt;code&gt;penalty&lt;/code&gt; parameter. SGD supports the following penalties:</source>
          <target state="translated">具体的なペナルティは、 &lt;code&gt;penalty&lt;/code&gt; パラメータを介して設定できます。SGDは次のペナルティをサポートしています。</target>
        </trans-unit>
        <trans-unit id="c0c66ccf788e88932593e02308e1dcae0bb5c1f0" translate="yes" xml:space="preserve">
          <source>The conditional probability distribution of each unit is given by the logistic sigmoid activation function of the input it receives:</source>
          <target state="translated">各ユニットの条件付き確率分布は、それが受け取る入力のロジスティック・シグモイド活性化関数で与えられる。</target>
        </trans-unit>
        <trans-unit id="6e94f7f28c2cc9bab8c93a85a4438b802d82d10d" translate="yes" xml:space="preserve">
          <source>The confidence score for a sample is the signed distance of that sample to the hyperplane.</source>
          <target state="translated">標本の信頼度スコアは、その標本の超平面までの符号付き距離です。</target>
        </trans-unit>
        <trans-unit id="b129f98741f30e6ae05e0872d86b02b5a921e905" translate="yes" xml:space="preserve">
          <source>The connectivity constraints are imposed via an connectivity matrix: a scipy sparse matrix that has elements only at the intersection of a row and a column with indices of the dataset that should be connected. This matrix can be constructed from a-priori information: for instance, you may wish to cluster web pages by only merging pages with a link pointing from one to another. It can also be learned from the data, for instance using &lt;a href=&quot;generated/sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt;&lt;code&gt;sklearn.neighbors.kneighbors_graph&lt;/code&gt;&lt;/a&gt; to restrict merging to nearest neighbors as in &lt;a href=&quot;../auto_examples/cluster/plot_agglomerative_clustering#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-py&quot;&gt;this example&lt;/a&gt;, or using &lt;a href=&quot;generated/sklearn.feature_extraction.image.grid_to_graph#sklearn.feature_extraction.image.grid_to_graph&quot;&gt;&lt;code&gt;sklearn.feature_extraction.image.grid_to_graph&lt;/code&gt;&lt;/a&gt; to enable only merging of neighboring pixels on an image, as in the &lt;a href=&quot;../auto_examples/cluster/plot_coin_ward_segmentation#sphx-glr-auto-examples-cluster-plot-coin-ward-segmentation-py&quot;&gt;coin&lt;/a&gt; example.</source>
          <target state="translated">接続性制約は、接続性マトリックスを介して課せられます。接続するデータセットのインデックスを持つ行と列の交点にのみ要素を持つscipyスパースマトリックス。このマトリックスは、事前情報から構築できます。たとえば、あるページから別のページにリンクしているページのみをマージすることで、Webページをクラスター化したい場合があります。また、データから学習することもできます。&lt;a href=&quot;../auto_examples/cluster/plot_agglomerative_clustering#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-py&quot;&gt;たとえば&lt;/a&gt;、この例のように&lt;a href=&quot;generated/sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt; &lt;code&gt;sklearn.neighbors.kneighbors_graph&lt;/code&gt; &lt;/a&gt;を使用してマージを最近傍に制限するか、&lt;a href=&quot;generated/sklearn.feature_extraction.image.grid_to_graph#sklearn.feature_extraction.image.grid_to_graph&quot;&gt; &lt;code&gt;sklearn.feature_extraction.image.grid_to_graph&lt;/code&gt; &lt;/a&gt;ようにsklearn.feature_extraction.image.grid_to_graphを使用して画像の隣接ピクセルのみのマージを有効にします。&lt;a href=&quot;../auto_examples/cluster/plot_coin_ward_segmentation#sphx-glr-auto-examples-cluster-plot-coin-ward-segmentation-py&quot;&gt;コイン&lt;/a&gt;例。</target>
        </trans-unit>
        <trans-unit id="cba08514635a6da99fcbf046720afd85a65680ea" translate="yes" xml:space="preserve">
          <source>The constant value which defines the covariance: k(x_1, x_2) = constant_value</source>
          <target state="translated">共分散を定義する定数値:k(x_1,x_2)=constant_value</target>
        </trans-unit>
        <trans-unit id="83836055dd86bf18d0180179c00f49511034950c" translate="yes" xml:space="preserve">
          <source>The contingency table calculated is typically utilized in the calculation of a similarity statistic (like the others listed in this document) between the two clusterings.</source>
          <target state="translated">計算された分割表は、典型的には、2つのクラスタリング間の類似度統計量(この文書に記載されている他のものと同様)の計算に利用される。</target>
        </trans-unit>
        <trans-unit id="ca0505a0687635a00a190438e7d26956e48c468c" translate="yes" xml:space="preserve">
          <source>The convergence threshold. EM iterations will stop when the lower bound average gain is below this threshold.</source>
          <target state="translated">収束のしきい値。下界平均利得がこの閾値を下回るとEMの反復は停止します。</target>
        </trans-unit>
        <trans-unit id="0e26ef32f94bda64e8f6e396c3c303a3311ac4e1" translate="yes" xml:space="preserve">
          <source>The convergence threshold. EM iterations will stop when the lower bound average gain on the likelihood (of the training data with respect to the model) is below this threshold.</source>
          <target state="translated">収束閾値。(モデルに対する訓練データの)尤度の下界平均利得がこの閾値を下回るとEMの反復は停止する.</target>
        </trans-unit>
        <trans-unit id="80f05388942910150044c6d47584d8bddc73ef4c" translate="yes" xml:space="preserve">
          <source>The converse mapping from feature name to column index is stored in the &lt;code&gt;vocabulary_&lt;/code&gt; attribute of the vectorizer:</source>
          <target state="translated">機能名から列インデックスへの逆マッピングは、ベクトライザーの &lt;code&gt;vocabulary_&lt;/code&gt; 属性に保存されます。</target>
        </trans-unit>
        <trans-unit id="42e30f7491cd0bbd1be9eaa1008d97761b319a5f" translate="yes" xml:space="preserve">
          <source>The converted and validated X.</source>
          <target state="translated">変換されて検証されたX。</target>
        </trans-unit>
        <trans-unit id="043f2ec629cc510b9c1127fe6b10b6fe4448d241" translate="yes" xml:space="preserve">
          <source>The converted and validated y.</source>
          <target state="translated">変換されて検証されたy。</target>
        </trans-unit>
        <trans-unit id="81a7bc326e697c66d8802c043c85fabeabbf241b" translate="yes" xml:space="preserve">
          <source>The converted dataname.</source>
          <target state="translated">変換されたデータ名。</target>
        </trans-unit>
        <trans-unit id="e11d4d7608fe1fa28c8a673b1389ac6181cb7877" translate="yes" xml:space="preserve">
          <source>The core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. The data modifications at each so-called boosting iteration consist of applying weights \(w_1\), \(w_2\), &amp;hellip;, \(w_N\) to each of the training samples. Initially, those weights are all set to \(w_i = 1/N\), so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence &lt;a href=&quot;#htf&quot; id=&quot;id10&quot;&gt;[HTF]&lt;/a&gt;.</source>
          <target state="translated">AdaBoostの中核となる原則は、一連の弱学習器（つまり、小さな決定木などのランダムな推測よりもほんの少しだけ優れているモデル）を繰り返し変更されたデータのバージョンに当てはめることです。それらすべての予測は、加重多数決（または合計）を介して結合され、最終的な予測が生成されます。いわゆるブースティング反復ごとのデータ変更は、各トレーニングサンプルに重み\（w_1 \）、\（w_2 \）、&amp;hellip;、\（w_N \）を適用することで構成されます。最初は、これらの重みはすべて\（w_i = 1 / N \）に設定されているため、最初のステップでは元のデータに対して弱学習器を単にトレーニングします。連続する反復ごとに、サンプルの重みが個別に変更され、再重み付けされたデータに学習アルゴリズムが再適用されます。特定のステップで、前のステップで生成されたブーストされたモデルによって誤って予測されたトレーニング例は、重みが増加しますが、正しく予測されたものは減少します。反復が進むにつれて、予測が困難な例は、ますます大きな影響を受けます。これにより、後続の各弱学習器は、シーケンス内の前の学習器で見逃された例に集中することを強いられます&lt;a href=&quot;#htf&quot; id=&quot;id10&quot;&gt;[HTF]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="81757ae28a30627b83ded3cb76d108f655541caf" translate="yes" xml:space="preserve">
          <source>The correlation between each regressor and the target is computed, that is, ((X[:, i] - mean(X[:, i])) * (y - mean_y)) / (std(X[:, i]) * std(y)).</source>
          <target state="translated">各回帰子と目標の間の相関が計算され,すなわち,((X[:,i]-平均(X[:,i]))*(y-mean_y)/(std(X[:,i])*std(y))。</target>
        </trans-unit>
        <trans-unit id="302d596f93db30183d7b9abc1999d9cfb95f2d1f" translate="yes" xml:space="preserve">
          <source>The corresponding image is:</source>
          <target state="translated">対応する画像は</target>
        </trans-unit>
        <trans-unit id="8f6395cc147644bb3051e08a46acbc9cd47145af" translate="yes" xml:space="preserve">
          <source>The cosine distance is defined as &lt;code&gt;1 - cosine_similarity&lt;/code&gt;: the lowest value is 0 (identical point) but it is bounded above by 2 for the farthest points. Its value does not depend on the norm of the vector points but only on their relative angles.</source>
          <target state="translated">コサイン距離は &lt;code&gt;1 - cosine_similarity&lt;/code&gt; として定義されます。最も低い値は0（同一のポイント）ですが、最も遠いポイントの場合、上限は2になります。その値は、ベクトルポイントのノルムには依存せず、相対角度にのみ依存します。</target>
        </trans-unit>
        <trans-unit id="c4b95b481d689de7dde58522ad60a16deab9f841" translate="yes" xml:space="preserve">
          <source>The cosine distance is equivalent to the half the squared euclidean distance if each sample is normalized to unit norm</source>
          <target state="translated">余弦距離は、各サンプルを単位ノルムに正規化した場合の2乗ユークリッド距離の半分に相当します。</target>
        </trans-unit>
        <trans-unit id="de99d6d126cf03f253c85bcda8fea4fdd47045eb" translate="yes" xml:space="preserve">
          <source>The cost function of an isomap embedding is</source>
          <target state="translated">アイソマップ埋め込みのコスト関数は</target>
        </trans-unit>
        <trans-unit id="40200d49debb9a752670d9f4e91dc77cd9d66f0a" translate="yes" xml:space="preserve">
          <source>The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.</source>
          <target state="translated">木を使用するコスト(すなわち、データを予測するコスト)は、木を訓練するために使用されるデータポイントの数で対数的になります。</target>
        </trans-unit>
        <trans-unit id="c9b6813659bffb0c08aeb974fcecdaaf97e786b0" translate="yes" xml:space="preserve">
          <source>The covariance matrix of a data set is known to be well approximated by the classical &lt;em&gt;maximum likelihood estimator&lt;/em&gt; (or &amp;ldquo;empirical covariance&amp;rdquo;), provided the number of observations is large enough compared to the number of features (the variables describing the observations). More precisely, the Maximum Likelihood Estimator of a sample is an unbiased estimator of the corresponding population&amp;rsquo;s covariance matrix.</source>
          <target state="translated">データセットの共分散行列は、観測数が特徴（観測を記述する変数）の数と比較して十分に大きい&lt;em&gt;場合&lt;/em&gt;、古典的な&lt;em&gt;最尤推定量&lt;/em&gt;（または「経験的共分散」）によってよく近似されることが知られています。より正確には、サンプルの最尤推定量は、対応する母集団の共分散行列の不偏推定量です。</target>
        </trans-unit>
        <trans-unit id="ff9747571dfcab155502f0d51c20ced0a4eda87c" translate="yes" xml:space="preserve">
          <source>The covariance matrix will be this value times the unit matrix. This dataset only produces symmetric normal distributions.</source>
          <target state="translated">共分散行列は,この値に単位行列をかけたものになります.このデータセットは,対称正規分布のみを生成する.</target>
        </trans-unit>
        <trans-unit id="d4f25f1f739fb6455b136c8d6d48a91032e89970" translate="yes" xml:space="preserve">
          <source>The covariance of each mixture component. The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">各混合コンポーネントの共分散。形状は &lt;code&gt;covariance_type&lt;/code&gt; によって異なります。</target>
        </trans-unit>
        <trans-unit id="37e0aa90ea88b46d75d1f0f3b55b29aae27815f8" translate="yes" xml:space="preserve">
          <source>The covariance to compare with.</source>
          <target state="translated">比較する共分散。</target>
        </trans-unit>
        <trans-unit id="a8664fdb20de58ef24fb2b254f60a073e2dad9f4" translate="yes" xml:space="preserve">
          <source>The cross decomposition module contains two main families of algorithms: the partial least squares (PLS) and the canonical correlation analysis (CCA).</source>
          <target state="translated">クロス分解モジュールは、部分最小二乗法(PLS)と正準相関分析(CCA)の2つの主要なアルゴリズムを含んでいます。</target>
        </trans-unit>
        <trans-unit id="5725093c72b0120c682574584b056d0b295efdd9" translate="yes" xml:space="preserve">
          <source>The cross validation score obtained on the training data</source>
          <target state="translated">学習データに対して得られたクロスバリデーションスコア</target>
        </trans-unit>
        <trans-unit id="7d9ee6d7f0b44964f013a45e604e6c29cc8a3f8f" translate="yes" xml:space="preserve">
          <source>The cross-validation can then be performed easily:</source>
          <target state="translated">その後、クロスバリデーションを簡単に行うことができます。</target>
        </trans-unit>
        <trans-unit id="b91dc81955b1dbc1a750c7c0ae1e516f1cb5789e" translate="yes" xml:space="preserve">
          <source>The cross-validation score can be directly calculated using the &lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; helper. Given an estimator, the cross-validation object and the input dataset, the &lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; splits the data repeatedly into a training and a testing set, trains the estimator using the training set and computes the scores based on the testing set for each iteration of cross-validation.</source>
          <target state="translated">交差検証スコアは、&lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt;ヘルパーを使用して直接計算できます。推定器、交差検証オブジェクト、および入力データセットを指定すると、&lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt;はデータを繰り返しトレーニングとテストセットに分割し、トレーニングセットを使用して推定器をトレーニングし、クロスの各反復のテストセットに基づいてスコアを計算します検証。</target>
        </trans-unit>
        <trans-unit id="0c9e67494c034c3aceceb77302443d5c2bb54301" translate="yes" xml:space="preserve">
          <source>The cross-validation scores such that &lt;code&gt;grid_scores_[i]&lt;/code&gt; corresponds to the CV score of the i-th subset of features.</source>
          <target state="translated">&lt;code&gt;grid_scores_[i]&lt;/code&gt; が特徴のi番目のサブセットのCVスコアに対応するような交差検証スコア。</target>
        </trans-unit>
        <trans-unit id="7186db76e260a93ceff6e7a012a18028bce16f5b" translate="yes" xml:space="preserve">
          <source>The current implementation uses ball trees and kd-trees to determine the neighborhood of points, which avoids calculating the full distance matrix (as was done in scikit-learn versions before 0.14). The possibility to use custom metrics is retained; for details, see &lt;code&gt;NearestNeighbors&lt;/code&gt;.</source>
          <target state="translated">現在の実装では、ボールツリーとkdツリーを使用してポイントの近傍を決定するため、（0.14より前のscikit-learnバージョンで行われていた）完全な距離行列の計算が回避されます。カスタム指標を使用する可能性は保持されます。詳細については、 &lt;code&gt;NearestNeighbors&lt;/code&gt; を参照してください。</target>
        </trans-unit>
        <trans-unit id="e3ec89890503565d6b8dae28b11789379133ea78" translate="yes" xml:space="preserve">
          <source>The current loss computed with the loss function.</source>
          <target state="translated">損失関数を用いて計算された現在の損失。</target>
        </trans-unit>
        <trans-unit id="a415ef6f77731ce51df5a94c5015135ebd3f462c" translate="yes" xml:space="preserve">
          <source>The curse of dimensionality</source>
          <target state="translated">次元の呪い</target>
        </trans-unit>
        <trans-unit id="3eaad00bc0bba0577db9b826b646317a24a90409" translate="yes" xml:space="preserve">
          <source>The data</source>
          <target state="translated">データ</target>
        </trans-unit>
        <trans-unit id="973622d96c176a0a77b0dde8f6175466c5afec2c" translate="yes" xml:space="preserve">
          <source>The data is always a 2D array, shape &lt;code&gt;(n_samples, n_features)&lt;/code&gt;, although the original data may have had a different shape. In the case of the digits, each original sample is an image of shape &lt;code&gt;(8, 8)&lt;/code&gt; and can be accessed using:</source>
          <target state="translated">データは常に2D配列の形状 &lt;code&gt;(n_samples, n_features)&lt;/code&gt; ですが、元のデータは異なる形状であった可能性があります。数字の場合、各元のサンプルは形状 &lt;code&gt;(8, 8)&lt;/code&gt; 8、8）の画像であり、次の方法でアクセスできます。</target>
        </trans-unit>
        <trans-unit id="0e6af14908ee7619e613da95e5ef5e8434ef93d9" translate="yes" xml:space="preserve">
          <source>The data is assumed to be non-negative, and is often normalized to have an L1-norm of one. The normalization is rationalized with the connection to the chi squared distance, which is a distance between discrete probability distributions.</source>
          <target state="translated">データは非負であると仮定され、しばしばL1ノルムが1になるように正規化される。正規化は,離散確率分布間の距離であるカイ2乗距離との関係で合理化される.</target>
        </trans-unit>
        <trans-unit id="aa2161d468d436ce31e1b1a4a535d19d4acf7995" translate="yes" xml:space="preserve">
          <source>The data is generated with the &lt;code&gt;make_checkerboard&lt;/code&gt; function, then shuffled and passed to the Spectral Biclustering algorithm. The rows and columns of the shuffled matrix are rearranged to show the biclusters found by the algorithm.</source>
          <target state="translated">データは &lt;code&gt;make_checkerboard&lt;/code&gt; 関数で生成され、シャッフルされてスペクトルバイクラスタリングアルゴリズムに渡されます。シャッフルされた行列の行と列が再配置され、アルゴリズムによって検出された双クラスターが表示されます。</target>
        </trans-unit>
        <trans-unit id="549dd87c3787b6b83f2456e4cff9d8aa392d12c1" translate="yes" xml:space="preserve">
          <source>The data is the results of a chemical analysis of wines grown in the same region in Italy by three different cultivators. There are thirteen different measurements taken for different constituents found in the three types of wine.</source>
          <target state="translated">このデータは、イタリアの同じ地域で3つの異なる栽培者によって栽培されたワインの化学分析の結果です。3種類のワインに含まれる異なる成分について13種類の測定が行われています。</target>
        </trans-unit>
        <trans-unit id="279fbf7584e00e713412e8ff4546300536ffeb5a" translate="yes" xml:space="preserve">
          <source>The data matrix</source>
          <target state="translated">データマトリクス</target>
        </trans-unit>
        <trans-unit id="a36ea8c8057a9ae699a69f099ee316c3295afaf3" translate="yes" xml:space="preserve">
          <source>The data matrix, with p features and n samples. The data set must be the one which was used to compute the raw estimates.</source>
          <target state="translated">p個の特徴量とn個のサンプルを持つデータ行列.データセットは,生の推定値を計算するのに使用されたものでなければなりません.</target>
        </trans-unit>
        <trans-unit id="f0ed1480aeed96966c83d245c0e82839723677ea" translate="yes" xml:space="preserve">
          <source>The data matrix.</source>
          <target state="translated">データマトリクスです。</target>
        </trans-unit>
        <trans-unit id="70cee63c42777050ee29bb5c41f5ed813382b6c9" translate="yes" xml:space="preserve">
          <source>The data of the returned sparse matrix. By default it is int</source>
          <target state="translated">返される疎な行列のデータ.デフォルトでは int</target>
        </trans-unit>
        <trans-unit id="13b8870b16632bb144f99a987da0a17b912fd261" translate="yes" xml:space="preserve">
          <source>The data of the returned sparse matrix. By default it is the dtype of img</source>
          <target state="translated">返される疎な行列のデータ。デフォルトでは img</target>
        </trans-unit>
        <trans-unit id="a9c1f6d9c961581c21421066f6ce8f7c709084a9" translate="yes" xml:space="preserve">
          <source>The data on which &lt;code&gt;gbrt&lt;/code&gt; was trained.</source>
          <target state="translated">&lt;code&gt;gbrt&lt;/code&gt; がトレーニングされたデータ。</target>
        </trans-unit>
        <trans-unit id="c66d20ce5405cff4e02a00c321afd4016f52379a" translate="yes" xml:space="preserve">
          <source>The data on which &lt;code&gt;gbrt&lt;/code&gt; was trained. It is used to generate a &lt;code&gt;grid&lt;/code&gt; for the &lt;code&gt;target_variables&lt;/code&gt;. The &lt;code&gt;grid&lt;/code&gt; comprises &lt;code&gt;grid_resolution&lt;/code&gt; equally spaced points between the two &lt;code&gt;percentiles&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;gbrt&lt;/code&gt; がトレーニングされたデータ。 &lt;code&gt;target_variables&lt;/code&gt; の &lt;code&gt;grid&lt;/code&gt; を生成するために使用されます。 &lt;code&gt;grid&lt;/code&gt; 含むが &lt;code&gt;grid_resolution&lt;/code&gt; 両者の等間隔の点 &lt;code&gt;percentiles&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="65c0e8f3b46fedbf2f41b7e1589997d8de30db77" translate="yes" xml:space="preserve">
          <source>The data set contains images of hand-written digits: 10 classes where each class refers to a digit.</source>
          <target state="translated">データセットには、手書きの数字の画像が含まれています。各クラスは1桁の数字を参照しています。</target>
        </trans-unit>
        <trans-unit id="a49ccf8db4843f7af3fe75d2e385d4c5ae4a247b" translate="yes" xml:space="preserve">
          <source>The data that should be scaled.</source>
          <target state="translated">秤にかけるべきデータ。</target>
        </trans-unit>
        <trans-unit id="4362ce6dec3d09ef8de03aa7af16c30845dd1b85" translate="yes" xml:space="preserve">
          <source>The data that should be transformed back.</source>
          <target state="translated">変換して戻すべきデータ。</target>
        </trans-unit>
        <trans-unit id="47ba75ee664e21a51401b4c1203a08e7a876f44e" translate="yes" xml:space="preserve">
          <source>The data to be transformed by subset.</source>
          <target state="translated">サブセットで変換するデータ。</target>
        </trans-unit>
        <trans-unit id="03bb048cfbbc3212fd60edf266a3822d3176ef87" translate="yes" xml:space="preserve">
          <source>The data to be transformed using a power transformation.</source>
          <target state="translated">電力変換を用いて変換するデータ。</target>
        </trans-unit>
        <trans-unit id="73ce115221fb870e05c90b8f043ca278fdee74a1" translate="yes" xml:space="preserve">
          <source>The data to be transformed.</source>
          <target state="translated">変換するデータです。</target>
        </trans-unit>
        <trans-unit id="df7f442215a9627450351a5239ccb7b837681c69" translate="yes" xml:space="preserve">
          <source>The data to binarize, element by element. scipy.sparse matrices should be in CSR format to avoid an un-necessary copy.</source>
          <target state="translated">scipy.sparse matricesは,不要なコピーを避けるためにCSR形式でなければなりません.</target>
        </trans-unit>
        <trans-unit id="5401bf0fdb954957c9a3f345344e508dbd6bac54" translate="yes" xml:space="preserve">
          <source>The data to binarize, element by element. scipy.sparse matrices should be in CSR or CSC format to avoid an un-necessary copy.</source>
          <target state="translated">scipy.sparse matricesは,不要なコピーを避けるためにCSRまたはCSC形式でなければなりません.</target>
        </trans-unit>
        <trans-unit id="515108ae5aad51ada5b234ccff65d93cdd42711e" translate="yes" xml:space="preserve">
          <source>The data to center and scale.</source>
          <target state="translated">センタリングしてスケールするデータ。</target>
        </trans-unit>
        <trans-unit id="975edd1be086184330a8a3ebbf935588408d4a98" translate="yes" xml:space="preserve">
          <source>The data to determine the categories of each feature.</source>
          <target state="translated">各特徴のカテゴリを決定するためのデータです。</target>
        </trans-unit>
        <trans-unit id="a77a7e1c81043cc97f700ad0d213253f486b563a" translate="yes" xml:space="preserve">
          <source>The data to encode.</source>
          <target state="translated">エンコードするデータです。</target>
        </trans-unit>
        <trans-unit id="d843c1d7e5e8986d5ad23251cff2a94ef8ee7955" translate="yes" xml:space="preserve">
          <source>The data to fit.</source>
          <target state="translated">フィットするデータ。</target>
        </trans-unit>
        <trans-unit id="e7fbe72ba2fcf8fff679b5e28b0fef9589e7589d" translate="yes" xml:space="preserve">
          <source>The data to fit. Can be for example a list, or an array.</source>
          <target state="translated">合わせたいデータ。リストや配列などにすることができます。</target>
        </trans-unit>
        <trans-unit id="00725d5de44209593a99040fefb16433276154c2" translate="yes" xml:space="preserve">
          <source>The data to fit. Can be, for example a list, or an array at least 2d.</source>
          <target state="translated">フィットするデータ。例えば、リストや少なくとも2次元の配列などが考えられます。</target>
        </trans-unit>
        <trans-unit id="53846ac28d489b02b9949ef562a942f522e52d48" translate="yes" xml:space="preserve">
          <source>The data to normalize, element by element. scipy.sparse matrices should be in CSR format to avoid an un-necessary copy.</source>
          <target state="translated">scipy.sparse行列は,不要なコピーを避けるためにCSR形式でなければなりません.</target>
        </trans-unit>
        <trans-unit id="c13e1ef3ee3dcba79bb567a64bd572c32814ce01" translate="yes" xml:space="preserve">
          <source>The data to normalize, row by row. scipy.sparse matrices should be in CSR format to avoid an un-necessary copy.</source>
          <target state="translated">scipy.sparse matricesは,不要なコピーを避けるためにCSR形式でなければなりません.</target>
        </trans-unit>
        <trans-unit id="a092ffb0fb8aec81b90c0802c73b2ae6cfa80dcc" translate="yes" xml:space="preserve">
          <source>The data to transform, row by row. Sparse input should preferably be in CSC format.</source>
          <target state="translated">変換するデータを行単位で指定します。疎な入力はCSCフォーマットであることが望ましい。</target>
        </trans-unit>
        <trans-unit id="3f0555e5cf2c3f2eb80a653fe1f67911e223ec90" translate="yes" xml:space="preserve">
          <source>The data to transform.</source>
          <target state="translated">変換するデータです。</target>
        </trans-unit>
        <trans-unit id="f0a016c9443bbddef85796fd3691360a3c817ea0" translate="yes" xml:space="preserve">
          <source>The data used to compute the mean and standard deviation used for later scaling along the features axis.</source>
          <target state="translated">特徴軸に沿った後のスケーリングに使用される平均と標準偏差を計算するために使用されるデータ。</target>
        </trans-unit>
        <trans-unit id="55438370e51e74e9e8aa5e6ed6a37b2c111bd898" translate="yes" xml:space="preserve">
          <source>The data used to compute the median and quantiles used for later scaling along the features axis.</source>
          <target state="translated">特徴軸に沿った後のスケーリングに使用される中央値と定量値の計算に使用されたデータ。</target>
        </trans-unit>
        <trans-unit id="0ae3bec0fd19bdece5355d54cf80d1ae7bcc7c26" translate="yes" xml:space="preserve">
          <source>The data used to compute the per-feature minimum and maximum used for later scaling along the features axis.</source>
          <target state="translated">フィーチャー軸に沿った後のスケーリングに使用されるフィーチャーごとの最小値と最大値を計算するために使用されるデータ。</target>
        </trans-unit>
        <trans-unit id="e26762b4f27727d3210d3fe2a64ad24b366c1062" translate="yes" xml:space="preserve">
          <source>The data used to estimate the optimal transformation parameters.</source>
          <target state="translated">最適な変換パラメータを推定するために使用されるデータ。</target>
        </trans-unit>
        <trans-unit id="56c320de81ba4f246c360453cbd2da4e63d63c7f" translate="yes" xml:space="preserve">
          <source>The data used to fit the model. If &lt;code&gt;copy_X=False&lt;/code&gt;, then &lt;code&gt;X_fit_&lt;/code&gt; is a reference. This attribute is used for the calls to transform.</source>
          <target state="translated">モデルの近似に使用されるデータ。場合 &lt;code&gt;copy_X=False&lt;/code&gt; 、その後 &lt;code&gt;X_fit_&lt;/code&gt; は参照です。この属性は、変換の呼び出しに使用されます。</target>
        </trans-unit>
        <trans-unit id="03376550c822ad450e2d14840686210c4804ebc6" translate="yes" xml:space="preserve">
          <source>The data used to scale along the features axis.</source>
          <target state="translated">フィーチャー軸に沿ってスケールするために使用されるデータです。</target>
        </trans-unit>
        <trans-unit id="c4374e3fd9ee0863ad791a18672a80272e651c9f" translate="yes" xml:space="preserve">
          <source>The data used to scale along the features axis. If a sparse matrix is provided, it will be converted into a sparse &lt;code&gt;csc_matrix&lt;/code&gt;. Additionally, the sparse matrix needs to be nonnegative if &lt;code&gt;ignore_implicit_zeros&lt;/code&gt; is False.</source>
          <target state="translated">フィーチャ軸に沿ってスケーリングするために使用されるデータ。スパース行列が指定されている場合は、スパース &lt;code&gt;csc_matrix&lt;/code&gt; に変換されます。さらに、 &lt;code&gt;ignore_implicit_zeros&lt;/code&gt; がFalseの場合、スパース行列は負でない必要があります。</target>
        </trans-unit>
        <trans-unit id="848647eb355f23af31ef1bf06cbb1e0d945eea47" translate="yes" xml:space="preserve">
          <source>The data used to scale along the specified axis.</source>
          <target state="translated">指定された軸に沿ってスケールするために使用されるデータです。</target>
        </trans-unit>
        <trans-unit id="db728424571c6d0aa73c940cbe613870747b4067" translate="yes" xml:space="preserve">
          <source>The data was used with many others for comparing various classifiers. The classes are separable, though only RDA has achieved 100% correct classification. (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) (All results using the leave-one-out technique)</source>
          <target state="translated">様々な分類器を比較するために、他の多くのデータと一緒に使用した。RDAだけが100%正しい分類を達成していますが、クラスは分離可能です。(RDA:100%、QDA 99.4%、LDA 98.9%、1NN 96.1% (z変換データ))(すべての結果は、リーブ・オン・アウト技法を使用しています)</target>
        </trans-unit>
        <trans-unit id="660fe59aa4f1107f7d968ac4c0471ddbff4317b5" translate="yes" xml:space="preserve">
          <source>The data.</source>
          <target state="translated">データです。</target>
        </trans-unit>
        <trans-unit id="02bda070eb404181c9796aadeceacf78aac05356" translate="yes" xml:space="preserve">
          <source>The dataset can be compressed, either by removing exact duplicates if these occur in your data, or by using BIRCH. Then you only have a relatively small number of representatives for a large number of points. You can then provide a &lt;code&gt;sample_weight&lt;/code&gt; when fitting DBSCAN.</source>
          <target state="translated">データセットは、データで重複が発生した場合に完全に重複するものを削除するか、BIRCHを使用して圧縮できます。そうすると、多数のポイントに対して比較的少数の代理人しか存在しなくなります。その後、DBSCANをフィッティングするときに &lt;code&gt;sample_weight&lt;/code&gt; を指定できます。</target>
        </trans-unit>
        <trans-unit id="6bbac4d66b74c67e38ab2b8e6e075ffda05e6522" translate="yes" xml:space="preserve">
          <source>The dataset is called &amp;ldquo;Twenty Newsgroups&amp;rdquo;. Here is the official description, quoted from the &lt;a href=&quot;http://people.csail.mit.edu/jrennie/20Newsgroups/&quot;&gt;website&lt;/a&gt;:</source>
          <target state="translated">データセットは「20ニュースグループ」と呼ばれます。これは&lt;a href=&quot;http://people.csail.mit.edu/jrennie/20Newsgroups/&quot;&gt;ウェブサイト&lt;/a&gt;から引用された公式の説明です：</target>
        </trans-unit>
        <trans-unit id="c4573e45f21f00151e7512eaf1b6ae1cd41e22bd" translate="yes" xml:space="preserve">
          <source>The dataset is from Zhu et al [1].</source>
          <target state="translated">データセットはZhu et al [1]からのものです。</target>
        </trans-unit>
        <trans-unit id="1a602d6b3832c4acaa760453be6cb4d26914f03f" translate="yes" xml:space="preserve">
          <source>The dataset is generated using the &lt;code&gt;make_biclusters&lt;/code&gt; function, which creates a matrix of small values and implants bicluster with large values. The rows and columns are then shuffled and passed to the Spectral Co-Clustering algorithm. Rearranging the shuffled matrix to make biclusters contiguous shows how accurately the algorithm found the biclusters.</source>
          <target state="translated">データセットは、 &lt;code&gt;make_biclusters&lt;/code&gt; 関数を使用して生成されます。関数make_biclustersは、小さな値のマトリックスを作成し、大きな値でbiclusterを埋め込みます。次に、行と列がシャッフルされ、スペクトル共クラスタリングアルゴリズムに渡されます。シャッフルされた行列を再配置してバイクラスターを隣接させると、アルゴリズムがどのように正確にバイクラスターを見つけたかがわかります。</target>
        </trans-unit>
        <trans-unit id="4131a4e690502c19383d02f0465052ba8df886e1" translate="yes" xml:space="preserve">
          <source>The dataset is structured such that points nearby in index order are nearby in parameter space, leading to an approximately block-diagonal matrix of K-nearest neighbors. Such a sparse graph is useful in a variety of circumstances which make use of spatial relationships between points for unsupervised learning: in particular, see &lt;a href=&quot;generated/sklearn.manifold.isomap#sklearn.manifold.Isomap&quot;&gt;&lt;code&gt;sklearn.manifold.Isomap&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt;&lt;code&gt;sklearn.manifold.LocallyLinearEmbedding&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;generated/sklearn.cluster.spectralclustering#sklearn.cluster.SpectralClustering&quot;&gt;&lt;code&gt;sklearn.cluster.SpectralClustering&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">データセットは、インデックス順で近くにあるポイントがパラメーター空間で近くなるように構造化されており、K最近傍のほぼブロック対角行列になります。このようなスパースグラフは、教師なし学習のためにポイント間の空間関係を利用するさまざまな状況で&lt;a href=&quot;generated/sklearn.cluster.spectralclustering#sklearn.cluster.SpectralClustering&quot;&gt; &lt;code&gt;sklearn.cluster.SpectralClustering&lt;/code&gt; &lt;/a&gt;ます。特に、&lt;a href=&quot;generated/sklearn.manifold.isomap#sklearn.manifold.Isomap&quot;&gt; &lt;code&gt;sklearn.manifold.Isomap&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt; &lt;code&gt;sklearn.manifold.LocallyLinearEmbedding&lt;/code&gt; &lt;/a&gt;、およびsklearn.cluster.SpectralClusteringを参照してください。</target>
        </trans-unit>
        <trans-unit id="774649dea26c780c5e43d2464445b85ef137fcca" translate="yes" xml:space="preserve">
          <source>The dataset is the Boston Housing dataset (resp. 20 Newsgroups) for regression (resp. classification).</source>
          <target state="translated">データセットは、回帰(resp.分類)のためのBoston Housingデータセット(resp.20 Newsgroups)です。</target>
        </trans-unit>
        <trans-unit id="c0f7ef482ab49522d3cd87d64587ff804f6727c1" translate="yes" xml:space="preserve">
          <source>The dataset used for evaluation is a 2D grid of isotropic Gaussian clusters widely spaced.</source>
          <target state="translated">評価に使用したデータセットは、等方性ガウスクラスターを広く間隔をおいて配置した2次元グリッドです。</target>
        </trans-unit>
        <trans-unit id="c07599d3a55d8254ba468cc109830533370f5dcf" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is Reuters-21578 as provided by the UCI ML repository. It will be automatically downloaded and uncompressed on first run.</source>
          <target state="translated">この例で使用したデータセットは、UCI ML リポジトリから提供されている Reuters-21578 です。データセットは自動的にダウンロードされ、最初の実行時に圧縮されません。</target>
        </trans-unit>
        <trans-unit id="f610543312a82d7ead69951d61b82950d647ad3c" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is a preprocessed excerpt of the &amp;ldquo;Labeled Faces in the Wild&amp;rdquo;, aka &lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/&quot;&gt;LFW&lt;/a&gt;:</source>
          <target state="translated">この例で使用されているデータセットは、「野生のラベル付き顔」、別名&lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/&quot;&gt;LFWの&lt;/a&gt;前処理された抜粋です。</target>
        </trans-unit>
        <trans-unit id="257296081e1c4b1a90a4ff7a918d81d5746f6705" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is a preprocessed excerpt of the &amp;ldquo;Labeled Faces in the Wild&amp;rdquo;, also known as &lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/&quot;&gt;LFW&lt;/a&gt;:</source>
          <target state="translated">この例で使用されているデータセットは、「野生のラベル付きの顔」の前処理された抜粋であり、&lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/&quot;&gt;LFW&lt;/a&gt;としても知られています。</target>
        </trans-unit>
        <trans-unit id="0e491b7a83df0aa734f40ae016e056d9d5ef3d92" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is the 20 newsgroups dataset which will be automatically downloaded and then cached and reused for the document classification example.</source>
          <target state="translated">この例で使用するデータセットは、自動的にダウンロードされ、キャッシュされて文書分類の例で再利用されるニュースグループのデータセットです。</target>
        </trans-unit>
        <trans-unit id="b391209f39032c4c37a7d267548501e64198b514" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is the 20 newsgroups dataset. It will be automatically downloaded, then cached.</source>
          <target state="translated">この例で使用するデータセットは、20 のニュースグループデータセットです。これは自動的にダウンロードされ、キャッシュされます。</target>
        </trans-unit>
        <trans-unit id="b6261e60f08853c79c52801fadd1554784335b15" translate="yes" xml:space="preserve">
          <source>The dataset used is the Wine Dataset available at UCI. This dataset has continuous features that are heterogeneous in scale due to differing properties that they measure (i.e alcohol content, and malic acid).</source>
          <target state="translated">使用したデータセットは、UCIで利用可能なWine Datasetです。このデータセットは、測定する特性(アルコール含有量、リンゴ酸など)が異なるため、スケールが不均一な連続的な特徴を持っています。</target>
        </trans-unit>
        <trans-unit id="2320c3b81471635dac46a3209d55417f5e2427c2" translate="yes" xml:space="preserve">
          <source>The dataset will be downloaded from the &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume5/lewis04a/&quot;&gt;rcv1 homepage&lt;/a&gt; if necessary. The compressed size is about 656 MB.</source>
          <target state="translated">データセットは、必要に応じて&lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume5/lewis04a/&quot;&gt;rcv1ホームページ&lt;/a&gt;からダウンロードされます。圧縮サイズは約656 MBです。</target>
        </trans-unit>
        <trans-unit id="a104c196a5a6b4961911da3fe75edb0c176425f0" translate="yes" xml:space="preserve">
          <source>The datasets also contain a full description in their &lt;code&gt;DESCR&lt;/code&gt; attribute and some contain &lt;code&gt;feature_names&lt;/code&gt; and &lt;code&gt;target_names&lt;/code&gt;. See the dataset descriptions below for details.</source>
          <target state="translated">また、データセットの &lt;code&gt;DESCR&lt;/code&gt; 属性には完全な説明が含まれ、一部には &lt;code&gt;feature_names&lt;/code&gt; と &lt;code&gt;target_names&lt;/code&gt; が含まれています。詳細については、以下のデータセットの説明を参照してください。</target>
        </trans-unit>
        <trans-unit id="f5da29d39d3005818dbe52be4adbd180f7105540" translate="yes" xml:space="preserve">
          <source>The decision function is:</source>
          <target state="translated">決定機能は</target>
        </trans-unit>
        <trans-unit id="c0e6f8aea5cc6c4291ed60eb8cccabc092233917" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The columns correspond to the classes in sorted order, as they appear in the attribute &lt;code&gt;classes_&lt;/code&gt;. Regression and binary classification are special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;.</source>
          <target state="translated">入力サンプルの決定関数。列は、属性 &lt;code&gt;classes_&lt;/code&gt; に表示されるとおり、ソートされた順序でクラスに対応します。回帰とバイナリ分類は &lt;code&gt;k == 1&lt;/code&gt; 特殊なケースであり、それ以外の場合は &lt;code&gt;k==n_classes&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="06ac27333a17bb0e5a70b7bdd7a4a5eafb52284d" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of outputs is the same of that of the &lt;code&gt;classes_&lt;/code&gt; attribute. Binary classification is a special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;. For binary classification, values closer to -1 or 1 mean more like the first or second class in &lt;code&gt;classes_&lt;/code&gt;, respectively.</source>
          <target state="translated">入力サンプルの決定関数。出力の順序は、 &lt;code&gt;classes_&lt;/code&gt; 属性の順序と同じです。バイナリ分類は &lt;code&gt;k == 1&lt;/code&gt; 特殊なケースであり、それ以外の場合は &lt;code&gt;k==n_classes&lt;/code&gt; です。バイナリ分類の場合、-1または1に近い値は、それぞれ &lt;code&gt;classes_&lt;/code&gt; の最初のクラスまたは2番目のクラスに似ています。</target>
        </trans-unit>
        <trans-unit id="271144c81e33420b0281bbd1b4c29fbb1374c6ac" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of the classes corresponds to that in the attribute &lt;code&gt;classes_&lt;/code&gt;. Regression and binary classification are special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;.</source>
          <target state="translated">入力サンプルの決定関数。クラスの順序は、属性 &lt;code&gt;classes_&lt;/code&gt; の順序に対応しています。回帰とバイナリ分類は &lt;code&gt;k == 1&lt;/code&gt; 特殊なケースであり、それ以外の場合は &lt;code&gt;k==n_classes&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="c04fb9b1c64a374fbc2424dc1dc2e69edec92fae" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of the classes corresponds to that in the attribute &lt;code&gt;classes_&lt;/code&gt;. Regression and binary classification produce an array of shape [n_samples].</source>
          <target state="translated">入力サンプルの決定関数。クラスの順序は、属性 &lt;code&gt;classes_&lt;/code&gt; の順序に対応しています。回帰とバイナリ分類により、形状の配列[n_samples]が生成されます。</target>
        </trans-unit>
        <trans-unit id="7a4d1c2d06404b468f740213ed27426daa73066a" translate="yes" xml:space="preserve">
          <source>The decision rule for Bernoulli naive Bayes is based on</source>
          <target state="translated">ベルヌーイナイーブベイズの決定規則は</target>
        </trans-unit>
        <trans-unit id="0e0b3ca71cdfb4dae3b0e05518ff98bb98da9e44" translate="yes" xml:space="preserve">
          <source>The decision tree structure can be analysed to gain further insight on the relation between the features and the target to predict. In this example, we show how to retrieve:</source>
          <target state="translated">決定木構造を分析することで、特徴と予測対象との関係についての更なる洞察を得ることができる。この例では、取得する方法を示しています。</target>
        </trans-unit>
        <trans-unit id="7930dea6ad7960124a06f25452c69dc408a6af03" translate="yes" xml:space="preserve">
          <source>The decision tree to be exported to GraphViz.</source>
          <target state="translated">GraphVizにエクスポートする決定木。</target>
        </trans-unit>
        <trans-unit id="41d54cbac8ff4bb674faa62fe3a273da70a2c8ec" translate="yes" xml:space="preserve">
          <source>The decision values for the samples are computed by adding the normalized sum of pair-wise classification confidence levels to the votes in order to disambiguate between the decision values when the votes for all the classes are equal leading to a tie.</source>
          <target state="translated">サンプルの決定値は、すべてのクラスの投票が同点になるように等しい場合に、決定値の間の曖昧さをなくすために、投票にペアごとの分類信頼度の正規化された合計を加えることによって計算されます。</target>
        </trans-unit>
        <trans-unit id="9db4a0cde49703606f721a9d732403d53161167d" translate="yes" xml:space="preserve">
          <source>The decoding strategy depends on the vectorizer parameters.</source>
          <target state="translated">デコード戦略はベクタライザのパラメータに依存します。</target>
        </trans-unit>
        <trans-unit id="8ab5c7033c87e7cea9882c536950d8e60cf30550" translate="yes" xml:space="preserve">
          <source>The default coding of images is based on the &lt;code&gt;uint8&lt;/code&gt; dtype to spare memory. Often machine learning algorithms work best if the input is converted to a floating point representation first. Also, if you plan to use &lt;code&gt;matplotlib.pyplpt.imshow&lt;/code&gt; don&amp;rsquo;t forget to scale to the range 0 - 1 as done in the following example.</source>
          <target state="translated">画像のデフォルトのコーディングは、メモリを節約するための &lt;code&gt;uint8&lt;/code&gt; dtypeに基づいています。多くの場合、入力を最初に浮動小数点表現に変換すると、機械学習アルゴリズムが最適に機能します。また、 &lt;code&gt;matplotlib.pyplpt.imshow&lt;/code&gt; を使用する場合は、次の例のように0〜1の範囲にスケーリングすることを忘れないでください。</target>
        </trans-unit>
        <trans-unit id="ec537a685ef4f8a1d5936e658cd9e5b93a584d77" translate="yes" xml:space="preserve">
          <source>The default configuration tokenizes the string by extracting words of at least 2 letters. The specific function that does this step can be requested explicitly:</source>
          <target state="translated">デフォルトの設定では、少なくとも2文字以上の単語を抽出して文字列をトークン化します。このステップを行う特定の関数を明示的に要求することができます。</target>
        </trans-unit>
        <trans-unit id="3d025c574ea3cc62ecf87d5b0a9f83d7b3c75d6a" translate="yes" xml:space="preserve">
          <source>The default cross-validation generator used is Stratified K-Folds. If an integer is provided, then it is the number of folds used. See the module &lt;a href=&quot;../classes#module-sklearn.model_selection&quot;&gt;&lt;code&gt;sklearn.model_selection&lt;/code&gt;&lt;/a&gt; module for the list of possible cross-validation objects.</source>
          <target state="translated">使用されるデフォルトの交差検定ジェネレーターは、層別K折りです。整数が指定されている場合、それは使用される折りたたみの数です。可能な相互検証オブジェクトのリストについては、モジュール&lt;a href=&quot;../classes#module-sklearn.model_selection&quot;&gt; &lt;code&gt;sklearn.model_selection&lt;/code&gt; &lt;/a&gt;モジュールを参照してください。</target>
        </trans-unit>
        <trans-unit id="53050da8c5d49a141b5e2d80a70c0fa67fd7eb00" translate="yes" xml:space="preserve">
          <source>The default dataset is the digits dataset. To run the example on the twenty newsgroups dataset, pass the &amp;ndash;twenty-newsgroups command line argument to this script.</source>
          <target state="translated">デフォルトのデータセットは数字データセットです。20のニュースグループデータセットで例を実行するには、このスクリプトに&amp;ndash;twenty-newsgroupsコマンドライン引数を渡します。</target>
        </trans-unit>
        <trans-unit id="713044bd0fefe6c8ff38e7a00814412176e5caf0" translate="yes" xml:space="preserve">
          <source>The default error message is, &amp;ldquo;This %(name)s instance is not fitted yet. Call &amp;lsquo;fit&amp;rsquo; with appropriate arguments before using this method.&amp;rdquo;</source>
          <target state="translated">デフォルトのエラーメッセージは、「この％（name）sインスタンスはまだ適合していません。このメソッドを使用する前に、適切な引数を指定して「fit」を呼び出してください。」</target>
        </trans-unit>
        <trans-unit id="bd1a08c69ea4f3f536ea9f567759a2eca8958b5b" translate="yes" xml:space="preserve">
          <source>The default parameters (n_samples / n_features / n_components) should make the example runnable in a couple of tens of seconds. You can try to increase the dimensions of the problem, but be aware that the time complexity is polynomial in NMF. In LDA, the time complexity is proportional to (n_samples * iterations).</source>
          <target state="translated">デフォルトのパラメータ(n_samples/n_features/n_components)を使用すると、数十秒で実行可能な例ができます。問題の次元を大きくすることもできますが、時間の複雑さはNMFでは多項式であることに注意してください。LDAでは、時間の複雑さは(n_samples*iterations)に比例します。</target>
        </trans-unit>
        <trans-unit id="e964bf4dcf7d784d0dee842886fe27c5060b1af1" translate="yes" xml:space="preserve">
          <source>The default setting is &lt;code&gt;penalty=&quot;l2&quot;&lt;/code&gt;. The L1 penalty leads to sparse solutions, driving most coefficients to zero. The Elastic Net solves some deficiencies of the L1 penalty in the presence of highly correlated attributes. The parameter &lt;code&gt;l1_ratio&lt;/code&gt; controls the convex combination of L1 and L2 penalty.</source>
          <target state="translated">デフォルト設定は &lt;code&gt;penalty=&quot;l2&quot;&lt;/code&gt; です。L1ペナルティはスパースソリューションにつながり、ほとんどの係数がゼロになります。Elastic Netは、相関性の高い属性が存在する場合のL1ペナルティのいくつかの欠点を解決します。パラメータ &lt;code&gt;l1_ratio&lt;/code&gt; は、L1およびL2ペナルティの凸の組み合わせを制御します。</target>
        </trans-unit>
        <trans-unit id="d600d29df52c52da3fc6f38fdcf73036ecf8cbdf" translate="yes" xml:space="preserve">
          <source>The default slice is a rectangular shape around the face, removing most of the background:</source>
          <target state="translated">デフォルトのスライスは顔の周りを囲む長方形で、 背景の大部分が除去されています。</target>
        </trans-unit>
        <trans-unit id="f2f8fa0d6181b12438bd7660924ca4c2f89302ad" translate="yes" xml:space="preserve">
          <source>The default solver is &amp;lsquo;svd&amp;rsquo;. It can perform both classification and transform, and it does not rely on the calculation of the covariance matrix. This can be an advantage in situations where the number of features is large. However, the &amp;lsquo;svd&amp;rsquo; solver cannot be used with shrinkage.</source>
          <target state="translated">デフォルトのソルバーは「svd」です。分類と変換の両方を実行でき、共分散行列の計算に依存しません。これは、フィーチャの数が多い場合に有利です。ただし、「svd」ソルバーは収縮では使用できません。</target>
        </trans-unit>
        <trans-unit id="2307e8c1575f885543e9c12c3e37adbd9d66c3dd" translate="yes" xml:space="preserve">
          <source>The default strategy implements one step of the bootstrapping procedure.</source>
          <target state="translated">デフォルトのストラテジーは、ブートストラップ手順の1つのステップを実装しています。</target>
        </trans-unit>
        <trans-unit id="f17725906dcf13fd5a8abccea776a77c1270d7a9" translate="yes" xml:space="preserve">
          <source>The default value &lt;code&gt;max_features=&quot;auto&quot;&lt;/code&gt; uses &lt;code&gt;n_features&lt;/code&gt; rather than &lt;code&gt;n_features / 3&lt;/code&gt;. The latter was originally suggested in [1], whereas the former was more recently justified empirically in [2].</source>
          <target state="translated">デフォルト値の &lt;code&gt;max_features=&quot;auto&quot;&lt;/code&gt; の用途の &lt;code&gt;n_features&lt;/code&gt; ではなく &lt;code&gt;n_features / 3&lt;/code&gt; 。後者はもともと[1]で提案されていましたが、前者は[2]で経験的に正当化されました。</target>
        </trans-unit>
        <trans-unit id="8ad5c264779356671425d0f732ca8de7347758f3" translate="yes" xml:space="preserve">
          <source>The default values for the parameters controlling the size of the trees (e.g. &lt;code&gt;max_depth&lt;/code&gt;, &lt;code&gt;min_samples_leaf&lt;/code&gt;, etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values.</source>
          <target state="translated">ツリーのサイズを制御するパラメーターのデフォルト値（たとえば、 &lt;code&gt;max_depth&lt;/code&gt; 、 &lt;code&gt;min_samples_leaf&lt;/code&gt; など）は、一部のデータセットでは非常に大きくなる可能性がある、完全に成長した枝刈りされていないツリーを導きます。メモリの消費を減らすには、これらのパラメータ値を設定して、ツリーの複雑さとサイズを制御する必要があります。</target>
        </trans-unit>
        <trans-unit id="4209f696edb2b5615bc39c07cc72cc1b347ebd54" translate="yes" xml:space="preserve">
          <source>The definitive description of key concepts and API elements for using scikit-learn and developing compatible tools.</source>
          <target state="translated">scikit-learnを使用して互換性のあるツールを開発するための重要なコンセプトとAPI要素の決定版です。</target>
        </trans-unit>
        <trans-unit id="767ee5df767908c4f8729c932c4a3d808fe558cd" translate="yes" xml:space="preserve">
          <source>The degree of the polynomial features. Default = 2.</source>
          <target state="translated">多項式特徴量の次数。デフォルトは2です。</target>
        </trans-unit>
        <trans-unit id="c0ae4038f8b612638e22a63002b9a0592ebf4ed6" translate="yes" xml:space="preserve">
          <source>The density of w, between 0 and 1</source>
          <target state="translated">0から1の間のwの密度</target>
        </trans-unit>
        <trans-unit id="64211b28a272c8cc95fc17412f0b29158a9e57aa" translate="yes" xml:space="preserve">
          <source>The desired absolute tolerance of the result. A larger tolerance will generally lead to faster execution. Default is 0.</source>
          <target state="translated">結果の希望する絶対的な許容範囲。許容範囲を大きくすると、一般的に実行が速くなります。デフォルトは0です。</target>
        </trans-unit>
        <trans-unit id="d976f3ec62dae27876361a8dc74b8b1f3089859a" translate="yes" xml:space="preserve">
          <source>The desired relative tolerance of the result. A larger tolerance will generally lead to faster execution. Default is 1E-8.</source>
          <target state="translated">結果の望ましい相対的な許容範囲。許容範囲を大きくすると、一般的に実行が速くなります。デフォルトは1E-8です。</target>
        </trans-unit>
        <trans-unit id="e06750706d15ac4ccfc7d440948abe624b2a5a8e" translate="yes" xml:space="preserve">
          <source>The diabetes dataset consists of 10 physiological variables (age, sex, weight, blood pressure) measure on 442 patients, and an indication of disease progression after one year:</source>
          <target state="translated">糖尿病データセットは、442人の患者の10の生理学的変数(年齢、性別、体重、血圧)を測定し、1年後の病気の進行度を示しています。</target>
        </trans-unit>
        <trans-unit id="9fc4977037d4a4b0eddabbe726bd2d0de7d06481" translate="yes" xml:space="preserve">
          <source>The dict at &lt;code&gt;search.cv_results_['params'][search.best_index_]&lt;/code&gt; gives the parameter setting for the best model, that gives the highest mean score (&lt;code&gt;search.best_score_&lt;/code&gt;).</source>
          <target state="translated">&lt;code&gt;search.cv_results_['params'][search.best_index_]&lt;/code&gt; のdict は、最高の平均スコア（ &lt;code&gt;search.best_score_&lt;/code&gt; ）を与える最良のモデルのパラメーター設定を提供します。</target>
        </trans-unit>
        <trans-unit id="bfaa3d320392a9f6c8addb9130a129b654790105" translate="yes" xml:space="preserve">
          <source>The dictionary atoms used for sparse coding. Lines are assumed to be normalized to unit norm.</source>
          <target state="translated">疎符号化に用いられる辞書アトム。行は単位ノルムに正規化されているものとする。</target>
        </trans-unit>
        <trans-unit id="7373631cc038017694fe3a2fdfb8d4bfc8e42605" translate="yes" xml:space="preserve">
          <source>The dictionary factor in the matrix factorization.</source>
          <target state="translated">行列因数分解の辞書因子。</target>
        </trans-unit>
        <trans-unit id="09a0fc68f904b631d6aa1871e7b81e42ac764443" translate="yes" xml:space="preserve">
          <source>The dictionary is fitted on the distorted left half of the image, and subsequently used to reconstruct the right half. Note that even better performance could be achieved by fitting to an undistorted (i.e. noiseless) image, but here we start from the assumption that it is not available.</source>
          <target state="translated">辞書は、歪んだ画像の左半分にフィットされ、その後、右半分を再構築するために使用されます。歪んでいない(つまりノイズのない)画像にフィットすることで、さらに優れた性能が得られることに注意してくださいが、ここでは、それが利用できないことを前提にしています。</target>
        </trans-unit>
        <trans-unit id="03531d72d72ed916646e794ac15c0622fe9c9b59" translate="yes" xml:space="preserve">
          <source>The dictionary learning objects offer, via the &lt;code&gt;split_code&lt;/code&gt; parameter, the possibility to separate the positive and negative values in the results of sparse coding. This is useful when dictionary learning is used for extracting features that will be used for supervised learning, because it allows the learning algorithm to assign different weights to negative loadings of a particular atom, from to the corresponding positive loading.</source>
          <target state="translated">辞書学習オブジェクトは、 &lt;code&gt;split_code&lt;/code&gt; パラメータを介して、スパースコーディングの結果で正と負の値を分離する可能性を提供します。これは、学習アルゴリズムが特定の原子の負の負荷から対応する正の負荷に異なる重みを割り当てることができるため、教師あり学習に使用される特徴を抽出するために辞書学習が使用される場合に役立ちます。</target>
        </trans-unit>
        <trans-unit id="c5012873b1ea68e5deed2b04cdb4e90687b2c340" translate="yes" xml:space="preserve">
          <source>The dictionary matrix against which to solve the sparse coding of the data. Some of the algorithms assume normalized rows for meaningful output.</source>
          <target state="translated">データの疎符号化を解くための辞書行列.アルゴリズムの中には,意味のある出力のために正規化された行を想定しているものもあります.</target>
        </trans-unit>
        <trans-unit id="f137f3a53fdf8d92f7463e89f6383b5a88f3c320" translate="yes" xml:space="preserve">
          <source>The dictionary with normalized components (D).</source>
          <target state="translated">正規化された成分を持つ辞書(D)。</target>
        </trans-unit>
        <trans-unit id="d49d4bb0bf7407d291ade3beb50a3cbdf55548f8" translate="yes" xml:space="preserve">
          <source>The difference between LeavePGroupsOut and GroupShuffleSplit is that the former generates splits using all subsets of size &lt;code&gt;p&lt;/code&gt; unique groups, whereas GroupShuffleSplit generates a user-determined number of random test splits, each with a user-determined fraction of unique groups.</source>
          <target state="translated">LeavePGroupsOutとGroupShuffleSplitの違いは、前者はサイズ &lt;code&gt;p&lt;/code&gt; の一意のグループのすべてのサブセットを使用して分割を生成するのに対し、GroupShuffleSplitはユーザーが決定した数のランダムテスト分割を生成し、それぞれが一意のグループのユーザーが決定した割合を生成します。</target>
        </trans-unit>
        <trans-unit id="f8e596d2cfcc0c0ae56904e18bdffdb0d463a655" translate="yes" xml:space="preserve">
          <source>The difference between LeavePGroupsOut and LeaveOneGroupOut is that the former builds the test sets with all the samples assigned to &lt;code&gt;p&lt;/code&gt; different values of the groups while the latter uses samples all assigned the same groups.</source>
          <target state="translated">LeavePGroupsOutとLeaveOneGroupOutの違いは、前者はすべてのサンプルがグループの &lt;code&gt;p&lt;/code&gt; 個の異なる値に割り当てられたテストセットを構築するのに対し、後者はすべて同じグループに割り当てられたサンプルを使用することです。</target>
        </trans-unit>
        <trans-unit id="123fc66dcb39304255b8e25c6c10c654dbb2b0f3" translate="yes" xml:space="preserve">
          <source>The different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of \(P(x_i \mid y)\).</source>
          <target state="translated">異なるナイーブベイズ分類器は、主に、\(P(x_i \mid y)y)の分布に関する仮定によって異なります。</target>
        </trans-unit>
        <trans-unit id="fc85907a08e94b0f17af8c0c01aad9ef06729185" translate="yes" xml:space="preserve">
          <source>The digits dataset is made of 1797 8x8 images of hand-written digits</source>
          <target state="translated">数字データセットは、手書きの数字の8x8画像1797枚で構成されています。</target>
        </trans-unit>
        <trans-unit id="46ad3d3589f97f144056aa3785c730497272c03e" translate="yes" xml:space="preserve">
          <source>The dimension of the projected subspace.</source>
          <target state="translated">投影された部分空間の次元。</target>
        </trans-unit>
        <trans-unit id="5572a493038acd1179abd12e1cd0c48e6709dea1" translate="yes" xml:space="preserve">
          <source>The dimension of the projection subspace.</source>
          <target state="translated">投影部分空間の次元。</target>
        </trans-unit>
        <trans-unit id="984d56bddca949d9f1aa278aca87eb46a4072ffd" translate="yes" xml:space="preserve">
          <source>The dimensionality of the resulting representation is &lt;code&gt;n_out &amp;lt;= n_estimators * max_leaf_nodes&lt;/code&gt;. If &lt;code&gt;max_leaf_nodes == None&lt;/code&gt;, the number of leaf nodes is at most &lt;code&gt;n_estimators * 2 ** max_depth&lt;/code&gt;.</source>
          <target state="translated">結果の表現の次元は &lt;code&gt;n_out &amp;lt;= n_estimators * max_leaf_nodes&lt;/code&gt; です。 &lt;code&gt;max_leaf_nodes == None&lt;/code&gt; 場合、リーフノードの数は最大で &lt;code&gt;n_estimators * 2 ** max_depth&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="068779d9c792d7cec6633fa1a442091cb7f6f6f3" translate="yes" xml:space="preserve">
          <source>The dimensions and distribution of Random Projections matrices are controlled so as to preserve the pairwise distances between any two samples of the dataset.</source>
          <target state="translated">ランダム投影行列の次元と分布は,データセットの任意の2つのサンプル間のペアワイズ距離を維持するように制御される.</target>
        </trans-unit>
        <trans-unit id="1fc3554ec8e7cb3510bb9bcb462301a368807c4a" translate="yes" xml:space="preserve">
          <source>The dimensions and distribution of random projections matrices are controlled so as to preserve the pairwise distances between any two samples of the dataset. Thus random projection is a suitable approximation technique for distance based method.</source>
          <target state="translated">ランダム射影行列の次元と分布は、データセットの任意の2つのサンプル間のペアワイズ距離を維持するように制御されている。このようにランダム射影は距離に基づく手法に適した近似手法である。</target>
        </trans-unit>
        <trans-unit id="16b41d4b4452bd551af457fee2f8b60407215654" translate="yes" xml:space="preserve">
          <source>The dirichlet concentration of each component on the weight distribution (Dirichlet).</source>
          <target state="translated">重量分布上の各成分のディリクレット濃度(ディリクレット)。</target>
        </trans-unit>
        <trans-unit id="6b13240ddd6531c8fdb797758cabfbb4633f5e80" translate="yes" xml:space="preserve">
          <source>The dirichlet concentration of each component on the weight distribution (Dirichlet). The type depends on &lt;code&gt;weight_concentration_prior_type&lt;/code&gt;:</source>
          <target state="translated">重量分布における各成分のディリクレ濃度（ディリクレ）。タイプは &lt;code&gt;weight_concentration_prior_type&lt;/code&gt; に依存します。</target>
        </trans-unit>
        <trans-unit id="b470809fd29296951902d887ca553f032b8a40c4" translate="yes" xml:space="preserve">
          <source>The dirichlet concentration of each component on the weight distribution (Dirichlet). This is commonly called gamma in the literature. The higher concentration puts more mass in the center and will lead to more components being active, while a lower concentration parameter will lead to more mass at the edge of the mixture weights simplex. The value of the parameter must be greater than 0. If it is None, it&amp;rsquo;s set to &lt;code&gt;1. / n_components&lt;/code&gt;.</source>
          <target state="translated">重量分布における各成分のディリクレ濃度（ディリクレ）。これは、文献では一般にガンマと呼ばれています。濃度が高いほど、中心に多くの質量が配置され、より多くのコンポーネントがアクティブになります。一方、濃度が低いほど、混合ウェイトシンプレックスのエッジでより多くの質量が得られます。パラメータの値は0より大きくなければなりません &lt;code&gt;1. / n_components&lt;/code&gt; の場合、1に設定されます。/ n_components。</target>
        </trans-unit>
        <trans-unit id="cbf40d766d38685ebd59a9222ccafd32de07a34f" translate="yes" xml:space="preserve">
          <source>The disadvantages of Bayesian regression include:</source>
          <target state="translated">ベイズ回帰のデメリットとしては、以下のようなものがあります。</target>
        </trans-unit>
        <trans-unit id="ec84046fbad625e7f2f2f6744eab5866f4d72369" translate="yes" xml:space="preserve">
          <source>The disadvantages of GBRT are:</source>
          <target state="translated">GBRTのデメリットは</target>
        </trans-unit>
        <trans-unit id="cfaf1a3af8c2483843ec10c36faa80968b886bf2" translate="yes" xml:space="preserve">
          <source>The disadvantages of Gaussian processes include:</source>
          <target state="translated">ガウス過程の欠点としては、以下のようなものがあります。</target>
        </trans-unit>
        <trans-unit id="c8e0c1974308fc5ae09e195effcd7396a8db8601" translate="yes" xml:space="preserve">
          <source>The disadvantages of Multi-layer Perceptron (MLP) include:</source>
          <target state="translated">多層パーセプトロン(MLP)の欠点としては、以下のようなものがある。</target>
        </trans-unit>
        <trans-unit id="c71179ab39a085455ac6f6888a2f6fd3afe84d6f" translate="yes" xml:space="preserve">
          <source>The disadvantages of Stochastic Gradient Descent include:</source>
          <target state="translated">確率勾配降下のデメリットとしては、以下のようなものがあります。</target>
        </trans-unit>
        <trans-unit id="84c28c658fb7eb00eb9347365efe97c2f04c30f3" translate="yes" xml:space="preserve">
          <source>The disadvantages of decision trees include:</source>
          <target state="translated">デシジョンツリーのデメリットとしては、以下のようなものがあります。</target>
        </trans-unit>
        <trans-unit id="47feb0b4f4476b90d13a7091a56a1b0f68ad3a62" translate="yes" xml:space="preserve">
          <source>The disadvantages of support vector machines include:</source>
          <target state="translated">サポートベクターマシンの欠点としては、以下のようなものがあります。</target>
        </trans-unit>
        <trans-unit id="266f0e7f0f21de8e86a9f2ce2320de66f17358df" translate="yes" xml:space="preserve">
          <source>The disadvantages of the LARS method include:</source>
          <target state="translated">LARS法の欠点としては、以下のようなものがある。</target>
        </trans-unit>
        <trans-unit id="fe9e13a1ad2c431f3f290607f881625ead9408c0" translate="yes" xml:space="preserve">
          <source>The disadvantages to using t-SNE are roughly:</source>
          <target state="translated">t-SNEを使うことのデメリットは大まかに言えば。</target>
        </trans-unit>
        <trans-unit id="a7ea673185d2a40501e91d63f573c417ac055cd1" translate="yes" xml:space="preserve">
          <source>The distance metric to use</source>
          <target state="translated">使用する距離メトリック</target>
        </trans-unit>
        <trans-unit id="0063e1a06973d2332dc280d77426f57f299b9005" translate="yes" xml:space="preserve">
          <source>The distance metric to use. Note that not all metrics are valid with all algorithms. Refer to the documentation of &lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;BallTree&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt;&lt;code&gt;KDTree&lt;/code&gt;&lt;/a&gt; for a description of available algorithms. Note that the normalization of the density output is correct only for the Euclidean distance metric. Default is &amp;lsquo;euclidean&amp;rsquo;.</source>
          <target state="translated">使用する距離メトリック。すべてのメトリックがすべてのアルゴリズムで有効であるとは限らないことに注意してください。使用可能なアルゴリズムの説明については、&lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;BallTree&lt;/code&gt; &lt;/a&gt;および&lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt; &lt;code&gt;KDTree&lt;/code&gt; の&lt;/a&gt;ドキュメントを参照してください。密度出力の正規化は、ユークリッド距離メトリックに対してのみ正しいことに注意してください。デフォルトは「ユークリッド」です。</target>
        </trans-unit>
        <trans-unit id="3c4790f8b52fd2ea34a9c9dfcea72e27e7f13da0" translate="yes" xml:space="preserve">
          <source>The distance metric used to calculate the k-Neighbors for each sample point. The DistanceMetric class gives a list of available metrics. The default distance is &amp;lsquo;euclidean&amp;rsquo; (&amp;lsquo;minkowski&amp;rsquo; metric with the p param equal to 2.)</source>
          <target state="translated">各サンプルポイントのk近傍を計算するために使用される距離メトリック。DistanceMetricクラスは、使用可能なメトリックのリストを提供します。デフォルトの距離は「ユークリッド」です（p paramが2の「minkowski」メトリック）。</target>
        </trans-unit>
        <trans-unit id="f687007319ee4f163c7b6668932fdc048712a932" translate="yes" xml:space="preserve">
          <source>The distance metric used to calculate the neighbors within a given radius for each sample point. The DistanceMetric class gives a list of available metrics. The default distance is &amp;lsquo;euclidean&amp;rsquo; (&amp;lsquo;minkowski&amp;rsquo; metric with the param equal to 2.)</source>
          <target state="translated">各サンプルポイントの特定の半径内の近傍を計算するために使用される距離メトリック。DistanceMetricクラスは、使用可能なメトリックのリストを提供します。デフォルトの距離は「ユークリッド」です（パラメーターが2の「ミンコフスキー」メトリック）。</target>
        </trans-unit>
        <trans-unit id="7e12a027c556a152948ba7c771e2cb65d4ac3e99" translate="yes" xml:space="preserve">
          <source>The distinct labels used in classifying instances.</source>
          <target state="translated">インスタンスの分類に使用される明確なラベル。</target>
        </trans-unit>
        <trans-unit id="2333c2cc37e50157e20c383f74b00f781cb37a1b" translate="yes" xml:space="preserve">
          <source>The distortion introduced by a random projection &lt;code&gt;p&lt;/code&gt; is asserted by the fact that &lt;code&gt;p&lt;/code&gt; is defining an eps-embedding with good probability as defined by:</source>
          <target state="translated">ランダム射影 &lt;code&gt;p&lt;/code&gt; によって導入される歪みは、 &lt;code&gt;p&lt;/code&gt; が次のように定義される確率の高いeps埋め込みを定義しているという事実によって主張されます。</target>
        </trans-unit>
        <trans-unit id="befef2d542b10663820383e2f1e59843332d14a0" translate="yes" xml:space="preserve">
          <source>The distortion introduced by a random projection &lt;code&gt;p&lt;/code&gt; only changes the distance between two points by a factor (1 +- eps) in an euclidean space with good probability. The projection &lt;code&gt;p&lt;/code&gt; is an eps-embedding as defined by:</source>
          <target state="translated">ランダムな投影 &lt;code&gt;p&lt;/code&gt; によって導入された歪みは、ユークリッド空間で2点間の距離を係数（1 +-eps）だけ変化させるだけです。射影 &lt;code&gt;p&lt;/code&gt; は、次のように定義されるeps埋め込みです。</target>
        </trans-unit>
        <trans-unit id="b6d88e183a439e17415e4b06c82155c9e34fa344" translate="yes" xml:space="preserve">
          <source>The distributions in &lt;code&gt;scipy.stats&lt;/code&gt; prior to version scipy 0.16 do not allow specifying a random state. Instead, they use the global numpy random state, that can be seeded via &lt;code&gt;np.random.seed&lt;/code&gt; or set using &lt;code&gt;np.random.set_state&lt;/code&gt;. However, beginning scikit-learn 0.18, the &lt;a href=&quot;classes#module-sklearn.model_selection&quot;&gt;&lt;code&gt;sklearn.model_selection&lt;/code&gt;&lt;/a&gt; module sets the random state provided by the user if scipy &amp;gt;= 0.16 is also available.</source>
          <target state="translated">バージョンscipy 0.16より前の &lt;code&gt;scipy.stats&lt;/code&gt; の配布では、ランダムな状態を指定できません。代わりに、それらは &lt;code&gt;np.random.seed&lt;/code&gt; を介してシードするか、または &lt;code&gt;np.random.set_state&lt;/code&gt; を使用して設定できる、グローバルなnumpyランダム状態を使用します。ただし、scikit-learn 0.18以降、scipy&amp;gt; = 0.16も使用できる場合、&lt;a href=&quot;classes#module-sklearn.model_selection&quot;&gt; &lt;code&gt;sklearn.model_selection&lt;/code&gt; &lt;/a&gt;モジュールはユーザーによって提供されるランダムな状態を設定します。</target>
        </trans-unit>
        <trans-unit id="e4d3a4f449c4df99f288fa9346370fef4e50edc3" translate="yes" xml:space="preserve">
          <source>The dual gap at the end of the optimization for the optimal alpha (&lt;code&gt;alpha_&lt;/code&gt;).</source>
          <target state="translated">最適化アルファ（ &lt;code&gt;alpha_&lt;/code&gt; ）の最適化の最後にあるデュアルギャップ。</target>
        </trans-unit>
        <trans-unit id="1c5b7990e4f078095970dbd44954a4ff0f6bdb22" translate="yes" xml:space="preserve">
          <source>The dual gaps at the end of the optimization for each alpha.</source>
          <target state="translated">各αの最適化の最後にあるデュアルギャップ。</target>
        </trans-unit>
        <trans-unit id="b5eac57a33eeb0d09a6d5ebfcbe611bef2da07d8" translate="yes" xml:space="preserve">
          <source>The edges of each bin. Contain arrays of varying shapes &lt;code&gt;(n_bins_, )&lt;/code&gt; Ignored features will have empty arrays.</source>
          <target state="translated">各ビンの端。さまざまな形状の配列を含む &lt;code&gt;(n_bins_, )&lt;/code&gt; 無視された機能には空の配列があります。</target>
        </trans-unit>
        <trans-unit id="50c07dc6eab301b9d9953e2bd5849c6aa3abe8d7" translate="yes" xml:space="preserve">
          <source>The effect of the transformer is weaker than on the synthetic data. However, the transform induces a decrease of the MAE.</source>
          <target state="translated">変換の効果は合成データよりも弱い。しかし、変換はMAEを減少させる。</target>
        </trans-unit>
        <trans-unit id="8b7256d0b08ed885751aee08f85ac36bb70ae336" translate="yes" xml:space="preserve">
          <source>The effective size of the batch is computed here. If there are no more jobs to dispatch, return False, else return True.</source>
          <target state="translated">バッチの有効なサイズはここで計算されます。もうディスパッチするジョブがない場合はFalseを返し、そうでない場合はTrueを返します。</target>
        </trans-unit>
        <trans-unit id="a044cf1265a1684d79e7b48898279d20ad6f0dac" translate="yes" xml:space="preserve">
          <source>The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities</source>
          <target state="translated">使用する固有値分解ストラテジー。AMGはpyamgをインストールする必要があります。非常に大規模で疎な問題では高速になりますが、不安定性を引き起こす可能性があります。</target>
        </trans-unit>
        <trans-unit id="1dc685b8ff4dd3ccaa26f5f653c44ef0324bd82f" translate="yes" xml:space="preserve">
          <source>The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities.</source>
          <target state="translated">使用する固有値分解ストラテジー。AMGはpyamgをインストールする必要があります。非常に大規模で疎な問題では高速化できますが、不安定性を引き起こす可能性もあります。</target>
        </trans-unit>
        <trans-unit id="f14ed9796539634c3492800089020cfe2ebbd7af" translate="yes" xml:space="preserve">
          <source>The elastic net optimization function varies for mono and multi-outputs.</source>
          <target state="translated">弾性ネット最適化関数は、単出力と多出力で変化します。</target>
        </trans-unit>
        <trans-unit id="e0191353bc3bca50b627784afbd555eb11c1f3dd" translate="yes" xml:space="preserve">
          <source>The empirical covariance matrix of a sample can be computed using the &lt;a href=&quot;generated/sklearn.covariance.empirical_covariance#sklearn.covariance.empirical_covariance&quot;&gt;&lt;code&gt;empirical_covariance&lt;/code&gt;&lt;/a&gt; function of the package, or by fitting an &lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance&quot;&gt;&lt;code&gt;EmpiricalCovariance&lt;/code&gt;&lt;/a&gt; object to the data sample with the &lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance.fit&quot;&gt;&lt;code&gt;EmpiricalCovariance.fit&lt;/code&gt;&lt;/a&gt; method. Be careful that results depend on whether the data are centered, so one may want to use the &lt;code&gt;assume_centered&lt;/code&gt; parameter accurately. More precisely, if &lt;code&gt;assume_centered=False&lt;/code&gt;, then the test set is supposed to have the same mean vector as the training set. If not, both should be centered by the user, and &lt;code&gt;assume_centered=True&lt;/code&gt; should be used.</source>
          <target state="translated">サンプルの経験的共分散行列を用いて計算することができる&lt;a href=&quot;generated/sklearn.covariance.empirical_covariance#sklearn.covariance.empirical_covariance&quot;&gt; &lt;code&gt;empirical_covariance&lt;/code&gt; の&lt;/a&gt;、または適合させることにより、パッケージの機能を&lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance&quot;&gt; &lt;code&gt;EmpiricalCovariance&lt;/code&gt; を&lt;/a&gt;有するデータサンプルにオブジェクトを&lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance.fit&quot;&gt; &lt;code&gt;EmpiricalCovariance.fit&lt;/code&gt; の&lt;/a&gt;方法。結果がデータが中央に配置されているかどうかに依存することに注意してください。 &lt;code&gt;assume_centered&lt;/code&gt; 、assume_centeredパラメータを正確に使用したい場合があります。より正確には、 &lt;code&gt;assume_centered=False&lt;/code&gt; の場合、テストセットはトレーニングセットと同じ平均ベクトルを持っていると想定されます。そうでない場合は、両方をユーザーが中央に配置し、 &lt;code&gt;assume_centered=True&lt;/code&gt; を使用する必要があります。</target>
        </trans-unit>
        <trans-unit id="d4b0da98d5cb047aa932ef62858edca40e188517" translate="yes" xml:space="preserve">
          <source>The encoded signal (Y).</source>
          <target state="translated">符号化された信号(Y)である。</target>
        </trans-unit>
        <trans-unit id="cc2fdf3023f61dc3df7bba02538cce691e7cf2cd" translate="yes" xml:space="preserve">
          <source>The energy function measures the quality of a joint assignment:</source>
          <target state="translated">エネルギー関数は、ジョイントアサインの質を測定します。</target>
        </trans-unit>
        <trans-unit id="1bfb5cae50ef0b1032c729ea4ab68ff502e7f906" translate="yes" xml:space="preserve">
          <source>The entry &lt;code&gt;test_fold[i]&lt;/code&gt; represents the index of the test set that sample &lt;code&gt;i&lt;/code&gt; belongs to. It is possible to exclude sample &lt;code&gt;i&lt;/code&gt; from any test set (i.e. include sample &lt;code&gt;i&lt;/code&gt; in every training set) by setting &lt;code&gt;test_fold[i]&lt;/code&gt; equal to -1.</source>
          <target state="translated">エントリ &lt;code&gt;test_fold[i]&lt;/code&gt; は、サンプル &lt;code&gt;i&lt;/code&gt; が属するテストセットのインデックスを表します。 &lt;code&gt;test_fold[i]&lt;/code&gt; を-1に設定することで、テストセットからサンプル &lt;code&gt;i&lt;/code&gt; を除外する（つまり、すべてのトレーニングセットにサンプル &lt;code&gt;i&lt;/code&gt; を含める）ことができます。</target>
        </trans-unit>
        <trans-unit id="503425d1f7e07b98f32316b1d85343640a4e1294" translate="yes" xml:space="preserve">
          <source>The equivalence between &lt;code&gt;alpha&lt;/code&gt; and the regularization parameter of SVM, &lt;code&gt;C&lt;/code&gt; is given by &lt;code&gt;alpha = 1 / C&lt;/code&gt; or &lt;code&gt;alpha = 1 / (n_samples * C)&lt;/code&gt;, depending on the estimator and the exact objective function optimized by the model.</source>
          <target state="translated">&lt;code&gt;alpha&lt;/code&gt; とSVMの正則化パラメーター &lt;code&gt;C&lt;/code&gt; の等価性は、推定量とモデルによって最適化された正確な目的関数に応じて、 &lt;code&gt;alpha = 1 / C&lt;/code&gt; または &lt;code&gt;alpha = 1 / (n_samples * C)&lt;/code&gt; で与えられます。</target>
        </trans-unit>
        <trans-unit id="2fec157ea85a1f513b11852f437f8dc13c9361bb" translate="yes" xml:space="preserve">
          <source>The error message or a substring of the error message.</source>
          <target state="translated">エラーメッセージまたはエラーメッセージの部分文字列。</target>
        </trans-unit>
        <trans-unit id="67a9569df158acc253e75d9fa812424d1f2b58c6" translate="yes" xml:space="preserve">
          <source>The estimated (sparse) precision matrix.</source>
          <target state="translated">推定された(疎な)精度行列。</target>
        </trans-unit>
        <trans-unit id="cdf0d65945f589fd5da3781e66a0a81a30e934b1" translate="yes" xml:space="preserve">
          <source>The estimated covariance matrix.</source>
          <target state="translated">推定共分散行列。</target>
        </trans-unit>
        <trans-unit id="77a18d9bfc3d1fc8448c4fb559b59c860611bbc7" translate="yes" xml:space="preserve">
          <source>The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999. See &amp;ldquo;Pattern Recognition and Machine Learning&amp;rdquo; by C. Bishop, 12.2.1 p. 574 or &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;.</source>
          <target state="translated">Tipping and Bishop 1999からの確率的PCAモデルに従う推定ノイズ共分散。C。Bishopによる「パターン認識と機械学習」、12.2.1ページを参照してください。574または&lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="735cdf6c1c61f38b1092aa1bd4262b34c0d5f0f3" translate="yes" xml:space="preserve">
          <source>The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999. See &amp;ldquo;Pattern Recognition and Machine Learning&amp;rdquo; by C. Bishop, 12.2.1 p. 574 or &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;. It is required to compute the estimated data covariance and score samples.</source>
          <target state="translated">Tipping and Bishop 1999からの確率的PCAモデルに従う推定ノイズ共分散。C。Bishopによる「パターン認識と機械学習」、12.2.1ページを参照してください。574または&lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;。推定データの共分散とスコアサンプルを計算する必要があります。</target>
        </trans-unit>
        <trans-unit id="cc49065d7efcb582d87899f7997c53b1f8d60c78" translate="yes" xml:space="preserve">
          <source>The estimated noise variance for each feature.</source>
          <target state="translated">各特徴の推定ノイズ分散。</target>
        </trans-unit>
        <trans-unit id="960e70b3d078f74487289ecbb537290d1f7473d4" translate="yes" xml:space="preserve">
          <source>The estimated number of components. Relevant when &lt;code&gt;n_components=None&lt;/code&gt;.</source>
          <target state="translated">コンポーネントの推定数。 &lt;code&gt;n_components=None&lt;/code&gt; の場合に関連します。</target>
        </trans-unit>
        <trans-unit id="df011fa4ccaad62cb4a83a0955409a4c4b3035c8" translate="yes" xml:space="preserve">
          <source>The estimated number of components. When n_components is set to &amp;lsquo;mle&amp;rsquo; or a number between 0 and 1 (with svd_solver == &amp;lsquo;full&amp;rsquo;) this number is estimated from input data. Otherwise it equals the parameter n_components, or the lesser value of n_features and n_samples if n_components is None.</source>
          <target state="translated">コンポーネントの推定数。n_componentsが 'mle'または0〜1の数値（svd_solver == 'full'）に設定されている場合、この数値は入力データから推定されます。それ以外の場合は、パラメーターn_componentsに等しいか、n_componentsがNoneの場合はn_featuresおよびn_samplesの小さい方の値になります。</target>
        </trans-unit>
        <trans-unit id="cd106f5afe35a52902113758dbc47d95941530e6" translate="yes" xml:space="preserve">
          <source>The estimated number of connected components in the graph.</source>
          <target state="translated">グラフ内の連結成分の推定数。</target>
        </trans-unit>
        <trans-unit id="a98c4d1bc7caeb1163bf14c014655348909c79f3" translate="yes" xml:space="preserve">
          <source>The estimation of the model is done by calculating the slopes and intercepts of a subpopulation of all possible combinations of p subsample points. If an intercept is fitted, p must be greater than or equal to n_features + 1. The final slope and intercept is then defined as the spatial median of these slopes and intercepts.</source>
          <target state="translated">モデルの推定は,p個のサブサンプル点のすべての可能な組み合わせのサブ集団の傾きと切片を計算することによって行われる.切片が適合する場合,p は n_features+1 以上でなければならない.最終的な傾きと切片は,これらの傾きと切片の空間的中央値として定義される.</target>
        </trans-unit>
        <trans-unit id="aed02faa2e2402cc32a5968b7e86a54cff535c4e" translate="yes" xml:space="preserve">
          <source>The estimation of the model is done by iteratively maximizing the marginal log-likelihood of the observations.</source>
          <target state="translated">モデルの推定は、オブザベーションの限界対数尤度を反復的に最大化することによって行われる。</target>
        </trans-unit>
        <trans-unit id="44fe06813d3627aaa56531c52961387365e10d66" translate="yes" xml:space="preserve">
          <source>The estimation of the number of degrees of freedom is given by:</source>
          <target state="translated">自由度数の推定は次のように与えられます。</target>
        </trans-unit>
        <trans-unit id="2bed56c931836016f42a33afdedfac5cb8f555f4" translate="yes" xml:space="preserve">
          <source>The estimator also implements &lt;code&gt;partial_fit&lt;/code&gt;, which updates the dictionary by iterating only once over a mini-batch. This can be used for online learning when the data is not readily available from the start, or for when the data does not fit into the memory.</source>
          <target state="translated">推定器は、ミニバッチに対して1回だけ繰り返すことによって辞書を更新する &lt;code&gt;partial_fit&lt;/code&gt; も実装します。これは、データが最初からすぐに利用できない場合、またはデータがメモリに収まらない場合に、オンライン学習に使用できます。</target>
        </trans-unit>
        <trans-unit id="efc1e3841cf31ff119d8e77fb55d7093c1df4dd4" translate="yes" xml:space="preserve">
          <source>The estimator objects for each cv split. This is available only if &lt;code&gt;return_estimator&lt;/code&gt; parameter is set to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">各CV分割の推定子オブジェクト。これは、 &lt;code&gt;return_estimator&lt;/code&gt; パラメーターが &lt;code&gt;True&lt;/code&gt; に設定されている場合にのみ使用できます。</target>
        </trans-unit>
        <trans-unit id="f33cc8b973e26f8a535386dbd610021f87dfdf8f" translate="yes" xml:space="preserve">
          <source>The estimator or group of estimators to be cloned</source>
          <target state="translated">クローン化される推定子または推定子のグループ</target>
        </trans-unit>
        <trans-unit id="65944411996b76786bcb63db12ac31b08b55648a" translate="yes" xml:space="preserve">
          <source>The estimator that provides the initial predictions. Set via the &lt;code&gt;init&lt;/code&gt; argument or &lt;code&gt;loss.init_estimator&lt;/code&gt;.</source>
          <target state="translated">初期予測を提供する推定量。 &lt;code&gt;init&lt;/code&gt; 引数または &lt;code&gt;loss.init_estimator&lt;/code&gt; を介して設定します。</target>
        </trans-unit>
        <trans-unit id="ade400270e658832b6e128c7ef187979eae33356" translate="yes" xml:space="preserve">
          <source>The estimators of a pipeline are stored as a list in the &lt;code&gt;steps&lt;/code&gt; attribute:</source>
          <target state="translated">パイプラインの推定量は、 &lt;code&gt;steps&lt;/code&gt; 属性にリストとして格納されます。</target>
        </trans-unit>
        <trans-unit id="a46349c122935a13db8091d877d4179b37995289" translate="yes" xml:space="preserve">
          <source>The estimators provided in this module are meta-estimators: they require a base estimator to be provided in their constructor. For example, it is possible to use these estimators to turn a binary classifier or a regressor into a multiclass classifier. It is also possible to use these estimators with multiclass estimators in the hope that their accuracy or runtime performance improves.</source>
          <target state="translated">このモジュールで提供される推定量はメタ推定量であり、コンストラクタでベース推定量を提供する必要があります。例えば、これらの推定量を使用して、バイナリ分類器またはレグレッサーをマルチクラス分類器に変換することができます。また、精度や実行時の性能が向上することを期待して、これらの推定子をマルチクラス推定子と一緒に使用することも可能である。</target>
        </trans-unit>
        <trans-unit id="1868049da2c813b87c2e3d48a5c71f72cc892852" translate="yes" xml:space="preserve">
          <source>The estimators provided in this module are meta-estimators: they require a base estimator to be provided in their constructor. The meta-estimator extends single output estimators to multioutput estimators.</source>
          <target state="translated">このモジュールで提供される推定量はメタ推定量であり、コンストラクタでベース推定量を提供する必要があります。メタ推定量は、単一出力の推定量を複数出力の推定量に拡張します。</target>
        </trans-unit>
        <trans-unit id="fe77b9f934c3d4135d1310eb25c7f2c922f35971" translate="yes" xml:space="preserve">
          <source>The exact API of all functions and classes, as given by the docstrings. The API documents expected types and allowed features for all functions, and all parameters available for the algorithms.</source>
          <target state="translated">docstringsによって与えられた、すべての関数とクラスの正確なAPI。このAPIは、すべての関数に期待される型と許容される機能、アルゴリズムで利用可能なすべてのパラメータを文書化しています。</target>
        </trans-unit>
        <trans-unit id="213a31afd656f2a0c8b37dd55b40fc5308db3ca1" translate="yes" xml:space="preserve">
          <source>The exact additive chi squared kernel.</source>
          <target state="translated">厳密な加法カイ二乗カーネル。</target>
        </trans-unit>
        <trans-unit id="14b1a49c77fdcb8a0d3244090e7c6df0b2a6cd07" translate="yes" xml:space="preserve">
          <source>The exact chi squared kernel.</source>
          <target state="translated">厳密なカイ二乗カーネル。</target>
        </trans-unit>
        <trans-unit id="21c56a998f59a2c6be6550db485efcc395359e33" translate="yes" xml:space="preserve">
          <source>The example below demonstrates how the OOB error can be measured at the addition of each new tree during training. The resulting plot allows a practitioner to approximate a suitable value of &lt;code&gt;n_estimators&lt;/code&gt; at which the error stabilizes.</source>
          <target state="translated">以下の例は、トレーニング中に新しいツリーを追加するたびにOOBエラーを測定する方法を示しています。結果のプロットにより、施術者はエラーが安定する &lt;code&gt;n_estimators&lt;/code&gt; の適切な値を概算できます。</target>
        </trans-unit>
        <trans-unit id="556b3073adbd8a84d1d4b2fe21db4807c2daf4c8" translate="yes" xml:space="preserve">
          <source>The example below uses a support vector classifier with a non-linear kernel to build a model with optimized hyperparameters by grid search. We compare the performance of non-nested and nested CV strategies by taking the difference between their scores.</source>
          <target state="translated">以下の例では、非線形カーネルを持つサポートベクター分類器を用いて、グリッド探索によって最適化されたハイパーパラメタを持つモデルを構築しています。非入れ子にしたCV戦略と入れ子にしたCV戦略の性能を,それぞれのスコアの差を取って比較しています.</target>
        </trans-unit>
        <trans-unit id="4cd198cf307950e0cb10d5d2e6b86c91624772aa" translate="yes" xml:space="preserve">
          <source>The example compares prediction result of linear regression (linear model) and decision tree (tree based model) with and without discretization of real-valued features.</source>
          <target state="translated">本例では、線形回帰(線形モデル)と決定木(木ベースモデル)の予測結果を、実値特徴量を離散化した場合としない場合で比較している。</target>
        </trans-unit>
        <trans-unit id="3101cd4fceb55d1be63d1bff5cb3f45996fd9aa0" translate="yes" xml:space="preserve">
          <source>The example demonstrates syntax and speed only; it doesn&amp;rsquo;t actually do anything useful with the extracted vectors. See the example scripts {document_classification_20newsgroups,clustering}.py for actual learning on text documents.</source>
          <target state="translated">この例は構文と速度のみを示しています。実際には、抽出されたベクトルに対して有用なことは何もしません。テキストドキュメントの実際の学習については、サンプルスクリプト{document_classification_20newsgroups、clustering} .pyをご覧ください。</target>
        </trans-unit>
        <trans-unit id="32ed3af5edbdaa27f9eba4e5a30255c6afab4414" translate="yes" xml:space="preserve">
          <source>The example is engineered to show the effect of the choice of different metrics. It is applied to waveforms, which can be seen as high-dimensional vector. Indeed, the difference between metrics is usually more pronounced in high dimension (in particular for euclidean and cityblock).</source>
          <target state="translated">この例は、異なるメトリクスの選択の効果を示すために設計されています。これは、高次元ベクトルとして見ることができる波形に適用されています。実際、メトリクス間の違いは通常、高次元ではより顕著になります(特にユークリッドとシティブロックの場合)。</target>
        </trans-unit>
        <trans-unit id="36ed82faabe70da57df289b8a54bb16e8830773a" translate="yes" xml:space="preserve">
          <source>The example shows that the predictions in ridge are strongly influenced by the outliers present in the dataset. The Huber regressor is less influenced by the outliers since the model uses the linear loss for these. As the parameter epsilon is increased for the Huber regressor, the decision function approaches that of the ridge.</source>
          <target state="translated">この例は、リッジの予測が、データセットに存在する外れ値に強く影響されていることを示しています。Huber回帰器は、モデルがこれらの外れ値に線形損失を使用するので、外れ値の影響をあまり受けません。Huber回帰器のパラメータεを増加させると、決定関数はリッジのそれに近づきます。</target>
        </trans-unit>
        <trans-unit id="889bc091af3b0106e91a81884d921d1b2df9bdd6" translate="yes" xml:space="preserve">
          <source>The examples below compare Gaussian mixture models with a fixed number of components, to the variational Gaussian mixture models with a Dirichlet process prior. Here, a classical Gaussian mixture is fitted with 5 components on a dataset composed of 2 clusters. We can see that the variational Gaussian mixture with a Dirichlet process prior is able to limit itself to only 2 components whereas the Gaussian mixture fits the data with a fixed number of components that has to be set a priori by the user. In this case the user has selected &lt;code&gt;n_components=5&lt;/code&gt; which does not match the true generative distribution of this toy dataset. Note that with very little observations, the variational Gaussian mixture models with a Dirichlet process prior can take a conservative stand, and fit only one component.</source>
          <target state="translated">以下の例では、コンポーネントの数が固定されているガウス混合モデルを、事前にディリクレプロセスを使用した変分ガウス混合モデルと比較しています。ここでは、古典的なガウス混合が2つのクラスターで構成されるデータセットの5つのコンポーネントに適合しています。事前にディリクレプロセスを使用した変分ガウス混合は、2つのコンポーネントのみに制限できることがわかりますが、ガウス混合は、ユーザーがアプリオリに事前に設定する必要がある固定数のコンポーネントを使用してデータに適合します。この場合、ユーザーは &lt;code&gt;n_components=5&lt;/code&gt; を選択しましたが、これはこのおもちゃのデータセットの真の生成分布と一致しません。観測が非常に少ない場合、事前にディリクレプロセスを使用した変分ガウス混合モデルは保守的な立場をとることができ、1つのコンポーネントのみに適合します。</target>
        </trans-unit>
        <trans-unit id="e5c4a6233958dced6338c85ecba7ef5860ffcbbc" translate="yes" xml:space="preserve">
          <source>The expected value for the mutual information can be calculated using the following equation [VEB2009]. In this equation, \(a_i = |U_i|\) (the number of elements in \(U_i\)) and \(b_j = |V_j|\) (the number of elements in \(V_j\)).</source>
          <target state="translated">相互情報の期待値は、次式[VEB2009]を用いて計算することができる。この式では、\(a_i=|U_i|\)(U_i|\)の要素数)と、\(b_j=|V_j|\)(V_j)の要素数)を表しています。</target>
        </trans-unit>
        <trans-unit id="5075a63ec7be1cbe42a641e5ae277f9e4b63f160" translate="yes" xml:space="preserve">
          <source>The experiment is performed on an artificial dataset for binary classification with 100,000 samples (1,000 of them are used for model fitting) with 20 features. Of the 20 features, only 2 are informative and 10 are redundant. The first figure shows the estimated probabilities obtained with logistic regression, Gaussian naive Bayes, and Gaussian naive Bayes with both isotonic calibration and sigmoid calibration. The calibration performance is evaluated with Brier score, reported in the legend (the smaller the better). One can observe here that logistic regression is well calibrated while raw Gaussian naive Bayes performs very badly. This is because of the redundant features which violate the assumption of feature-independence and result in an overly confident classifier, which is indicated by the typical transposed-sigmoid curve.</source>
          <target state="translated">実験は、20個の特徴量を持つ10万サンプル(うち1000サンプルをモデルフィッティングに使用)の2値分類用人工データセットを用いて行います。20個の特徴のうち、情報量が多いのは2個だけで、冗長性があるのは10個です。最初の図は、ロジスティック回帰、ガウスナイーブベイズ、ガウスナイーブベイズを用いて、アイソトニック校正とシグモイド校正の両方で得られた推定確率を示しています。校正性能は、凡例で報告されているBrierスコアで評価されます(小さいほど良い)。ここでは、ロジスティック回帰がよくキャリブレーションされているのに対し、生のガウスナイーブベイズは非常に悪い結果になることが観察できます。これは、特徴の非依存性の仮定に違反する冗長な特徴のためで、典型的な転置シグモイド曲線で示されるように、自信過剰な分類器になります。</target>
        </trans-unit>
        <trans-unit id="e1db213d528f30cf745c1554ad2a43991932646f" translate="yes" xml:space="preserve">
          <source>The explained variance or ndarray if &amp;lsquo;multioutput&amp;rsquo; is &amp;lsquo;raw_values&amp;rsquo;.</source>
          <target state="translated">'multioutput'が 'raw_values'の場合​​、説明された分散またはndarray。</target>
        </trans-unit>
        <trans-unit id="dff7adf6114ae66681ccc96cdb5c8abce36c0555" translate="yes" xml:space="preserve">
          <source>The explicit constant as predicted by the &amp;ldquo;constant&amp;rdquo; strategy. This parameter is useful only for the &amp;ldquo;constant&amp;rdquo; strategy.</source>
          <target state="translated">「定数」戦略によって予測される明示的な定数。このパラメーターは、「一定」戦略の場合にのみ役立ちます。</target>
        </trans-unit>
        <trans-unit id="96483cbe0c7b434b8387960f89d9c2700f0af59b" translate="yes" xml:space="preserve">
          <source>The exponent for inverse scaling learning rate [default 0.5].</source>
          <target state="translated">逆スケーリング学習率の指数 [デフォルト0.5]。</target>
        </trans-unit>
        <trans-unit id="1fa85799d065de026e0253585d451384c9ee6013" translate="yes" xml:space="preserve">
          <source>The exponent for inverse scaling learning rate. It is used in updating effective learning rate when the learning_rate is set to &amp;lsquo;invscaling&amp;rsquo;. Only used when solver=&amp;rsquo;sgd&amp;rsquo;.</source>
          <target state="translated">逆スケーリング学習率の指数。learning_rateが 'invscaling'に設定されている場合、効果的な学習率の更新に使用されます。solver = 'sgd'の場合にのみ使用されます。</target>
        </trans-unit>
        <trans-unit id="c7acf68e47ad4fe8cc8cf54d82e56ffc97c60a11" translate="yes" xml:space="preserve">
          <source>The exponent for the base kernel</source>
          <target state="translated">ベースカーネルの指数</target>
        </trans-unit>
        <trans-unit id="fe68de8b995171a61c550ec2f7815b01a2d28ec7" translate="yes" xml:space="preserve">
          <source>The exponentiated version of the kernel, which is usually preferable.</source>
          <target state="translated">カーネルの指数化されたバージョンで、通常はこれが好ましい。</target>
        </trans-unit>
        <trans-unit id="d1490a4bcb15c22959ee30800b231e4413480004" translate="yes" xml:space="preserve">
          <source>The external estimator fit on the reduced dataset.</source>
          <target state="translated">縮小されたデータセットに外部推定器がフィットします。</target>
        </trans-unit>
        <trans-unit id="18d0cd5609d8f78695dc43242ffb9cf995a65ee9" translate="yes" xml:space="preserve">
          <source>The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero components by sample in a more than 30000-dimensional space (less than .5% non-zero features):</source>
          <target state="translated">抽出されたTF-IDFベクトルは非常に疎であり、30000次元以上の空間に平均159個の非ゼロ成分が存在する(0.5%以下の非ゼロ特徴量)。</target>
        </trans-unit>
        <trans-unit id="e5bc1ae1e0e90623cdbf461e900aded771c1b79d" translate="yes" xml:space="preserve">
          <source>The extracted dataset will only retain pictures of people that have at least &lt;code&gt;min_faces_per_person&lt;/code&gt; different pictures.</source>
          <target state="translated">抽出されたデータセットは、少なくとも &lt;code&gt;min_faces_per_person&lt;/code&gt; の異なる写真を持つ人々の写真のみを保持します。</target>
        </trans-unit>
        <trans-unit id="7cb7e9ef70987101280d538c40ac3f0463557635" translate="yes" xml:space="preserve">
          <source>The factor multiplying the hypercube size. Larger values spread out the clusters/classes and make the classification task easier.</source>
          <target state="translated">ハイパーキューブのサイズを乗算した値。値が大きいほどクラスタ/クラスが分散し、分類作業が容易になります。</target>
        </trans-unit>
        <trans-unit id="0cd5de0c793252ae54dc423f82b86e165f05af5f" translate="yes" xml:space="preserve">
          <source>The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken from Fisher&amp;rsquo;s paper. Note that it&amp;rsquo;s the same as in R, but not as in the UCI Machine Learning Repository, which has two wrong data points.</source>
          <target state="translated">有名なアイリスデータベース。最初はRAフィッシャーによって使用されました。データセットはフィッシャーの論文から取られました。これはRと同じですが、2つの誤ったデータポイントがあるUCI Machine Learning Repositoryとは異なります。</target>
        </trans-unit>
        <trans-unit id="e9c1f9439f6a49e182ea739a4c5151e4783dfb88" translate="yes" xml:space="preserve">
          <source>The feature importance scores of a fit gradient boosting model can be accessed via the &lt;code&gt;feature_importances_&lt;/code&gt; property:</source>
          <target state="translated">適合勾配ブースティングモデルの機能重要度スコアには、 &lt;code&gt;feature_importances_&lt;/code&gt; プロパティを介してアクセスできます。</target>
        </trans-unit>
        <trans-unit id="40d3ffb0824ad292679a9b4129b9e33a1cef7858" translate="yes" xml:space="preserve">
          <source>The feature matrix. Categorical features are encoded as ordinals.</source>
          <target state="translated">特徴行列.カテゴリ特徴量は順序として符号化されます.</target>
        </trans-unit>
        <trans-unit id="0a487efba080872bb151e33f60795da2b55ed658" translate="yes" xml:space="preserve">
          <source>The feature ranking, such that &lt;code&gt;ranking_[i]&lt;/code&gt; corresponds to the ranking position of the i-th feature. Selected (i.e., estimated best) features are assigned rank 1.</source>
          <target state="translated">&lt;code&gt;ranking_[i]&lt;/code&gt; がi番目のフィーチャのランキング位置に対応するような、フィーチャランキング。選択された（つまり、最良の推定）機能にはランク1が割り当てられます。</target>
        </trans-unit>
        <trans-unit id="acbf27a6428b599cd8900c8ca4f4ecae3f0cc8b0" translate="yes" xml:space="preserve">
          <source>The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data and &lt;code&gt;max_features=n_features&lt;/code&gt;, if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, &lt;code&gt;random_state&lt;/code&gt; has to be fixed.</source>
          <target state="translated">機能は常に、分割ごとにランダムに並べ替えられます。したがって、最適なスプリットの検索中に列挙されたいくつかのスプリットで基準の改善が同じである場合、同じトレーニングデータと &lt;code&gt;max_features=n_features&lt;/code&gt; を使用しても、最適なスプリットは異なる場合があります。フィッティング中に確定的な動作を得るには、 &lt;code&gt;random_state&lt;/code&gt; を修正する必要があります。</target>
        </trans-unit>
        <trans-unit id="c25846da01d7420f77d01daefe6ea3229aead8d7" translate="yes" xml:space="preserve">
          <source>The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data, &lt;code&gt;max_features=n_features&lt;/code&gt; and &lt;code&gt;bootstrap=False&lt;/code&gt;, if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, &lt;code&gt;random_state&lt;/code&gt; has to be fixed.</source>
          <target state="translated">機能は常に、分割ごとにランダムに並べ替えられます。したがって、最適なスプリットの検索中に列挙されたいくつかのスプリットで基準の改善が同じである場合、同じトレーニングデータ &lt;code&gt;max_features=n_features&lt;/code&gt; および &lt;code&gt;bootstrap=False&lt;/code&gt; を使用しても、最適なスプリットは異なる場合があります。フィッティング中に確定的な動作を得るには、 &lt;code&gt;random_state&lt;/code&gt; を修正する必要があります。</target>
        </trans-unit>
        <trans-unit id="79b5615baa935539329fdffd05466c4966130183" translate="yes" xml:space="preserve">
          <source>The features indices which will be returned when calling &lt;code&gt;transform&lt;/code&gt;. They are computed during &lt;code&gt;fit&lt;/code&gt;. For &lt;code&gt;features='all'&lt;/code&gt;, it is to &lt;code&gt;range(n_features)&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;transform&lt;/code&gt; を呼び出したときに返される機能のインデックス。それらは、 &lt;code&gt;fit&lt;/code&gt; 中に計算されます。以下のため &lt;code&gt;features='all'&lt;/code&gt; 、それがにある &lt;code&gt;range(n_features)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="7bb18d685bec7903d9b61cd6026a227300d22db3" translate="yes" xml:space="preserve">
          <source>The features of &lt;code&gt;X&lt;/code&gt; have been transformed from \([x_1, x_2]\) to \([1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]\), and can now be used within any linear model.</source>
          <target state="translated">&lt;code&gt;X&lt;/code&gt; の機能は\（[x_1、x_2] \）から\（[1、x_1、x_2、x_1 ^ 2、x_1 x_2、x_2 ^ 2] \）に変換され、任意の線形モデル内で使用できるようになりました。</target>
        </trans-unit>
        <trans-unit id="b791adba0e25e8ba284f5da492871659f500ccc6" translate="yes" xml:space="preserve">
          <source>The features of X have been transformed from \((X_1, X_2)\) to \((1, X_1, X_2, X_1^2, X_1X_2, X_2^2)\).</source>
          <target state="translated">Xの特徴は、\((X_1,X_2)から\((1,X_1,X_2,X_1^2,X_1X_2,X_2^2)に変換されています。)</target>
        </trans-unit>
        <trans-unit id="0ec9e3dee3b599b742aa9c67dee3f4cab82e4d00" translate="yes" xml:space="preserve">
          <source>The features of X have been transformed from \((X_1, X_2, X_3)\) to \((1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3)\).</source>
          <target state="translated">Xの特徴を、\((X_1,X_2,X_3)から\((1,X_1,X_2,X_3,X_1X_2,X_1X_3,X_1X_3,X_2X_3,X_1X_2X_3)に変換しました。)</target>
        </trans-unit>
        <trans-unit id="eaccd9379a9dbc6cce41fc318001c8bf42339abe" translate="yes" xml:space="preserve">
          <source>The figure below illustrates the effect of shrinkage and subsampling on the goodness-of-fit of the model. We can clearly see that shrinkage outperforms no-shrinkage. Subsampling with shrinkage can further increase the accuracy of the model. Subsampling without shrinkage, on the other hand, does poorly.</source>
          <target state="translated">下の図は、モデルの適合度における収縮とサブサンプリングの効果を示しています。縮みが縮みなしよりも優れていることがよくわかります。縮小を伴うサブサンプリングは、モデルの精度をさらに高めることができる。一方,収縮なしのサブサンプリングは,あまりよくない.</target>
        </trans-unit>
        <trans-unit id="50a711fcdd2b061e3d348535a1d7a4d9bddb7ed5" translate="yes" xml:space="preserve">
          <source>The figure below shows the results of applying &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; with least squares loss and 500 base learners to the Boston house price dataset (&lt;a href=&quot;generated/sklearn.datasets.load_boston#sklearn.datasets.load_boston&quot;&gt;&lt;code&gt;sklearn.datasets.load_boston&lt;/code&gt;&lt;/a&gt;). The plot on the left shows the train and test error at each iteration. The train error at each iteration is stored in the &lt;code&gt;train_score_&lt;/code&gt; attribute of the gradient boosting model. The test error at each iterations can be obtained via the &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor.staged_predict&quot;&gt;&lt;code&gt;staged_predict&lt;/code&gt;&lt;/a&gt; method which returns a generator that yields the predictions at each stage. Plots like these can be used to determine the optimal number of trees (i.e. &lt;code&gt;n_estimators&lt;/code&gt;) by early stopping. The plot on the right shows the feature importances which can be obtained via the &lt;code&gt;feature_importances_&lt;/code&gt; property.</source>
          <target state="translated">以下の図は、最小二乗損失と500の基本学習器を持つ&lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; &lt;/a&gt;をボストンの住宅価格データセット（&lt;a href=&quot;generated/sklearn.datasets.load_boston#sklearn.datasets.load_boston&quot;&gt; &lt;code&gt;sklearn.datasets.load_boston&lt;/code&gt; &lt;/a&gt;）に適用した結果を示しています。左側のプロットは、各反復での学習誤差とテスト誤差を示しています。各反復での &lt;code&gt;train_score_&lt;/code&gt; は、勾配ブースティングモデルのtrain_score_属性に格納されます。各反復でのテストエラーは、各ステージで予測を生成するジェネレーターを返す&lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor.staged_predict&quot;&gt; &lt;code&gt;staged_predict&lt;/code&gt; &lt;/a&gt;メソッドを介して取得できます。このようなプロットを使用して、早期停止することにより、ツリー（つまり &lt;code&gt;n_estimators&lt;/code&gt; ）の最適な数を決定できます。右側のプロットは、機能の重要性を示しています。 &lt;code&gt;feature_importances_&lt;/code&gt; プロパティ。</target>
        </trans-unit>
        <trans-unit id="cc8de6d2f1f6034186ab0af10d91754eadd34954" translate="yes" xml:space="preserve">
          <source>The figure shows that both methods learn reasonable models of the target function. GPR correctly identifies the periodicity of the function to be roughly 2*pi (6.28), while KRR chooses the doubled periodicity 4*pi. Besides that, GPR provides reasonable confidence bounds on the prediction which are not available for KRR. A major difference between the two methods is the time required for fitting and predicting: while fitting KRR is fast in principle, the grid-search for hyperparameter optimization scales exponentially with the number of hyperparameters (&amp;ldquo;curse of dimensionality&amp;rdquo;). The gradient-based optimization of the parameters in GPR does not suffer from this exponential scaling and is thus considerable faster on this example with 3-dimensional hyperparameter space. The time for predicting is similar; however, generating the variance of the predictive distribution of GPR takes considerable longer than just predicting the mean.</source>
          <target state="translated">この図は、両方の方法がターゲット関数の合理的なモデルを学習することを示しています。 GPRは関数の周期性をおよそ2 * pi（6.28）であると正しく識別しますが、KRRは2倍の周期性4 * piを選択します。その上、GPRはKRRでは利用できない予測に対して妥当な信頼限界を提供します。 2つの方法の主な違いは、フィッティングと予測に必要な時間です。KRRのフィッティングは原理的に高速ですが、ハイパーパラメーター最適化のグリッド検索は、ハイパーパラメーターの数（「次元数の呪い」）に応じて指数関数的にスケーリングします。 GPRのパラメーターの勾配ベースの最適化は、この指数スケーリングの影響を受けないため、3次元ハイパーパラメーター空間を使用したこの例ではかなり高速になります。予測の時間は似ています。しかしながら、GPRの予測分布の分散の生成には、平均の予測よりもかなり時間がかかります。</target>
        </trans-unit>
        <trans-unit id="7a70cd6dc569a4ad4c8d12310e45e3bf56f13e7d" translate="yes" xml:space="preserve">
          <source>The figure shows that both methods learn reasonable models of the target function. GPR correctly identifies the periodicity of the function to be roughly \(2*\pi\) (6.28), while KRR chooses the doubled periodicity \(4*\pi\) . Besides that, GPR provides reasonable confidence bounds on the prediction which are not available for KRR. A major difference between the two methods is the time required for fitting and predicting: while fitting KRR is fast in principle, the grid-search for hyperparameter optimization scales exponentially with the number of hyperparameters (&amp;ldquo;curse of dimensionality&amp;rdquo;). The gradient-based optimization of the parameters in GPR does not suffer from this exponential scaling and is thus considerable faster on this example with 3-dimensional hyperparameter space. The time for predicting is similar; however, generating the variance of the predictive distribution of GPR takes considerable longer than just predicting the mean.</source>
          <target state="translated">この図は、両方の方法がターゲット関数の合理的なモデルを学習することを示しています。 GPRは関数の周期性をおよそ\（2 * \ pi \）（6.28）であると正しく識別しますが、KRRは2倍の周期性\（4 * \ pi \）を選択します。その上、GPRはKRRでは利用できない予測に対して妥当な信頼限界を提供します。 2つの方法の主な違いは、フィッティングと予測に必要な時間です。KRRのフィッティングは原理的に高速ですが、ハイパーパラメーター最適化のグリッド検索は、ハイパーパラメーターの数（「次元数の呪い」）に応じて指数関数的にスケーリングします。 GPRのパラメーターの勾配ベースの最適化は、この指数スケーリングの影響を受けないため、3次元ハイパーパラメーター空間を使用したこの例ではかなり高速になります。予測の時間は似ています。しかしながら、GPRの予測分布の分散の生成には、平均の予測よりもかなり時間がかかります。</target>
        </trans-unit>
        <trans-unit id="4c819c571e9cf27ccc7a880b7ae493959bac8667" translate="yes" xml:space="preserve">
          <source>The figure shows the cumulative sum of the negative OOB improvements as a function of the boosting iteration. As you can see, it tracks the test loss for the first hundred iterations but then diverges in a pessimistic way. The figure also shows the performance of 3-fold cross validation which usually gives a better estimate of the test loss but is computationally more demanding.</source>
          <target state="translated">図は、ブースティングの反復回数の関数としての負のOOB改善の累積和を示しています。ご覧のように、最初の100回の繰り返しでテスト損失を追跡しますが、その後は悲観的な方法で分岐します。この図はまた、通常はテスト損失のより良い推定値を提供しますが、計算量が多くなる3倍クロスバリデーションの性能を示しています。</target>
        </trans-unit>
        <trans-unit id="fc5bec6cdf25a030ca7f5736bc605af5657e6e59" translate="yes" xml:space="preserve">
          <source>The figures below are used to illustrate the effect of scaling our &lt;code&gt;C&lt;/code&gt; to compensate for the change in the number of samples, in the case of using an &lt;code&gt;l1&lt;/code&gt; penalty, as well as the &lt;code&gt;l2&lt;/code&gt; penalty.</source>
          <target state="translated">以下の図は、 &lt;code&gt;l1&lt;/code&gt; ペナルティと &lt;code&gt;l2&lt;/code&gt; ペナルティを使用する場合に、サンプル数の変化を補償するために &lt;code&gt;C&lt;/code&gt; をスケーリングする効果を示すために使用されます。</target>
        </trans-unit>
        <trans-unit id="0b0d592cc5daeb578d9b82714e3800c3092f375f" translate="yes" xml:space="preserve">
          <source>The figures illustrate the interpolating property of the Gaussian Process model as well as its probabilistic nature in the form of a pointwise 95% confidence interval.</source>
          <target state="translated">図は、ガウスプロセスモデルの補間特性とその確率的性質を、点単位の95%信頼区間の形で示しています。</target>
        </trans-unit>
        <trans-unit id="e96dfdb18e509ef4ea2f5731757d5c9fff16f17b" translate="yes" xml:space="preserve">
          <source>The figures show the confusion matrix with and without normalization by class support size (number of elements in each class). This kind of normalization can be interesting in case of class imbalance to have a more visual interpretation of which class is being misclassified.</source>
          <target state="translated">図は,クラスサポートサイズ(各クラスの要素数)による正規化の有無による混同行列を示しています.このような正規化は、クラスの不均衡の場合に、どのクラスが誤分類されているかをより視覚的に解釈できるようにするために興味深いものです。</target>
        </trans-unit>
        <trans-unit id="83d2d2e5c2f316a3531e949e8bac6210f7e21821" translate="yes" xml:space="preserve">
          <source>The files themselves are loaded in memory in the &lt;code&gt;data&lt;/code&gt; attribute. For reference the filenames are also available:</source>
          <target state="translated">ファイル自体は、 &lt;code&gt;data&lt;/code&gt; 属性でメモリに読み込まれます。参考のために、ファイル名も使用できます。</target>
        </trans-unit>
        <trans-unit id="4834eeb362cb8f80edd0fa52b738fe7db255a20e" translate="yes" xml:space="preserve">
          <source>The filesystem path to the root folder where MLComp datasets are stored, if mlcomp_root is None, the MLCOMP_DATASETS_HOME environment variable is looked up instead.</source>
          <target state="translated">mlcomp_root が None の場合は、MLCOMP_DATASETS_HOME 環境変数が代わりに検索されます。</target>
        </trans-unit>
        <trans-unit id="5e83705f6f5cf8453734b26cfa75b02cefbf9a06" translate="yes" xml:space="preserve">
          <source>The final sum of similarities is divided by the size of the larger set.</source>
          <target state="translated">最終的な類似性の合計は、より大きな集合の大きさで割られています。</target>
        </trans-unit>
        <trans-unit id="6023cd4bbdadd0261fe4fb93bbac4f79dd623983" translate="yes" xml:space="preserve">
          <source>The final value of the inertia criterion (sum of squared distances to the closest centroid for all observations in the training set).</source>
          <target state="translated">慣性基準の最終値(訓練セット内のすべてのオブザベーションの最も近いセントロイドまでの二乗距離の合計).</target>
        </trans-unit>
        <trans-unit id="65db46a7a2384cdd4de97d3c70536cd9da4c8616" translate="yes" xml:space="preserve">
          <source>The final value of the stress (sum of squared distance of the disparities and the distances for all constrained points).</source>
          <target state="translated">応力の最終値(視差の二乗距離と拘束されたすべての点の距離の和)。</target>
        </trans-unit>
        <trans-unit id="7e7a148ceab046969e5524f650bd9948ad6ae0ac" translate="yes" xml:space="preserve">
          <source>The first &lt;code&gt;[.9, .1]&lt;/code&gt; in &lt;code&gt;y_pred&lt;/code&gt; denotes 90% probability that the first sample has label 0. The log loss is non-negative.</source>
          <target state="translated">&lt;code&gt;y_pred&lt;/code&gt; の最初の &lt;code&gt;[.9, .1]&lt;/code&gt; は、最初のサンプルのラベルが0である確率が90％であることを示します。ログの損失は負ではありません。</target>
        </trans-unit>
        <trans-unit id="c41f4a219241dbe01041bd4816c28f057f1f8fc9" translate="yes" xml:space="preserve">
          <source>The first &lt;code&gt;n_samples % n_splits&lt;/code&gt; folds have size &lt;code&gt;n_samples // n_splits + 1&lt;/code&gt;, other folds have size &lt;code&gt;n_samples // n_splits&lt;/code&gt;, where &lt;code&gt;n_samples&lt;/code&gt; is the number of samples.</source>
          <target state="translated">最初の &lt;code&gt;n_samples % n_splits&lt;/code&gt; フォールドのサイズは &lt;code&gt;n_samples // n_splits + 1&lt;/code&gt; 、他のフォールドのサイズは &lt;code&gt;n_samples // n_splits&lt;/code&gt; 。ここで、 &lt;code&gt;n_samples&lt;/code&gt; はサンプルの数です。</target>
        </trans-unit>
        <trans-unit id="318062ceb48883ba19b024b3ac85479b5d766c8b" translate="yes" xml:space="preserve">
          <source>The first array returned contains the distances to all points which are closer than 1.6, while the second array returned contains their indices. In general, multiple points can be queried at the same time.</source>
          <target state="translated">返される最初の配列には、1.6よりも近いすべての点までの距離が含まれ、2番目の配列にはそのインデックスが含まれます。一般的に、複数の点を同時に問い合わせることができます。</target>
        </trans-unit>
        <trans-unit id="b072f11f22e8fd40f8f011ced740f859610c5839" translate="yes" xml:space="preserve">
          <source>The first base-kernel of the product-kernel</source>
          <target state="translated">製品カーネルの最初のベースカーネル</target>
        </trans-unit>
        <trans-unit id="2843deb348cef2ebb64b5cb6c478da3efa931340" translate="yes" xml:space="preserve">
          <source>The first base-kernel of the sum-kernel</source>
          <target state="translated">和カーネルの第一基底カーネル</target>
        </trans-unit>
        <trans-unit id="23baf5dbd5fca89380aba285e5a6dca77d9ff4b8" translate="yes" xml:space="preserve">
          <source>The first column of images shows true faces. The next columns illustrate how extremely randomized trees, k nearest neighbors, linear regression and ridge regression complete the lower half of those faces.</source>
          <target state="translated">画像の最初の列は真の顔を示しています。次の列は、極端にランダム化された木、k個の最も近い隣人、線形回帰、尾根回帰が、これらの顔の下半分をどのように完成させるかを示しています。</target>
        </trans-unit>
        <trans-unit id="758127cc4d0b762bc4a77876dd4551f141ccd7cd" translate="yes" xml:space="preserve">
          <source>The first corresponds to a model with a high noise level and a large length scale, which explains all variations in the data by noise.</source>
          <target state="translated">1つ目は、ノイズレベルが高く、長さスケールが大きいモデルに対応しており、ノイズによるデータの変動をすべて説明しています。</target>
        </trans-unit>
        <trans-unit id="2d71e99749591e16661c08d2c19738355ed2fad8" translate="yes" xml:space="preserve">
          <source>The first element of each line can be used to store a target variable to predict.</source>
          <target state="translated">各行の最初の要素は、予測する対象変数を格納するために使用することができます。</target>
        </trans-unit>
        <trans-unit id="d151d2a5867c5ba125b375d964535691cae0b0d6" translate="yes" xml:space="preserve">
          <source>The first example illustrates how robust covariance estimation can help concentrating on a relevant cluster when another one exists. Here, many observations are confounded into one and break down the empirical covariance estimation. Of course, some screening tools would have pointed out the presence of two clusters (Support Vector Machines, Gaussian Mixture Models, univariate outlier detection, &amp;hellip;). But had it been a high-dimensional example, none of these could be applied that easily.</source>
          <target state="translated">最初の例は、堅牢な共分散推定が、別のクラスターが存在する場合に関連クラスターに集中するのにどのように役立つかを示しています。ここでは、多くの観察結果が1つに混同されており、経験的共分散推定を分析しています。もちろん、いくつかのスクリーニングツールは、2つのクラスター（サポートベクターマシン、ガウス混合モデル、一変量外れ値検出など）の存在を指摘します。しかし、それが高次元の例であったとしたら、これらのどれもそれほど簡単には適用できませんでした。</target>
        </trans-unit>
        <trans-unit id="ecfbbb8300d863a918a602e1c4f90841d4b71a3f" translate="yes" xml:space="preserve">
          <source>The first loader is used for the Face Identification task: a multi-class classification task (hence supervised learning):</source>
          <target state="translated">最初のローダは、顔識別タスクに使用されます:マルチクラス分類タスク(したがって教師付き学習)。</target>
        </trans-unit>
        <trans-unit id="1272790baab931c65e23f13d4b17128820578899" translate="yes" xml:space="preserve">
          <source>The first model is a classical Gaussian Mixture Model with 10 components fit with the Expectation-Maximization algorithm.</source>
          <target state="translated">最初のモデルは、10個の成分を持つ古典的なガウス混合モデルで、期待値最大化アルゴリズムを用いて適合させます。</target>
        </trans-unit>
        <trans-unit id="a28da4c4f339f2d2823befd3907c74a3bd7b1459" translate="yes" xml:space="preserve">
          <source>The first plot is a visualization of the decision function for a variety of parameter values on a simplified classification problem involving only 2 input features and 2 possible target classes (binary classification). Note that this kind of plot is not possible to do for problems with more features or target classes.</source>
          <target state="translated">最初のプロットは、2つの入力特徴と2つの可能なターゲット・クラス(2値分類)のみを含む単純化された分類問題における、さまざまなパラメータ値の決定関数の可視化です。この種のプロットは、より多くの特徴や対象クラスを持つ問題ではできないことに注意してください。</target>
        </trans-unit>
        <trans-unit id="fdb1a20b80ba5f009466ef7acd3785afda979734" translate="yes" xml:space="preserve">
          <source>The first plot shows one of the problems with using histograms to visualize the density of points in 1D. Intuitively, a histogram can be thought of as a scheme in which a unit &amp;ldquo;block&amp;rdquo; is stacked above each point on a regular grid. As the top two panels show, however, the choice of gridding for these blocks can lead to wildly divergent ideas about the underlying shape of the density distribution. If we instead center each block on the point it represents, we get the estimate shown in the bottom left panel. This is a kernel density estimation with a &amp;ldquo;top hat&amp;rdquo; kernel. This idea can be generalized to other kernel shapes: the bottom-right panel of the first figure shows a Gaussian kernel density estimate over the same distribution.</source>
          <target state="translated">最初のプロットは、ヒストグラムを使用してポイントの密度を1Dで視覚化する際の問題の1つを示しています。直感的に、ヒストグラムは、単位「ブロック」が規則的なグリッドの各ポイントの上に積み上げられるスキームと考えることができます。ただし、上部の2つのパネルが示すように、これらのブロックのグリッドを選択すると、密度分布の基になる形状について、非常に多様なアイデアが生じる可能性があります。代わりに、各ブロックをそれが表す点の中央に配置すると、左下のパネルに推定値が表示されます。これは、「シルクハット」カーネルを使用したカーネル密度推定です。このアイデアは、他のカーネル形状に一般化できます。最初の図の右下のパネルは、同じ分布に対するガウスカーネル密度推定を示しています。</target>
        </trans-unit>
        <trans-unit id="80a6f35d6de94c1655a0ab570a2d81b8e9f0c281" translate="yes" xml:space="preserve">
          <source>The first plot shows that with an increasing number of samples &lt;code&gt;n_samples&lt;/code&gt;, the minimal number of dimensions &lt;code&gt;n_components&lt;/code&gt; increased logarithmically in order to guarantee an &lt;code&gt;eps&lt;/code&gt;-embedding.</source>
          <target state="translated">サンプルの数が増加していることを最初のプロットが示す &lt;code&gt;n_samples&lt;/code&gt; 、寸法の最小数 &lt;code&gt;n_components&lt;/code&gt; を保証するために対数的に増加 &lt;code&gt;eps&lt;/code&gt; -Embeddingを。</target>
        </trans-unit>
        <trans-unit id="0fa5069414b33f645c4a3a3261f97c50a380bf97" translate="yes" xml:space="preserve">
          <source>The first plot shows the best inertia reached for each combination of the model (&lt;code&gt;KMeans&lt;/code&gt; or &lt;code&gt;MiniBatchKMeans&lt;/code&gt;) and the init method (&lt;code&gt;init=&quot;random&quot;&lt;/code&gt; or &lt;code&gt;init=&quot;kmeans++&quot;&lt;/code&gt;) for increasing values of the &lt;code&gt;n_init&lt;/code&gt; parameter that controls the number of initializations.</source>
          <target state="translated">最初のプロットは、モデル（ &lt;code&gt;KMeans&lt;/code&gt; または &lt;code&gt;MiniBatchKMeans&lt;/code&gt; ）とinitメソッド（ &lt;code&gt;init=&quot;random&quot;&lt;/code&gt; または &lt;code&gt;init=&quot;kmeans++&quot;&lt;/code&gt; ）の各組み合わせで到達した最適な慣性を示し、初期化の数を制御する &lt;code&gt;n_init&lt;/code&gt; パラメーターの値を増やしています。</target>
        </trans-unit>
        <trans-unit id="35a5988878bfd98d1cc6f738d4af80878102b501" translate="yes" xml:space="preserve">
          <source>The first row of output array indicates that there are three samples whose true cluster is &amp;ldquo;a&amp;rdquo;. Of them, two are in predicted cluster 0, one is in 1, and none is in 2. And the second row indicates that there are three samples whose true cluster is &amp;ldquo;b&amp;rdquo;. Of them, none is in predicted cluster 0, one is in 1 and two are in 2.</source>
          <target state="translated">出力配列の最初の行は、真のクラスターが「a」である3つのサンプルがあることを示しています。それらのうち、2つは予測クラスター0にあり、1つは1にあり、1つは2にありません。2行目は、真のクラスターが「b」である3つのサンプルがあることを示しています。それらのうち、どれも予測クラスター0にありません。1つは1にあり、2つは2にあります。</target>
        </trans-unit>
        <trans-unit id="94d22c244cece5d36684b54eb3a3c36ef460564d" translate="yes" xml:space="preserve">
          <source>The first two loss functions are lazy, they only update the model parameters if an example violates the margin constraint, which makes training very efficient and may result in sparser models, even when L2 penalty is used.</source>
          <target state="translated">最初の2つの損失関数は怠惰で、例がマージン制約に違反した場合にのみモデルパラメータを更新するため、訓練が非常に効率的になり、L2ペナルティが使用されている場合でも、よりスパースなモデルが得られる可能性があります。</target>
        </trans-unit>
        <trans-unit id="865ac3c349894331005e5c9a9918fbe1c4576705" translate="yes" xml:space="preserve">
          <source>The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.</source>
          <target state="translated">また、フィットしたモデルは、最も識別性の高い方向に投影することで、入力の次元を下げるために使用することができます。</target>
        </trans-unit>
        <trans-unit id="47ade7eb6fd49b2a025d9a87896bfcf3ba24402d" translate="yes" xml:space="preserve">
          <source>The fitted model.</source>
          <target state="translated">フィットしたモデル。</target>
        </trans-unit>
        <trans-unit id="14120dbe50cfe7a93d61eb3782205eb1e9322e9d" translate="yes" xml:space="preserve">
          <source>The flexibility of controlling the smoothness of the learned function via \(\nu\) allows adapting to the properties of the true underlying functional relation. The prior and posterior of a GP resulting from a Mat&amp;eacute;rn kernel are shown in the following figure:</source>
          <target state="translated">\（\ nu \）を介して学習された関数の滑らかさを制御する柔軟性により、真の基本的な関数関係のプロパティに適応できます。次の図は、Mat&amp;eacute;rnカーネルからのGPの前後を示しています。</target>
        </trans-unit>
        <trans-unit id="130cb7c407aba5d31bddc566759f6c2692fa934a" translate="yes" xml:space="preserve">
          <source>The flowchart below is designed to give users a bit of a rough guide on how to approach problems with regard to which estimators to try on your data.</source>
          <target state="translated">以下のフローチャートは、どの推定値をデータ上で試すべきかについて、どのように問題にアプローチするかについて、ユーザーに少しだけ大まかなガイドを与えるように設計されています。</target>
        </trans-unit>
        <trans-unit id="f162534c96a36fe6b7c86b6775d49d882723bf35" translate="yes" xml:space="preserve">
          <source>The folder names are used as supervised signal label names. The individual file names are not important.</source>
          <target state="translated">フォルダ名は、スーパーバイズドシグナルのラベル名として使用されます。個々のファイル名は重要ではありません。</target>
        </trans-unit>
        <trans-unit id="f2db8b6a5929e1458b6943c5d307e1ce8a02db89" translate="yes" xml:space="preserve">
          <source>The folds are approximately balanced in the sense that the number of distinct groups is approximately the same in each fold.</source>
          <target state="translated">それぞれの折り目の中で、はっきりとしたグループの数がほぼ同じという意味では、ほぼバランスが取れています。</target>
        </trans-unit>
        <trans-unit id="b43c7b928f1a786f0281c067a04292ccfddc7963" translate="yes" xml:space="preserve">
          <source>The following are a set of methods intended for regression in which the target value is expected to be a linear combination of the input variables. In mathematical notion, if \(\hat{y}\) is the predicted value.</source>
          <target state="translated">以下は、目標値が入力変数の線形結合であると予想される回帰のための方法のセットである。数学的な概念では、もし \(\hat{y}\)が予測値であれば、次のようになります。</target>
        </trans-unit>
        <trans-unit id="81301a81f070c1a1a05c02b69a367906c9b88d21" translate="yes" xml:space="preserve">
          <source>The following clustering assignment is slightly better, since it is homogeneous but not complete:</source>
          <target state="translated">以下のクラスタリング割り当ては、均質ではあるが完全ではないので、やや良い。</target>
        </trans-unit>
        <trans-unit id="563a9a6f3a15e2fd51ba9e9b647870430bf12e58" translate="yes" xml:space="preserve">
          <source>The following code defines a linear kernel and creates a classifier instance that will use that kernel:</source>
          <target state="translated">以下のコードは,線形カーネルを定義し,そのカーネルを使用する分類器インスタンスを作成します.</target>
        </trans-unit>
        <trans-unit id="b8e362395586349d1fcb8b526001a536e06f5d5d" translate="yes" xml:space="preserve">
          <source>The following code is a bit verbose, feel free to jump directly to the analysis of the &lt;a href=&quot;#results&quot;&gt;results&lt;/a&gt;.</source>
          <target state="translated">次のコードは少し冗長です。&lt;a href=&quot;#results&quot;&gt;結果の&lt;/a&gt;分析に直接進んでください。</target>
        </trans-unit>
        <trans-unit id="b414b22c1d4fa210fe910463f00d0b843e42262f" translate="yes" xml:space="preserve">
          <source>The following cross-validation splitters can be used to do that. The grouping identifier for the samples is specified via the &lt;code&gt;groups&lt;/code&gt; parameter.</source>
          <target state="translated">これを行うには、次の相互検証スプリッターを使用できます。サンプルのグループ化識別子は &lt;code&gt;groups&lt;/code&gt; パラメータを介して指定されます。</target>
        </trans-unit>
        <trans-unit id="6757b5cfa24458748d25f81af60d324180a74dc4" translate="yes" xml:space="preserve">
          <source>The following cross-validators can be used in such cases.</source>
          <target state="translated">このような場合には、以下のようなクロスバリデーターを使用することができます。</target>
        </trans-unit>
        <trans-unit id="6da3fe3659ba9973062031cbded9ac1ee9e80559" translate="yes" xml:space="preserve">
          <source>The following dataset has integer features, two of which are the same in every sample. These are removed with the default setting for threshold:</source>
          <target state="translated">以下のデータセットは,整数の特徴量を持ち,そのうちの2つはすべてのサンプルで同じである.これらは,しきい値のデフォルト設定で削除されます.</target>
        </trans-unit>
        <trans-unit id="f50629b49593a5773731e8a4d6d7fbab3d06afa6" translate="yes" xml:space="preserve">
          <source>The following example demonstrates how to estimate the accuracy of a linear kernel support vector machine on the iris dataset by splitting the data, fitting a model and computing the score 5 consecutive times (with different splits each time):</source>
          <target state="translated">以下の例は、データを分割し、モデルをフィットさせ、スコアを5回連続して計算することで、虹彩データセット上の線形カーネルサポートベクターマシンの精度を推定する方法を示しています(毎回異なる分割を行う)。</target>
        </trans-unit>
        <trans-unit id="50761302c7bb4483750fc53752f14a1bf26cb023" translate="yes" xml:space="preserve">
          <source>The following example illustrates 16 components extracted using sparse PCA from the Olivetti faces dataset. It can be seen how the regularization term induces many zeros. Furthermore, the natural structure of the data causes the non-zero coefficients to be vertically adjacent. The model does not enforce this mathematically: each component is a vector \(h \in \mathbf{R}^{4096}\), and there is no notion of vertical adjacency except during the human-friendly visualization as 64x64 pixel images. The fact that the components shown below appear local is the effect of the inherent structure of the data, which makes such local patterns minimize reconstruction error. There exist sparsity-inducing norms that take into account adjacency and different kinds of structure; see &lt;a href=&quot;#jen09&quot; id=&quot;id2&quot;&gt;[Jen09]&lt;/a&gt; for a review of such methods. For more details on how to use Sparse PCA, see the Examples section, below.</source>
          <target state="translated">次の例は、Olivetti顔データセットからスパースPCAを使用して抽出された16個のコンポーネントを示しています。正則化項がどのように多くのゼロを引き起こすかがわかります。さらに、データの自然な構造により、ゼロ以外の係数が垂直方向に隣接します。モデルはこれを数学的に強制しません。各コンポーネントはベクトル\（h \ in \ mathbf {R} ^ {4096} \）であり、64x64ピクセルの画像として人間にわかりやすい視覚化の間を除いて、垂直隣接の概念はありません。以下に示すコンポーネントがローカルに見えるという事実は、データの固有の構造の影響であり、そのため、このようなローカルパターンは再構成エラーを最小限に抑えます。隣接性とさまざまな種類の構造を考慮したスパース性を誘発する規範が存在します。&lt;a href=&quot;#jen09&quot; id=&quot;id2&quot;&gt;[Jen09]を&lt;/a&gt;参照そのような方法のレビューのため。スパースPCAの使用方法の詳細については、以下の「例」セクションを参照してください。</target>
        </trans-unit>
        <trans-unit id="0b4e5d16e2ae8075091f7ec9c797f8dd043b5cfe" translate="yes" xml:space="preserve">
          <source>The following example illustrates how the decision regions may change when a soft &lt;code&gt;VotingClassifier&lt;/code&gt; is used based on an linear Support Vector Machine, a Decision Tree, and a K-nearest neighbor classifier:</source>
          <target state="translated">次の例は、線形サポートベクターマシン、ディシジョンツリー、およびK最近傍分類子に基づいてソフト &lt;code&gt;VotingClassifier&lt;/code&gt; を使用した場合に、決定領域がどのように変化するかを示しています。</target>
        </trans-unit>
        <trans-unit id="fcb0630f60eb0a1b1e5514ec57d9a7f6cbd21ce7" translate="yes" xml:space="preserve">
          <source>The following example illustrates the effect of scaling the regularization parameter when using &lt;a href=&quot;../../modules/svm#svm&quot;&gt;Support Vector Machines&lt;/a&gt; for &lt;a href=&quot;../../modules/svm#svm-classification&quot;&gt;classification&lt;/a&gt;. For SVC classification, we are interested in a risk minimization for the equation:</source>
          <target state="translated">次の例は、&lt;a href=&quot;../../modules/svm#svm-classification&quot;&gt;分類に&lt;/a&gt;&lt;a href=&quot;../../modules/svm#svm&quot;&gt;サポートベクターマシン&lt;/a&gt;を使用する場合の正則化パラメーターのスケーリングの効果を示しています。SVC分類の場合、次の方程式のリスク最小化に関心があります。</target>
        </trans-unit>
        <trans-unit id="35227ebb51bdeaa9a401c5e44c0ca9ca35e1f0e6" translate="yes" xml:space="preserve">
          <source>The following example shows a color-coded representation of the relative importances of each individual pixel for a face recognition task using a &lt;a href=&quot;generated/sklearn.ensemble.extratreesclassifier#sklearn.ensemble.ExtraTreesClassifier&quot;&gt;&lt;code&gt;ExtraTreesClassifier&lt;/code&gt;&lt;/a&gt; model.</source>
          <target state="translated">次の例は、&lt;a href=&quot;generated/sklearn.ensemble.extratreesclassifier#sklearn.ensemble.ExtraTreesClassifier&quot;&gt; &lt;code&gt;ExtraTreesClassifier&lt;/code&gt; &lt;/a&gt;モデルを使用した顔認識タスクの各ピクセルの相対的な重要度を色分けして示しています。</target>
        </trans-unit>
        <trans-unit id="e4e3a609b2ed09432a121fc3917b934875c3544c" translate="yes" xml:space="preserve">
          <source>The following example shows how to fit an AdaBoost classifier with 100 weak learners:</source>
          <target state="translated">次の例では、AdaBoost分類器を100人の弱い学習者にフィットさせる方法を示しています。</target>
        </trans-unit>
        <trans-unit id="486be98f63ccda08601d245e4a85066d3abba297" translate="yes" xml:space="preserve">
          <source>The following example shows how to fit the majority rule classifier:</source>
          <target state="translated">以下の例では、多数決分類器を適合させる方法を示しています。</target>
        </trans-unit>
        <trans-unit id="376fe34989034d77dfd504e8201c3d4b2dfa1573" translate="yes" xml:space="preserve">
          <source>The following example shows how to retrieve the 5 right informative features in the Friedman #1 dataset.</source>
          <target state="translated">以下の例では、Friedman #1データセットの中の5つの右の情報的特徴を取得する方法を示しています。</target>
        </trans-unit>
        <trans-unit id="25b5efc453e06d92e8572618fefd1930c5c8aab3" translate="yes" xml:space="preserve">
          <source>The following example shows how to retrieve the a-priori not known 5 informative features in the Friedman #1 dataset.</source>
          <target state="translated">以下の例では、Friedman #1データセットの中の事前に知られていない5つの情報特徴量を取得する方法を示します。</target>
        </trans-unit>
        <trans-unit id="e99af0f029f79715308d4d1c9ecbe2acb8983e4d" translate="yes" xml:space="preserve">
          <source>The following example will, for instance, transform some British spelling to American spelling:</source>
          <target state="translated">次の例では、例えば、いくつかのイギリスのスペルをアメリカのスペルに変換します。</target>
        </trans-unit>
        <trans-unit id="ad9262c5496a82045b0e3b64071f384beed06659" translate="yes" xml:space="preserve">
          <source>The following experiment is performed on an artificial dataset for binary classification with 100,000 samples (1,000 of them are used for model fitting) with 20 features. Of the 20 features, only 2 are informative and 10 are redundant. The figure shows the estimated probabilities obtained with logistic regression, a linear support-vector classifier (SVC), and linear SVC with both isotonic calibration and sigmoid calibration. The Brier score is a metric which is a combination of calibration loss and refinement loss, &lt;a href=&quot;generated/sklearn.metrics.brier_score_loss#sklearn.metrics.brier_score_loss&quot;&gt;&lt;code&gt;brier_score_loss&lt;/code&gt;&lt;/a&gt;, reported in the legend (the smaller the better). Calibration loss is defined as the mean squared deviation from empirical probabilities derived from the slope of ROC segments. Refinement loss can be defined as the expected optimal loss as measured by the area under the optimal cost curve.</source>
          <target state="translated">次の実験は、20の特徴を持つ100,000個のサンプル（そのうち1000個はモデルのフィッティングに使用されます）を使用したバイナリ分類の人工データセットで実行されます。20の機能のうち、2つだけが有益であり、10は冗長です。この図は、ロジスティック回帰、線形サポートベクトル分類器（SVC）、および等張性キャリブレーションとシグモイドキャリブレーションの両方を備えた線形SVCで得られた推定確率を示しています。ブライアスコアは、キャリブレーション損失とリファインメント損失の組み合わせであるメトリック、&lt;a href=&quot;generated/sklearn.metrics.brier_score_loss#sklearn.metrics.brier_score_loss&quot;&gt; &lt;code&gt;brier_score_loss&lt;/code&gt; です。&lt;/a&gt;、凡例で報告されている（小さいほど良い）。校正損失は、ROCセグメントの傾きから導き出された経験的確率からの偏差の二乗平均として定義されます。リファインメントロスは、最適なコストカーブの下の領域で測定される、予想される最適なロスとして定義できます。</target>
        </trans-unit>
        <trans-unit id="2b3bbe94b0947452c8fc360afc3d0e6f8832ae22" translate="yes" xml:space="preserve">
          <source>The following figure compares &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;SVR&lt;/code&gt; on an artificial dataset, which consists of a sinusoidal target function and strong noise added to every fifth datapoint. The learned model of &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;SVR&lt;/code&gt; is plotted, where both complexity/regularization and bandwidth of the RBF kernel have been optimized using grid-search. The learned functions are very similar; however, fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is approx. seven times faster than fitting &lt;code&gt;SVR&lt;/code&gt; (both with grid-search). However, prediction of 100000 target values is more than three times faster with SVR since it has learned a sparse model using only approx. 1/3 of the 100 training datapoints as support vectors.</source>
          <target state="translated">次の図は、正弦波のターゲット関数と5番目ごとのデータ&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt;追加された強いノイズで構成される人工データセットのKernelRidgeと &lt;code&gt;SVR&lt;/code&gt; を比較しています。&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt;と &lt;code&gt;SVR&lt;/code&gt; の学習モデルがプロットされ、RBFカーネルの複雑さ/正規化と帯域幅の両方がグリッド検索を使用して最適化されています。学習した機能は非常に似ています。ただし、&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; の&lt;/a&gt;フィッティングは約です。 &lt;code&gt;SVR&lt;/code&gt; のフィッティングよりも7倍高速です（両方ともグリッド検索を使用）。ただし、SVRを使用すると、約100,000だけを使用してスパースモデルが学習されるため、100,000のターゲット値の予測は3倍以上速くなります。サポートベクターとしての100トレーニングデータポイントの1/3。</target>
        </trans-unit>
        <trans-unit id="b11b5ffd2f0d8dbd878722867b9b243b2e73a5ea" translate="yes" xml:space="preserve">
          <source>The following figure compares the location of the non-zeros in W obtained with a simple Lasso or a MultiTaskLasso. The Lasso estimates yields scattered non-zeros while the non-zeros of the MultiTaskLasso are full columns.</source>
          <target state="translated">次の図は、単純なLassoとMultiTaskLassoで得られたWの非ゼロの位置を比較したものです。Lassoの推定では、散在した非ゼロが得られますが、MultiTaskLassoの非ゼロは完全な列になります。</target>
        </trans-unit>
        <trans-unit id="c66df17d5ba5efe635efde7f93de1fc62e75c222" translate="yes" xml:space="preserve">
          <source>The following figure illustrates both methods on an artificial dataset, which consists of a sinusoidal target function and strong noise. The figure compares the learned model of KRR and GPR based on a ExpSineSquared kernel, which is suited for learning periodic functions. The kernel&amp;rsquo;s hyperparameters control the smoothness (length_scale) and periodicity of the kernel (periodicity). Moreover, the noise level of the data is learned explicitly by GPR by an additional WhiteKernel component in the kernel and by the regularization parameter alpha of KRR.</source>
          <target state="translated">次の図は、正弦波のターゲット関数と強いノイズで構成される人工データセットの両方の方法を示しています。この図は、周期関数の学習に適したExpSineSquaredカーネルに基づいて、KRRとGPRの学習モデルを比較しています。カーネルのハイパーパラメーターは、カーネルの滑らかさ（length_scale）と周期性（周期性）を制御します。さらに、データのノイズレベルは、カーネル内の追加のWhiteKernelコンポーネントとGRRの正則化パラメーターalphaによってGPRによって明示的に学習されます。</target>
        </trans-unit>
        <trans-unit id="c2a01fd3fe0b5b9cd28910783764aacd71509b19" translate="yes" xml:space="preserve">
          <source>The following illustrate the probability density functions of the target before and after applying the logarithmic functions.</source>
          <target state="translated">以下に、対数関数を適用する前と適用後の対象の確率密度関数を説明する。</target>
        </trans-unit>
        <trans-unit id="b0293fdb9cc21af9db21d1f9f61ccde2a4565147" translate="yes" xml:space="preserve">
          <source>The following image shows how a dictionary learned from 4x4 pixel image patches extracted from part of the image of a raccoon face looks like.</source>
          <target state="translated">下の画像は、タヌキの顔の画像の一部から抽出した4×4ピクセルの画像パッチから学習した辞書がどのように見えるかを示しています。</target>
        </trans-unit>
        <trans-unit id="a988a0a50289dff96522065b976c597b6e4e63e8" translate="yes" xml:space="preserve">
          <source>The following image shows on the data above the estimated probability using a Gaussian naive Bayes classifier without calibration, with a sigmoid calibration and with a non-parametric isotonic calibration. One can observe that the non-parametric model provides the most accurate probability estimates for samples in the middle, i.e., 0.5.</source>
          <target state="translated">次の画像は、校正なし、シグモイド校正あり、ノンパラメトリック等張校正ありのガウスナイーブベイズ分類器を用いた推定確率を上のデータ上に示しています。ノンパラメトリックモデルが、中間の標本、すなわち0.5の標本に対して最も正確な確率推定値を提供していることがわかります。</target>
        </trans-unit>
        <trans-unit id="b45c55863d114ed9459a8475837831820ff65766" translate="yes" xml:space="preserve">
          <source>The following images demonstrate the benefit of probability calibration. The first image present a dataset with 2 classes and 3 blobs of data. The blob in the middle contains random samples of each class. The probability for the samples in this blob should be 0.5.</source>
          <target state="translated">以下の画像は、確率校正の利点を示しています。最初の画像は、2つのクラスと3つのブロブからなるデータセットを示しています。中央のブロブには、各クラスのランダムなサンプルが含まれています。このブロブのサンプルの確率は0.5でなければなりません。</target>
        </trans-unit>
        <trans-unit id="807c0e6975f025a025e2951d9c9164c1f3454c71" translate="yes" xml:space="preserve">
          <source>The following loss functions are supported and can be specified using the parameter &lt;code&gt;loss&lt;/code&gt;:</source>
          <target state="translated">以下の損失関数がサポートされており、パラメーター &lt;code&gt;loss&lt;/code&gt; を使用して指定できます。</target>
        </trans-unit>
        <trans-unit id="70624e811991b8fac875c2f0dd0db6a8fb2e1e2b" translate="yes" xml:space="preserve">
          <source>The following plots demonstrate the impact of the number of clusters and number of samples on various clustering performance evaluation metrics.</source>
          <target state="translated">以下のプロットは、クラスタ数とサンプル数が様々なクラスタリング性能評価指標に与える影響を示しています。</target>
        </trans-unit>
        <trans-unit id="661cd7d1cca4025c0fd4f4a1e938e7140f6ef24b" translate="yes" xml:space="preserve">
          <source>The following sections contain further explanations and examples that illustrate how the tf-idfs are computed exactly and how the tf-idfs computed in scikit-learn&amp;rsquo;s &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt;&lt;code&gt;TfidfTransformer&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt;&lt;code&gt;TfidfVectorizer&lt;/code&gt;&lt;/a&gt; differ slightly from the standard textbook notation that defines the idf as</source>
          <target state="translated">以下のセクションには、tf-idfsが正確に計算される方法と、scikit-learnの&lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt; &lt;code&gt;TfidfTransformer&lt;/code&gt; &lt;/a&gt;および&lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt; &lt;code&gt;TfidfVectorizer&lt;/code&gt; で&lt;/a&gt;計算されるtf-idfsが、idfを</target>
        </trans-unit>
        <trans-unit id="78ad5f101e233b65633dffc5c68bec29c1eae0b0" translate="yes" xml:space="preserve">
          <source>The following sections list utilities to generate indices that can be used to generate dataset splits according to different cross validation strategies.</source>
          <target state="translated">以下のセクションでは、異なるクロスバリデーション戦略に応じてデータセットの分割を生成するために使用できるインデックスを生成するためのユーティリティをリストアップします。</target>
        </trans-unit>
        <trans-unit id="5c75d78bf5e5b93a438a72e22f9eee2db09a82ed" translate="yes" xml:space="preserve">
          <source>The following snippet demonstrates how to replace missing values, encoded as &lt;code&gt;np.nan&lt;/code&gt;, using the mean value of the columns (axis 0) that contain the missing values:</source>
          <target state="translated">次のスニペットは、欠損値を含む列（軸0）の平均値を使用して、 &lt;code&gt;np.nan&lt;/code&gt; としてエンコードされた欠損値を置き換える方法を示しています。</target>
        </trans-unit>
        <trans-unit id="46781e9d9b616f25e94b9f1718ce2c06cffa693e" translate="yes" xml:space="preserve">
          <source>The following two references explain the iterations used in the coordinate descent solver of scikit-learn, as well as the duality gap computation used for convergence control.</source>
          <target state="translated">以下の2つの参考文献は、scikit-learnの座標降下ソルバーで使用される反復と、収束制御に使用される二元性ギャップ計算について説明しています。</target>
        </trans-unit>
        <trans-unit id="d492404a63c6d65da32b70e6dcf87e75a94b6a4d" translate="yes" xml:space="preserve">
          <source>The form of the model learned by &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is identical to support vector regression (&lt;code&gt;SVR&lt;/code&gt;). However, different loss functions are used: KRR uses squared error loss while support vector regression uses \(\epsilon\)-insensitive loss, both combined with l2 regularization. In contrast to &lt;code&gt;SVR&lt;/code&gt;, fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; can be done in closed-form and is typically faster for medium-sized datasets. On the other hand, the learned model is non-sparse and thus slower than SVR, which learns a sparse model for \(\epsilon &amp;gt; 0\), at prediction-time.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt;によって学習されたモデルの形式は、サポートベクトル回帰（ &lt;code&gt;SVR&lt;/code&gt; ）と同じです。ただし、異なる損失関数が使用されます。KRRは二乗誤差損失を使用し、サポートベクトル回帰は\（\ epsilon \）に依存しない損失を使用します。どちらもl2正則化と組み合わせて使用​​されます。 &lt;code&gt;SVR&lt;/code&gt; とは異なり、&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; の&lt;/a&gt;フィッティングは閉じた形式で行うことができ、通常、中規模のデータセットの方が高速です。一方、学習されたモデルはスパースではないため、予測時に\（\ epsilon&amp;gt; 0 \）のスパースモデルを学習するSVRよりも低速です。</target>
        </trans-unit>
        <trans-unit id="cb6eb14fd1b6ffac38e7d6c15d36b7076beafa85" translate="yes" xml:space="preserve">
          <source>The form of the model learned by KRR is identical to support vector regression (SVR). However, different loss functions are used: KRR uses squared error loss while support vector regression uses epsilon-insensitive loss, both combined with l2 regularization. In contrast to SVR, fitting a KRR model can be done in closed-form and is typically faster for medium-sized datasets. On the other hand, the learned model is non-sparse and thus slower than SVR, which learns a sparse model for epsilon &amp;gt; 0, at prediction-time.</source>
          <target state="translated">KRRによって学習されたモデルの形式は、サポートベクトル回帰（SVR）と同じです。ただし、異なる損失関数が使用されます。KRRでは二乗誤差損失が使用され、サポートベクトル回帰ではイプシロンに依存しない損失が使用されます。両方ともl2正則化と組み合わせて使用​​されます。 SVRとは対照的に、KRRモデルの近似は閉じた形式で行うことができ、通常、中規模のデータセットの方が高速です。一方、学習されたモデルは非スパースであるため、&amp;epsilon;が0より大きいスパースモデルを予測時に学習するSVRよりも遅くなります。</target>
        </trans-unit>
        <trans-unit id="785bfd19f7d4d298d07ec2673131bad97bee7c7c" translate="yes" xml:space="preserve">
          <source>The form of these kernels is as follows:</source>
          <target state="translated">これらのカーネルの形態は以下の通りである。</target>
        </trans-unit>
        <trans-unit id="99fa4cbad7a403537a73165a2b1d717c58bb9b6b" translate="yes" xml:space="preserve">
          <source>The formula that is used to compute the tf-idf of term t is tf-idf(d, t) = tf(t) * idf(d, t), and the idf is computed as idf(d, t) = log [ n / df(d, t) ] + 1 (if &lt;code&gt;smooth_idf=False&lt;/code&gt;), where n is the total number of documents and df(d, t) is the document frequency; the document frequency is the number of documents d that contain term t. The effect of adding &amp;ldquo;1&amp;rdquo; to the idf in the equation above is that terms with zero idf, i.e., terms that occur in all documents in a training set, will not be entirely ignored. (Note that the idf formula above differs from the standard textbook notation that defines the idf as idf(d, t) = log [ n / (df(d, t) + 1) ]).</source>
          <target state="translated">項tのtf-idfを計算するために使用される式はtf-idf（d、t）= tf（t）* idf（d、t）であり、idfはidf（d、t）= logとして計算されます[n / df（d、t）] + 1（ &lt;code&gt;smooth_idf=False&lt;/code&gt; の場合）、ここでnはドキュメントの総数、df（d、t）はドキュメントの頻度です。ドキュメントの頻度は、用語tを含むドキュメントdの数です。上記の式でidfに「1」を追加すると、idfがゼロの項、つまりトレーニングセット内のすべてのドキュメントで発生する項が完全に無視されるわけではありません。 （上記のidfの式は、idf（d、t）= log [n /（df（d、t）+ 1）]としてidfを定義する標準的なテキスト表記とは異なることに注意してください。</target>
        </trans-unit>
        <trans-unit id="2757038bccf5826fff274cfe8684f779a3893302" translate="yes" xml:space="preserve">
          <source>The formula used here does not correspond to the one given in the article. In the original article, formula (23) states that 2/p is multiplied by Trace(cov*cov) in both the numerator and denominator, but this operation is omitted because for a large p, the value of 2/p is so small that it doesn&amp;rsquo;t affect the value of the estimator.</source>
          <target state="translated">ここで使用されている式は、記事に記載されている式に対応していません。元の記事では、式（23）に、分子と分母の両方で2 / pにTrace（cov * cov）が乗算されると記載されていますが、この操作は省略されています。推定器の値に影響を与えないこと。</target>
        </trans-unit>
        <trans-unit id="25426ebad0f81610af376f208a1d7f7cdb7c96c5" translate="yes" xml:space="preserve">
          <source>The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. &lt;code&gt;subsample&lt;/code&gt; interacts with the parameter &lt;code&gt;n_estimators&lt;/code&gt;. Choosing &lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt; leads to a reduction of variance and an increase in bias.</source>
          <target state="translated">個々の基本学習者をフィッティングするために使用されるサンプルの割合。1.0より小さい場合、これは確率的勾配ブーストになります。 &lt;code&gt;subsample&lt;/code&gt; はパラメーター &lt;code&gt;n_estimators&lt;/code&gt; と相互作用します。 &lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt; 選択すると、分散が減少し、バイアスが増加します。</target>
        </trans-unit>
        <trans-unit id="568fbbbfe83b2ef390104a99310641880cb62ce5" translate="yes" xml:space="preserve">
          <source>The fraction of samples to be used in each randomized design. Should be between 0 and 1. If 1, all samples are used.</source>
          <target state="translated">各無作為化デザインで使用するサンプルの割合。0から1の間である必要があります。 1の場合、すべてのサンプルが使用されます。</target>
        </trans-unit>
        <trans-unit id="52b16a6d5efc38a8b1d594773d860718286a7706" translate="yes" xml:space="preserve">
          <source>The fraction of samples whose class are randomly exchanged. Larger values introduce noise in the labels and make the classification task harder.</source>
          <target state="translated">クラスがランダムに交換されるサンプルの割合。値が大きくなるとラベルにノイズが入り、分類作業が難しくなります。</target>
        </trans-unit>
        <trans-unit id="2b8586795acd1de1d253165ce5ff74dbee8020e0" translate="yes" xml:space="preserve">
          <source>The free parameters in the model are C and epsilon.</source>
          <target state="translated">モデルの自由パラメータはCとεです。</target>
        </trans-unit>
        <trans-unit id="852901dbf93c375963721fc84a4888e6b45c78dd" translate="yes" xml:space="preserve">
          <source>The full description of the dataset</source>
          <target state="translated">データセットの完全な説明</target>
        </trans-unit>
        <trans-unit id="19877feb57bb09f6d25fbaaa466f42d45ca4d944" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.feature_extraction.image.img_to_graph#sklearn.feature_extraction.image.img_to_graph&quot;&gt;&lt;code&gt;img_to_graph&lt;/code&gt;&lt;/a&gt; returns such a matrix from a 2D or 3D image. Similarly, &lt;a href=&quot;generated/sklearn.feature_extraction.image.grid_to_graph#sklearn.feature_extraction.image.grid_to_graph&quot;&gt;&lt;code&gt;grid_to_graph&lt;/code&gt;&lt;/a&gt; build a connectivity matrix for images given the shape of these image.</source>
          <target state="translated">関数&lt;a href=&quot;generated/sklearn.feature_extraction.image.img_to_graph#sklearn.feature_extraction.image.img_to_graph&quot;&gt; &lt;code&gt;img_to_graph&lt;/code&gt; &lt;/a&gt;は、2Dまたは3D画像からそのような行列を返します。同様に、&lt;a href=&quot;generated/sklearn.feature_extraction.image.grid_to_graph#sklearn.feature_extraction.image.grid_to_graph&quot;&gt; &lt;code&gt;grid_to_graph&lt;/code&gt; &lt;/a&gt;は、画像の形状が指定された画像の接続性マトリックスを作成します。</target>
        </trans-unit>
        <trans-unit id="6bbdd98a7d5d87ddd3bbdfb8569481a5e5e04495" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.cohen_kappa_score#sklearn.metrics.cohen_kappa_score&quot;&gt;&lt;code&gt;cohen_kappa_score&lt;/code&gt;&lt;/a&gt; computes &lt;a href=&quot;https://en.wikipedia.org/wiki/Cohen%27s_kappa&quot;&gt;Cohen&amp;rsquo;s kappa&lt;/a&gt; statistic. This measure is intended to compare labelings by different human annotators, not a classifier versus a ground truth.</source>
          <target state="translated">関数&lt;a href=&quot;generated/sklearn.metrics.cohen_kappa_score#sklearn.metrics.cohen_kappa_score&quot;&gt; &lt;code&gt;cohen_kappa_score&lt;/code&gt; は、&lt;/a&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Cohen%27s_kappa&quot;&gt;コーエンのカッパ&lt;/a&gt;統計量を計算します。この測定は、分類子とグラウンドトゥルースではなく、異なる人間のアノテーターによるラベリングを比較することを目的としています。</target>
        </trans-unit>
        <trans-unit id="93f21bf6b976a0d8004246b9140ac3f9b0310fba" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.laplacian_kernel#sklearn.metrics.pairwise.laplacian_kernel&quot;&gt;&lt;code&gt;laplacian_kernel&lt;/code&gt;&lt;/a&gt; is a variant on the radial basis function kernel defined as:</source>
          <target state="translated">関数&lt;a href=&quot;generated/sklearn.metrics.pairwise.laplacian_kernel#sklearn.metrics.pairwise.laplacian_kernel&quot;&gt; &lt;code&gt;laplacian_kernel&lt;/code&gt; &lt;/a&gt;は、次のように定義された動径基底関数カーネルのバリアントです。</target>
        </trans-unit>
        <trans-unit id="3fe1ad7d2ae4a1d31d30a7b5c63e81d53b7568bb" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.linear_kernel#sklearn.metrics.pairwise.linear_kernel&quot;&gt;&lt;code&gt;linear_kernel&lt;/code&gt;&lt;/a&gt; computes the linear kernel, that is, a special case of &lt;a href=&quot;generated/sklearn.metrics.pairwise.polynomial_kernel#sklearn.metrics.pairwise.polynomial_kernel&quot;&gt;&lt;code&gt;polynomial_kernel&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;degree=1&lt;/code&gt; and &lt;code&gt;coef0=0&lt;/code&gt; (homogeneous). If &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are column vectors, their linear kernel is:</source>
          <target state="translated">関数&lt;a href=&quot;generated/sklearn.metrics.pairwise.linear_kernel#sklearn.metrics.pairwise.linear_kernel&quot;&gt; &lt;code&gt;linear_kernel&lt;/code&gt; &lt;/a&gt;は線形カーネルを計算します。つまり、 &lt;code&gt;degree=1&lt;/code&gt; 、 &lt;code&gt;coef0=0&lt;/code&gt; （同種）の&lt;a href=&quot;generated/sklearn.metrics.pairwise.polynomial_kernel#sklearn.metrics.pairwise.polynomial_kernel&quot;&gt; &lt;code&gt;polynomial_kernel&lt;/code&gt; の&lt;/a&gt;特殊なケースです。場合 &lt;code&gt;x&lt;/code&gt; と &lt;code&gt;y&lt;/code&gt; は列ベクトルで、その線形カーネルは、次のとおりです。</target>
        </trans-unit>
        <trans-unit id="72295044331d14363e1e1fe37cea504256d7c67f" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.polynomial_kernel#sklearn.metrics.pairwise.polynomial_kernel&quot;&gt;&lt;code&gt;polynomial_kernel&lt;/code&gt;&lt;/a&gt; computes the degree-d polynomial kernel between two vectors. The polynomial kernel represents the similarity between two vectors. Conceptually, the polynomial kernels considers not only the similarity between vectors under the same dimension, but also across dimensions. When used in machine learning algorithms, this allows to account for feature interaction.</source>
          <target state="translated">関数&lt;a href=&quot;generated/sklearn.metrics.pairwise.polynomial_kernel#sklearn.metrics.pairwise.polynomial_kernel&quot;&gt; &lt;code&gt;polynomial_kernel&lt;/code&gt; &lt;/a&gt;は、2つのベクトル間の次数dの多項式カーネルを計算します。多項式カーネルは、2つのベクトル間の類似性を表します。概念的には、多項式カーネルは、同じ次元のベクトル間の類似性だけでなく、次元全体の類似性も考慮します。機械学習アルゴリズムで使用すると、機能の相互作用を考慮することができます。</target>
        </trans-unit>
        <trans-unit id="2f5b051b13aa3478b0b4a17587fa0bec1e6b87b4" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.rbf_kernel#sklearn.metrics.pairwise.rbf_kernel&quot;&gt;&lt;code&gt;rbf_kernel&lt;/code&gt;&lt;/a&gt; computes the radial basis function (RBF) kernel between two vectors. This kernel is defined as:</source>
          <target state="translated">関数&lt;a href=&quot;generated/sklearn.metrics.pairwise.rbf_kernel#sklearn.metrics.pairwise.rbf_kernel&quot;&gt; &lt;code&gt;rbf_kernel&lt;/code&gt; &lt;/a&gt;は、2つのベクトル間の動径基底関数（RBF）カーネルを計算します。このカーネルは次のように定義されています。</target>
        </trans-unit>
        <trans-unit id="a4ac87fe4bd82908551f017a1e984b845dfe00ca" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.sigmoid_kernel#sklearn.metrics.pairwise.sigmoid_kernel&quot;&gt;&lt;code&gt;sigmoid_kernel&lt;/code&gt;&lt;/a&gt; computes the sigmoid kernel between two vectors. The sigmoid kernel is also known as hyperbolic tangent, or Multilayer Perceptron (because, in the neural network field, it is often used as neuron activation function). It is defined as:</source>
          <target state="translated">関数&lt;a href=&quot;generated/sklearn.metrics.pairwise.sigmoid_kernel#sklearn.metrics.pairwise.sigmoid_kernel&quot;&gt; &lt;code&gt;sigmoid_kernel&lt;/code&gt; &lt;/a&gt;は、2つのベクトル間のシグモイドカーネルを計算します。シグモイドカーネルは、双曲線正接、または多層パーセプトロンとも呼ばれます（ニューラルネットワークの分野では、ニューロンの活性化関数としてよく使用されるため）。次のように定義されます。</target>
        </trans-unit>
        <trans-unit id="2e327cd188c9064345dfa4a6a6764985ae429bc7" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.roc_curve#sklearn.metrics.roc_curve&quot;&gt;&lt;code&gt;roc_curve&lt;/code&gt;&lt;/a&gt; computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&quot;&gt;receiver operating characteristic curve, or ROC curve&lt;/a&gt;. Quoting Wikipedia :</source>
          <target state="translated">関数&lt;a href=&quot;generated/sklearn.metrics.roc_curve#sklearn.metrics.roc_curve&quot;&gt; &lt;code&gt;roc_curve&lt;/code&gt; &lt;/a&gt;は、&lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&quot;&gt;受信者動作特性曲線、またはROC曲線を&lt;/a&gt;計算します。ウィキペディアの引用：</target>
        </trans-unit>
        <trans-unit id="a52cb65c85990e02ab77ee98f8132a1c3b7a7b6c" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt;&lt;code&gt;cross_val_predict&lt;/code&gt;&lt;/a&gt; has a similar interface to &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt;, but returns, for each element in the input, the prediction that was obtained for that element when it was in the test set. Only cross-validation strategies that assign all elements to a test set exactly once can be used (otherwise, an exception is raised).</source>
          <target state="translated">関数&lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt; &lt;code&gt;cross_val_predict&lt;/code&gt; &lt;/a&gt;はcross_val_scoreと同様のインターフェースを&lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt;いますが、入力内の各要素について、テストセット内にあるときにその要素に対して取得された予測を返します。すべての要素を1回だけテストセットに割り当てる相互検証戦略のみを使用できます（それ以外の場合は、例外が発生します）。</target>
        </trans-unit>
        <trans-unit id="9da333e152484b3ac872458a2a577c489d5042de" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.model_selection.validation_curve#sklearn.model_selection.validation_curve&quot;&gt;&lt;code&gt;validation_curve&lt;/code&gt;&lt;/a&gt; can help in this case:</source>
          <target state="translated">この場合、関数&lt;a href=&quot;generated/sklearn.model_selection.validation_curve#sklearn.model_selection.validation_curve&quot;&gt; &lt;code&gt;validation_curve&lt;/code&gt; &lt;/a&gt;が役立ちます。</target>
        </trans-unit>
        <trans-unit id="ef1bfedb8d96897b52fb746234adf67b6ec8f0b2" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.preprocessing.normalize#sklearn.preprocessing.normalize&quot;&gt;&lt;code&gt;normalize&lt;/code&gt;&lt;/a&gt; provides a quick and easy way to perform this operation on a single array-like dataset, either using the &lt;code&gt;l1&lt;/code&gt; or &lt;code&gt;l2&lt;/code&gt; norms:</source>
          <target state="translated">関数&lt;a href=&quot;generated/sklearn.preprocessing.normalize#sklearn.preprocessing.normalize&quot;&gt; &lt;code&gt;normalize&lt;/code&gt; &lt;/a&gt;は、 &lt;code&gt;l1&lt;/code&gt; または &lt;code&gt;l2&lt;/code&gt; ノルムを使用して、単一の配列のようなデータセットに対してこの操作を実行する迅速かつ簡単な方法を提供します。</target>
        </trans-unit>
        <trans-unit id="621182b188f45e3dd8884ee1d03015ee530041c8" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.preprocessing.scale#sklearn.preprocessing.scale&quot;&gt;&lt;code&gt;scale&lt;/code&gt;&lt;/a&gt; provides a quick and easy way to perform this operation on a single array-like dataset:</source>
          <target state="translated">関数&lt;a href=&quot;generated/sklearn.preprocessing.scale#sklearn.preprocessing.scale&quot;&gt; &lt;code&gt;scale&lt;/code&gt; &lt;/a&gt;は、単一の配列のようなデータセットに対してこの操作を実行するための迅速かつ簡単な方法を提供します。</target>
        </trans-unit>
        <trans-unit id="14656519a034ad24c06a19460128dde85e00e72d" translate="yes" xml:space="preserve">
          <source>The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances as described in &lt;a href=&quot;#r37d39d7589e2-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt; and &lt;a href=&quot;#r37d39d7589e2-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt;. Both methods are based on the idea originally proposed in &lt;a href=&quot;#r37d39d7589e2-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt;.</source>
          <target state="translated">この関数は、&lt;a href=&quot;#r37d39d7589e2-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt;および&lt;a href=&quot;#r37d39d7589e2-3&quot; id=&quot;id3&quot;&gt;[3]で&lt;/a&gt;説明されているように、k最近傍距離からのエントロピー推定に基づくノンパラメトリック手法に依存しています。どちらの方法も、元々&lt;a href=&quot;#r37d39d7589e2-4&quot; id=&quot;id4&quot;&gt;[4]で&lt;/a&gt;提案されたアイデアに基づいています。</target>
        </trans-unit>
        <trans-unit id="cf1d60e35345e80734f9e0bef9342c1dede910a1" translate="yes" xml:space="preserve">
          <source>The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances as described in &lt;a href=&quot;#r50b872b699c4-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt; and &lt;a href=&quot;#r50b872b699c4-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt;. Both methods are based on the idea originally proposed in &lt;a href=&quot;#r50b872b699c4-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt;.</source>
          <target state="translated">この関数は、&lt;a href=&quot;#r50b872b699c4-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt;および&lt;a href=&quot;#r50b872b699c4-3&quot; id=&quot;id3&quot;&gt;[3]で&lt;/a&gt;説明されているように、k最近傍距離からのエントロピー推定に基づくノンパラメトリック手法に依存しています。どちらの方法も、元々&lt;a href=&quot;#r50b872b699c4-4&quot; id=&quot;id4&quot;&gt;[4]で&lt;/a&gt;提案されたアイデアに基づいています。</target>
        </trans-unit>
        <trans-unit id="fb8314efd5d7c36c4d228413206cd5331b82e456" translate="yes" xml:space="preserve">
          <source>The function requires either the argument &lt;code&gt;grid&lt;/code&gt; which specifies the values of the target features on which the partial dependence function should be evaluated or the argument &lt;code&gt;X&lt;/code&gt; which is a convenience mode for automatically creating &lt;code&gt;grid&lt;/code&gt; from the training data. If &lt;code&gt;X&lt;/code&gt; is given, the &lt;code&gt;axes&lt;/code&gt; value returned by the function gives the axis for each target feature.</source>
          <target state="translated">関数には、部分依存関数を評価する必要のあるターゲットフィーチャの値を指定する引数 &lt;code&gt;grid&lt;/code&gt; 、またはトレーニングデータから &lt;code&gt;grid&lt;/code&gt; を自動的に作成するための便利なモードである引数 &lt;code&gt;X&lt;/code&gt; が必要です。場合 &lt;code&gt;X&lt;/code&gt; が与えられ、 &lt;code&gt;axes&lt;/code&gt; 関数によって返される値は、各対象地物のための軸を与えます。</target>
        </trans-unit>
        <trans-unit id="d2fedea237bdc334f6322c4692b523df3b5f6fbf" translate="yes" xml:space="preserve">
          <source>The function to be decorated</source>
          <target state="translated">飾る機能</target>
        </trans-unit>
        <trans-unit id="f7fc9606332ff37416d86060e52aa56595a5e3ce" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;friedman_mse&amp;rdquo; for the mean squared error with improvement score by Friedman, &amp;ldquo;mse&amp;rdquo; for mean squared error, and &amp;ldquo;mae&amp;rdquo; for the mean absolute error. The default value of &amp;ldquo;friedman_mse&amp;rdquo; is generally the best as it can provide a better approximation in some cases.</source>
          <target state="translated">分割の品質を測定する関数。サポートされている基準は、Friedmanによる改善スコアのある平均二乗誤差の場合は「friedman_mse」、平均二乗誤差の場合は「mse」、平均絶対誤差の場合は「mae」です。「friedman_mse」のデフォルト値は、場合によってはより良い近似を提供できるため、一般的には最適です。</target>
        </trans-unit>
        <trans-unit id="8c0bede693f27f1257cedfa964bd89180e6bfada" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;gini&amp;rdquo; for the Gini impurity and &amp;ldquo;entropy&amp;rdquo; for the information gain.</source>
          <target state="translated">分割の品質を測定する関数。サポートされている基準は、ジニ不純物の「ジニ」と情報ゲインの「エントロピー」です。</target>
        </trans-unit>
        <trans-unit id="03e746b06910ee8c07bc239c0b780e5294140dfb" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;gini&amp;rdquo; for the Gini impurity and &amp;ldquo;entropy&amp;rdquo; for the information gain. Note: this parameter is tree-specific.</source>
          <target state="translated">分割の品質を測定する関数。サポートされている基準は、ジニ不純物の「ジニ」と情報ゲインの「エントロピー」です。注：このパラメーターはツリー固有です。</target>
        </trans-unit>
        <trans-unit id="c211c5fb9289c19b7ab7fb609c5f42d0fd7fb417" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;mse&amp;rdquo; for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node, &amp;ldquo;friedman_mse&amp;rdquo;, which uses mean squared error with Friedman&amp;rsquo;s improvement score for potential splits, and &amp;ldquo;mae&amp;rdquo; for the mean absolute error, which minimizes the L1 loss using the median of each terminal node.</source>
          <target state="translated">分割の品質を測定する関数。サポートされている基準は、平均二乗誤差の「mse」です。これは、特徴選択基準としての分散減少に等しく、各末端ノードの平均「friedman_mse」を使用してL2損失を最小化します。これは、平均二乗誤差とフリードマンの改善スコアの可能性を使用します。分割、および平均絶対誤差の「前」。これは、各端末ノードの中央値を使用してL1損失を最小化します。</target>
        </trans-unit>
        <trans-unit id="bf422f8c78875448c6e34d2c008baf4a5621c47d" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;mse&amp;rdquo; for the mean squared error, which is equal to variance reduction as feature selection criterion, and &amp;ldquo;mae&amp;rdquo; for the mean absolute error.</source>
          <target state="translated">分割の品質を測定する関数。サポートされる基準は、特徴の選択基準としての分散減少に等しい平均二乗誤差の「mse」と平均絶対誤差の「mae」です。</target>
        </trans-unit>
        <trans-unit id="8b3c9548b1b42f5af71c7bfe5f885c712284a1e6" translate="yes" xml:space="preserve">
          <source>The function which is applied on each chunk of the distance matrix, reducing it to needed values. &lt;code&gt;reduce_func(D_chunk, start)&lt;/code&gt; is called repeatedly, where &lt;code&gt;D_chunk&lt;/code&gt; is a contiguous vertical slice of the pairwise distance matrix, starting at row &lt;code&gt;start&lt;/code&gt;. It should return an array, a list, or a sparse matrix of length &lt;code&gt;D_chunk.shape[0]&lt;/code&gt;, or a tuple of such objects.</source>
          <target state="translated">距離行列の各チャンクに適用され、必要な値に削減される関数。 &lt;code&gt;reduce_func(D_chunk, start)&lt;/code&gt; は繰り返し呼び出されます。ここで、 &lt;code&gt;D_chunk&lt;/code&gt; はペアの距離行列の隣接する垂直スライスで、行 &lt;code&gt;start&lt;/code&gt; から始まります。これは、長さ &lt;code&gt;D_chunk.shape[0]&lt;/code&gt; 配列、リスト、スパース行列、またはそのようなオブジェクトのタプルを返す必要があります。</target>
        </trans-unit>
        <trans-unit id="34f983a96e6e1ba7a36ae52fb70df739e3a5aca9" translate="yes" xml:space="preserve">
          <source>The functional form of the G function used in the approximation to neg-entropy. Could be either &amp;lsquo;logcosh&amp;rsquo;, &amp;lsquo;exp&amp;rsquo;, or &amp;lsquo;cube&amp;rsquo;. You can also provide your own function. It should return a tuple containing the value of the function, and of its derivative, in the point. Example:</source>
          <target state="translated">否定エントロピーの近似で使用されるG関数の関数形。「logcosh」、「exp」、または「cube」のいずれかです。独自の機能を提供することもできます。関数とその派生物の値をポイントに含むタプルを返す必要があります。例：</target>
        </trans-unit>
        <trans-unit id="f3580ae3ff67a44925f6075f1fc46034cd1f3d14" translate="yes" xml:space="preserve">
          <source>The generated array.</source>
          <target state="translated">生成された配列。</target>
        </trans-unit>
        <trans-unit id="22dd341f8c92381f47326cd6fbe4fff46dd59a6b" translate="yes" xml:space="preserve">
          <source>The generated matrix.</source>
          <target state="translated">生成された行列。</target>
        </trans-unit>
        <trans-unit id="b670a7959baa25f4a9c290d5df951114cede8bad" translate="yes" xml:space="preserve">
          <source>The generated samples.</source>
          <target state="translated">生成されたサンプル。</target>
        </trans-unit>
        <trans-unit id="61a0705074aa05bf429a2ff6dd9f09842d1c86f2" translate="yes" xml:space="preserve">
          <source>The generator used to initialize the centers. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">センターを初期化するために使用されるジェネレータ。intの場合、random_stateは乱数ジェネレータによって使用されるシードです。RandomStateインスタンスの場合、random_stateは乱数ジェネレーターです。Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。</target>
        </trans-unit>
        <trans-unit id="473c7ed23812d4bf8fcf6c97ab0551b7301ccf95" translate="yes" xml:space="preserve">
          <source>The generator used to initialize the codebook. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">コードブックの初期化に使用されるジェネレーター。intの場合、random_stateは乱数ジェネレータによって使用されるシードです。RandomStateインスタンスの場合、random_stateは乱数ジェネレーターです。Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。</target>
        </trans-unit>
        <trans-unit id="4e831547e83750929880db075c40fc4aa61eb4b8" translate="yes" xml:space="preserve">
          <source>The generator used to randomize the design. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">設計をランダム化するために使用されるジェネレーター。intの場合、random_stateは乱数ジェネレータによって使用されるシードです。RandomStateインスタンスの場合、random_stateは乱数ジェネレーターです。Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。</target>
        </trans-unit>
        <trans-unit id="cd84f22d9e60e3f86bb2455d78aaf109647d76b1" translate="yes" xml:space="preserve">
          <source>The generator used to randomly select a subset of samples. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;sample_size is not None&lt;/code&gt;.</source>
          <target state="translated">サンプルのサブセットをランダムに選択するために使用されるジェネレーター。intの場合、random_stateは乱数ジェネレータによって使用されるシードです。RandomStateインスタンスの場合、random_stateは乱数ジェネレーターです。Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。 &lt;code&gt;sample_size is not None&lt;/code&gt; 場合に使用されます。</target>
        </trans-unit>
        <trans-unit id="36aa9d7d033388d006150bbc613bf9f61afb2f31" translate="yes" xml:space="preserve">
          <source>The generator used to randomly select the samples from input points for bandwidth estimation. Use an int to make the randomness deterministic. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="translated">帯域幅推定のために入力ポイントからサンプルをランダムに選択するために使用されるジェネレーター。intを使用して、ランダム性を決定論的にします。&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-random-state&quot;&gt;用語集を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="6c9c479a6511fc655c70a7c51ab0a85c04034b21" translate="yes" xml:space="preserve">
          <source>The goal is to measure the latency one can expect when doing predictions either in bulk or atomic (i.e. one by one) mode.</source>
          <target state="translated">目標は、バルクモードまたはアトミック(1つずつ)モードで予測を行う際に期待できる待ち時間を測定することです。</target>
        </trans-unit>
        <trans-unit id="95b3715b7309073ce23a5e1ae6bd1c83fb5ab128" translate="yes" xml:space="preserve">
          <source>The goal of &lt;strong&gt;ensemble methods&lt;/strong&gt; is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.</source>
          <target state="translated">&lt;strong&gt;アンサンブル法&lt;/strong&gt;の目的は、特定の学習アルゴリズムで構築されたいくつかの基本推定量の予測を組み合わせて、単一の推定量に対する一般化可能性/ロバスト性を改善することです。</target>
        </trans-unit>
        <trans-unit id="4be143708c968fbd1f8e4e8288622de6f1251f81" translate="yes" xml:space="preserve">
          <source>The goal of this example is to analyze the graph of links inside wikipedia articles to rank articles by relative importance according to this eigenvector centrality.</source>
          <target state="translated">この例の目的は、wikipediaの記事内のリンクのグラフを分析して、この固有ベクトルの中心性に応じて相対的な重要度で記事をランク付けすることです。</target>
        </trans-unit>
        <trans-unit id="5095c3c5239296131fc48d25860f3771ab9b91e3" translate="yes" xml:space="preserve">
          <source>The goal of this example is to show intuitively how the metrics behave, and not to find good clusters for the digits. This is why the example works on a 2D embedding.</source>
          <target state="translated">この例の目的は、メトリクスがどのように振る舞うかを直感的に示すことであり、桁の良いクラスタを見つけることではありません。このため、この例は 2D エンベッディングで動作します。</target>
        </trans-unit>
        <trans-unit id="20bd11aa678c9e04777a40ec0f194ab6b7581aa6" translate="yes" xml:space="preserve">
          <source>The goal of this guide is to explore some of the main &lt;code&gt;scikit-learn&lt;/code&gt; tools on a single practical task: analyzing a collection of text documents (newsgroups posts) on twenty different topics.</source>
          <target state="translated">このガイドの目的は、20の異なるトピックに関するテキストドキュメントのコレクション（ニュースグループの投稿）を分析するという1つの実用的なタスクに関する主要な &lt;code&gt;scikit-learn&lt;/code&gt; ツールのいくつかを探索することです。</target>
        </trans-unit>
        <trans-unit id="c224302bdf144aacfdfc260a6638fe1107c2e8d4" translate="yes" xml:space="preserve">
          <source>The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.</source>
          <target state="translated">与えられた文書内のトークンの生の出現頻度の代わりに tf-idf を使用する目的は、与えられたコーパス内で非常に頻繁に出現し、それゆえに経験的には学習コーパスのごく一部に出現する特徴よりも情報量が少ないトークンの影響をスケールダウンさせることです。</target>
        </trans-unit>
        <trans-unit id="3325084d0d02139eb921a2545ba48cff39657720" translate="yes" xml:space="preserve">
          <source>The gradient of the kernel k(X, X) with respect to the hyperparameter of the kernel. Only returned when eval_gradient is True.</source>
          <target state="translated">カーネルのハイパーパラメータに対するカーネル k(X,X)の勾配.eval_gradientがTrueの場合のみ返される。</target>
        </trans-unit>
        <trans-unit id="1d28aeaa2593827c70ee0cbf35f8d5891fd13e08" translate="yes" xml:space="preserve">
          <source>The graph data is fetched from the DBpedia dumps. DBpedia is an extraction of the latent structured data of the Wikipedia content.</source>
          <target state="translated">グラフデータはDBpediaのダンプから取得します。DBpediaはWikipediaのコンテンツの潜在構造化データを抽出します。</target>
        </trans-unit>
        <trans-unit id="94a8641249b9d38c741294dd700ba9c013e60d15" translate="yes" xml:space="preserve">
          <source>The graph should contain only one connect component, elsewhere the results make little sense.</source>
          <target state="translated">グラフは1つの接続コンポーネントだけを含むべきであり、それ以外の場所では結果はほとんど意味をなさない。</target>
        </trans-unit>
        <trans-unit id="0aca214d3abec7143ce0e3db3703c7759829a517" translate="yes" xml:space="preserve">
          <source>The graphical model of LDA is a three-level Bayesian model:</source>
          <target state="translated">LDAのグラフィカルモデルは3階層ベイズモデルです。</target>
        </trans-unit>
        <trans-unit id="b70b496cfa90301063390d7d501e40e03165e505" translate="yes" xml:space="preserve">
          <source>The graphical model of an RBM is a fully-connected bipartite graph.</source>
          <target state="translated">RBMのグラフィカルモデルは、完全に接続された二部グラフです。</target>
        </trans-unit>
        <trans-unit id="43e8165a75805bcff11ce07ae2103365830550d8" translate="yes" xml:space="preserve">
          <source>The grid of &lt;code&gt;target_variables&lt;/code&gt; values for which the partial dependecy should be evaluated (either &lt;code&gt;grid&lt;/code&gt; or &lt;code&gt;X&lt;/code&gt; must be specified).</source>
          <target state="translated">部分的な依存関係を評価する必要がある &lt;code&gt;target_variables&lt;/code&gt; 値のグリッド（ &lt;code&gt;grid&lt;/code&gt; または &lt;code&gt;X&lt;/code&gt; を指定する必要があります）。</target>
        </trans-unit>
        <trans-unit id="1fb55e8edd66947697c84e91d6ac3fa247bd29e9" translate="yes" xml:space="preserve">
          <source>The grid of alphas used for fitting</source>
          <target state="translated">フィッティングに使用されるアルファのグリッド</target>
        </trans-unit>
        <trans-unit id="c8ce42c94fb7c4644fc397d481823f721280ec22" translate="yes" xml:space="preserve">
          <source>The grid of alphas used for fitting, for each l1_ratio</source>
          <target state="translated">各 l1_ratio に対して、フィッティングに使用されるアルファのグリッド</target>
        </trans-unit>
        <trans-unit id="92edf0193fba09badba27bf7739525bce6da7601" translate="yes" xml:space="preserve">
          <source>The grid of alphas used for fitting, for each l1_ratio.</source>
          <target state="translated">各l1_ratioのフィッティングに使用されるアルファのグリッド。</target>
        </trans-unit>
        <trans-unit id="22daba72217df04ec1af1a8a340277c45da4b54e" translate="yes" xml:space="preserve">
          <source>The grid of alphas used for fitting.</source>
          <target state="translated">フィッティングに使用されるアルファのグリッド。</target>
        </trans-unit>
        <trans-unit id="37c7f40c413d2aeb11b8645a838ea3c671b821b6" translate="yes" xml:space="preserve">
          <source>The grid points between 0 and 1: alpha/alpha_max</source>
          <target state="translated">0 と 1 の間の格子点:alpha/alpha_max</target>
        </trans-unit>
        <trans-unit id="1e90c7186240eacb6693334f5a2cb603522a3c95" translate="yes" xml:space="preserve">
          <source>The grid search instance behaves like a normal &lt;code&gt;scikit-learn&lt;/code&gt; model. Let&amp;rsquo;s perform the search on a smaller subset of the training data to speed up the computation:</source>
          <target state="translated">グリッド検索インスタンスは、通常の &lt;code&gt;scikit-learn&lt;/code&gt; モデルのように動作します。計算を高速化するために、トレーニングデータのより小さなサブセットで検索を実行してみましょう。</target>
        </trans-unit>
        <trans-unit id="0fb72e7d32db09c24209afb30ebd0c3ef9469a12" translate="yes" xml:space="preserve">
          <source>The grid search provided by &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt; exhaustively generates candidates from a grid of parameter values specified with the &lt;code&gt;param_grid&lt;/code&gt; parameter. For instance, the following &lt;code&gt;param_grid&lt;/code&gt;:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;GridSearchCV&lt;/code&gt; &lt;/a&gt;によって提供されるグリッド検索は、 &lt;code&gt;param_grid&lt;/code&gt; パラメーターで指定されたパラメーター値のグリッドから候補を徹底的に生成します。たとえば、次の &lt;code&gt;param_grid&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="5d877967ec11cd12214237a4561547e48123553d" translate="yes" xml:space="preserve">
          <source>The guidelines for choosing a metric is to use one that maximizes the distance between samples in different classes, and minimizes that within each class.</source>
          <target state="translated">メトリックを選択するためのガイドラインは、異なるクラスのサンプル間の距離を最大化し、各クラス内での距離を最小化するものを使用することです。</target>
        </trans-unit>
        <trans-unit id="42b20fdbf19bf6fb28b9336fb1c931e7ebd9ff54" translate="yes" xml:space="preserve">
          <source>The handwritten digit dataset has 1797 total points. The model will be trained using all points, but only 30 will be labeled. Results in the form of a confusion matrix and a series of metrics over each class will be very good.</source>
          <target state="translated">手書きの数字データセットには1797点の合計点があります。モデルはすべてのポイントを使用して訓練されますが、ラベル付けされるのは30点のみです。混乱行列と各クラスの一連のメトリクスの形での結果は非常に良いものになるでしょう。</target>
        </trans-unit>
        <trans-unit id="1dbebc05eff701a160c5a2149017afcbb50998d6" translate="yes" xml:space="preserve">
          <source>The hash function employed is the signed 32-bit version of Murmurhash3.</source>
          <target state="translated">採用されているハッシュ関数は、Murmurhash3の符号付き32ビット版です。</target>
        </trans-unit>
        <trans-unit id="6ddf3e17ce9800f43a67078b41067ddc0d8c6a4c" translate="yes" xml:space="preserve">
          <source>The higher concentration puts more mass in the center and will lead to more components being active, while a lower concentration parameter will lead to more mass at the edge of the simplex.</source>
          <target state="translated">濃度が高いほど中央に多くの質量が置かれ、より多くの成分が活性化され、濃度が低いパラメータは単純体の端に多くの質量が置かれることになります。</target>
        </trans-unit>
        <trans-unit id="1006ebebebbef4245b7568132e1eabf566a79140" translate="yes" xml:space="preserve">
          <source>The highest p-value for features to be kept.</source>
          <target state="translated">残すべき特徴の最も高いp値。</target>
        </trans-unit>
        <trans-unit id="0c1d204d0071e8e2a8101579dd644bade4aef0d9" translate="yes" xml:space="preserve">
          <source>The highest uncorrected p-value for features to keep.</source>
          <target state="translated">残すべき特徴のための最も高い無補正のp値。</target>
        </trans-unit>
        <trans-unit id="3708900f6a079bc16cd76c37da5647812d1a0427" translate="yes" xml:space="preserve">
          <source>The histogram of the estimated weights is very peaked, as a sparsity-inducing prior is implied on the weights.</source>
          <target state="translated">推定された重みのヒストグラムは,重みにスパースシティを誘発する事前分布が暗示されているため,非常にピークを迎えています.</target>
        </trans-unit>
        <trans-unit id="d24bdc7c6b1e4dbeb474c6e9f76042366464386e" translate="yes" xml:space="preserve">
          <source>The hyperparameters</source>
          <target state="translated">ハイパーパラメタ</target>
        </trans-unit>
        <trans-unit id="25f29cead3cfba36338bcf026da5a846edf25636" translate="yes" xml:space="preserve">
          <source>The i-th score &lt;code&gt;train_score_[i]&lt;/code&gt; is the deviance (= loss) of the model at iteration &lt;code&gt;i&lt;/code&gt; on the in-bag sample. If &lt;code&gt;subsample == 1&lt;/code&gt; this is the deviance on the training data.</source>
          <target state="translated">i番目のスコア &lt;code&gt;train_score_[i]&lt;/code&gt; は、バッグ内サンプルの反復 &lt;code&gt;i&lt;/code&gt; におけるモデルの逸脱（=損失）です。 &lt;code&gt;subsample == 1&lt;/code&gt; 場合、これはトレーニングデータの逸脱です。</target>
        </trans-unit>
        <trans-unit id="7768180d6d5c29f9be6eb9d80f99cbf93007e25a" translate="yes" xml:space="preserve">
          <source>The i.i.d. assumption is broken if the underlying generative process yield groups of dependent samples.</source>
          <target state="translated">i.i.d.の仮定は、基礎となる生成過程で従属サンプルのグループが得られる場合には破られます。</target>
        </trans-unit>
        <trans-unit id="190c7529dfd22ae7ed70d4f84d1deb499abbe749" translate="yes" xml:space="preserve">
          <source>The idea behind the &lt;code&gt;VotingClassifier&lt;/code&gt; is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses.</source>
          <target state="translated">&lt;code&gt;VotingClassifier&lt;/code&gt; の背後にあるアイデアは、概念的に異なる機械学習分類子を組み合わせ、多数決または平均予測確率（ソフト投票）を使用してクラスラベルを予測することです。そのような分類子は、個々の弱点のバランスをとるために、同等に機能する一連のモデルに役立ちます。</target>
        </trans-unit>
        <trans-unit id="f4f9efa7d25a183372e2e9ce75bfe20581578033" translate="yes" xml:space="preserve">
          <source>The image as a numpy array: height x width x color</source>
          <target state="translated">画像を numpy 配列で表現したもの:高さ x 幅 x 色</target>
        </trans-unit>
        <trans-unit id="77bae2f833c03e6f85b6a43b3aa3ead8f81ce2e5" translate="yes" xml:space="preserve">
          <source>The image is quantized to 256 grey levels and stored as unsigned 8-bit integers; the loader will convert these to floating point values on the interval [0, 1], which are easier to work with for many algorithms.</source>
          <target state="translated">画像は256階調に量子化され、符号なし8ビット整数として格納されます。ローダはこれらを間隔[0,1]の浮動小数点値に変換します。</target>
        </trans-unit>
        <trans-unit id="6db946ce103c288b47eadb4f1488a8fdc91a7488" translate="yes" xml:space="preserve">
          <source>The implementation in the class &lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt;&lt;code&gt;Lasso&lt;/code&gt;&lt;/a&gt; uses coordinate descent as the algorithm to fit the coefficients. See &lt;a href=&quot;#least-angle-regression&quot;&gt;Least Angle Regression&lt;/a&gt; for another implementation:</source>
          <target state="translated">クラス&lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt; &lt;code&gt;Lasso&lt;/code&gt; &lt;/a&gt;の実装では、係数を近似するアルゴリズムとして座標降下を使用します。別の実装については、&lt;a href=&quot;#least-angle-regression&quot;&gt;最小角度回帰&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="64dabeaa3f8e38c30333615b1b174cbd1348b11e" translate="yes" xml:space="preserve">
          <source>The implementation in the class &lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnet#sklearn.linear_model.MultiTaskElasticNet&quot;&gt;&lt;code&gt;MultiTaskElasticNet&lt;/code&gt;&lt;/a&gt; uses coordinate descent as the algorithm to fit the coefficients.</source>
          <target state="translated">クラス&lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnet#sklearn.linear_model.MultiTaskElasticNet&quot;&gt; &lt;code&gt;MultiTaskElasticNet&lt;/code&gt; &lt;/a&gt;の実装では、係数を適合させるアルゴリズムとして座標降下を使用します。</target>
        </trans-unit>
        <trans-unit id="555e24352725a2133c35e7a52f56a702cd7f81c4" translate="yes" xml:space="preserve">
          <source>The implementation in the class &lt;a href=&quot;generated/sklearn.linear_model.multitasklasso#sklearn.linear_model.MultiTaskLasso&quot;&gt;&lt;code&gt;MultiTaskLasso&lt;/code&gt;&lt;/a&gt; uses coordinate descent as the algorithm to fit the coefficients.</source>
          <target state="translated">クラス&lt;a href=&quot;generated/sklearn.linear_model.multitasklasso#sklearn.linear_model.MultiTaskLasso&quot;&gt; &lt;code&gt;MultiTaskLasso&lt;/code&gt; &lt;/a&gt;の実装では、係数を適合させるアルゴリズムとして座標降下を使用します。</target>
        </trans-unit>
        <trans-unit id="f9cb17c43bed4736104925d68075ebd6b07ace68" translate="yes" xml:space="preserve">
          <source>The implementation is based on Algorithm 2.1 of &lt;a href=&quot;#rw2006&quot; id=&quot;id1&quot;&gt;[RW2006]&lt;/a&gt;. In addition to the API of standard scikit-learn estimators, GaussianProcessRegressor:</source>
          <target state="translated">この実装は、&lt;a href=&quot;#rw2006&quot; id=&quot;id1&quot;&gt;[RW2006]の&lt;/a&gt;アルゴリズム2.1に基づいています。標準のscikit-learn推定器のAPIに加えて、GaussianProcessRegressorは次のことを行います。</target>
        </trans-unit>
        <trans-unit id="70780cc3dac260d0fb4e85faed94f4134c9dc0ae" translate="yes" xml:space="preserve">
          <source>The implementation is based on Algorithm 2.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams.</source>
          <target state="translated">実装は、Rasmussen and WilliamsによるGaussian Processes for Machine Learning (GPML)のアルゴリズム2.1に基づいています。</target>
        </trans-unit>
        <trans-unit id="1904d3648b9818da40270920e1855bd9dcd752d1" translate="yes" xml:space="preserve">
          <source>The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams.</source>
          <target state="translated">実装は、Rasmussen and WilliamsによるGaussian Processes for Machine Learning (GPML)のアルゴリズム3.1,3.2,5.1に基づいています。</target>
        </trans-unit>
        <trans-unit id="166f55526ba60cbc129406f3899569f7b0eb4efd" translate="yes" xml:space="preserve">
          <source>The implementation is based on libsvm.</source>
          <target state="translated">実装はlibsvmをベースにしています。</target>
        </trans-unit>
        <trans-unit id="cb75b7ddb016087965f5fad7be27fc6c1cb16290" translate="yes" xml:space="preserve">
          <source>The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples.</source>
          <target state="translated">実装はlibsvmをベースにしています。フィット時間の複雑さはサンプル数の2次関数以上であり、数万サンプル以上のデータセットへのスケールアップは困難です。</target>
        </trans-unit>
        <trans-unit id="08d84e0cf0c38a38100a9dad48e0efc744fd2db8" translate="yes" xml:space="preserve">
          <source>The implementation of &lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt;&lt;code&gt;TheilSenRegressor&lt;/code&gt;&lt;/a&gt; in scikit-learn follows a generalization to a multivariate linear regression model &lt;a href=&quot;#f1&quot; id=&quot;id30&quot;&gt;[8]&lt;/a&gt; using the spatial median which is a generalization of the median to multiple dimensions &lt;a href=&quot;#f2&quot; id=&quot;id31&quot;&gt;[9]&lt;/a&gt;.</source>
          <target state="translated">実装&lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt; &lt;code&gt;TheilSenRegressor&lt;/code&gt; &lt;/a&gt; scikit学習では、多変量線形回帰モデルの一般化は以下の&lt;a href=&quot;#f1&quot; id=&quot;id30&quot;&gt;[8]&lt;/a&gt;複数の寸法の中央値の一般化である空間メジアン使用&lt;a href=&quot;#f2&quot; id=&quot;id31&quot;&gt;[9] &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="eb185b2e148e3613cdde933dd05a6b32e3b9acbe" translate="yes" xml:space="preserve">
          <source>The implementation of SGD is influenced by the &lt;a href=&quot;http://leon.bottou.org/projects/sgd&quot;&gt;Stochastic Gradient SVM&lt;/a&gt; of L&amp;eacute;on Bottou. Similar to SvmSGD, the weight vector is represented as the product of a scalar and a vector which allows an efficient weight update in the case of L2 regularization. In the case of sparse feature vectors, the intercept is updated with a smaller learning rate (multiplied by 0.01) to account for the fact that it is updated more frequently. Training examples are picked up sequentially and the learning rate is lowered after each observed example. We adopted the learning rate schedule from Shalev-Shwartz et al. 2007. For multi-class classification, a &amp;ldquo;one versus all&amp;rdquo; approach is used. We use the truncated gradient algorithm proposed by Tsuruoka et al. 2009 for L1 regularization (and the Elastic Net). The code is written in Cython.</source>
          <target state="translated">SGDの実装は、&lt;a href=&quot;http://leon.bottou.org/projects/sgd&quot;&gt;確率的勾配SVMの&lt;/a&gt;影響を受けますレオン・ボトゥの。 SvmSGDと同様に、重みベクトルはスカラーとL2正則化の場合に効率的な重み更新を可能にするベクトルの積として表されます。スパース特徴ベクトルの場合、より頻繁に更新されるという事実を考慮して、切片はより小さな学習率（0.01倍）で更新されます。トレーニング例は順番に取り上げられ、観察された例ごとに学習率が低下します。 Shalev-Shwartzらの学習率スケジュールを採用しました。 2007.マルチクラス分類では、「1対すべて」のアプローチが使用されます。鶴岡他により提案された打ち切り勾配アルゴリズムを使用する。 L1正則化のための2009（およびElastic Net）。コードはCythonで書かれています。</target>
        </trans-unit>
        <trans-unit id="234c6abfe6fe107b5770fe372b7d9e14789fc069" translate="yes" xml:space="preserve">
          <source>The implementation of logistic regression in scikit-learn can be accessed from class &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;. This implementation can fit binary, One-vs- Rest, or multinomial logistic regression with optional L2 or L1 regularization.</source>
          <target state="translated">scikit-learnのロジスティック回帰の実装には、&lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt;クラスからアクセスできます。この実装は、オプションのL2またはL1正則化を使用して、バイナリ、One対Rest、または多項ロジスティック回帰に適合できます。</target>
        </trans-unit>
        <trans-unit id="11a45f793137837fb140d6499ba2bfe32aacbae3" translate="yes" xml:space="preserve">
          <source>The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance.</source>
          <target state="translated">特徴の重要度は,その特徴によってもたらされる基準の(正規化された)総削減量として計算される.これは,ジニ重要度としても知られている.</target>
        </trans-unit>
        <trans-unit id="285ba1aeef83b4f72aaed81188dabeb94680bb69" translate="yes" xml:space="preserve">
          <source>The improvement in loss (= deviance) on the out-of-bag samples relative to the previous iteration. &lt;code&gt;oob_improvement_[0]&lt;/code&gt; is the improvement in loss of the first stage over the &lt;code&gt;init&lt;/code&gt; estimator.</source>
          <target state="translated">前の反復と比較した、out-of-bagサンプルの損失（=逸脱）の改善。 &lt;code&gt;oob_improvement_[0]&lt;/code&gt; は、 &lt;code&gt;init&lt;/code&gt; 推定量よりも第1ステージの損失の改善です。</target>
        </trans-unit>
        <trans-unit id="d60773790f2e7bcb156876ca46ea3f1191f69076" translate="yes" xml:space="preserve">
          <source>The impurity at \(m\) is computed using an impurity function \(H()\), the choice of which depends on the task being solved (classification or regression)</source>
          <target state="translated">不純物は、不純物関数を使って計算されます。</target>
        </trans-unit>
        <trans-unit id="f89a5a9d5e4b0a0db4c9a50975418d448408f475" translate="yes" xml:space="preserve">
          <source>The imputation fill value for each feature if axis == 0.</source>
          <target state="translated">axis ==0 の場合の各特徴のインputation fill 値。</target>
        </trans-unit>
        <trans-unit id="52ad716a60ba342ef84206f96283ef6240a59825" translate="yes" xml:space="preserve">
          <source>The imputation fill value for each feature.</source>
          <target state="translated">各特徴のインputation fill値。</target>
        </trans-unit>
        <trans-unit id="7cd5e57b66f65afed5b5b066bdba074e0cee3d73" translate="yes" xml:space="preserve">
          <source>The imputation strategy.</source>
          <target state="translated">インプット戦略。</target>
        </trans-unit>
        <trans-unit id="aa1b27a8286b1b98805decd6657b8fa02347921e" translate="yes" xml:space="preserve">
          <source>The index (of the &lt;code&gt;cv_results_&lt;/code&gt; arrays) which corresponds to the best candidate parameter setting.</source>
          <target state="translated">最適な候補パラメーター設定に対応する（ &lt;code&gt;cv_results_&lt;/code&gt; 配列の）インデックス。</target>
        </trans-unit>
        <trans-unit id="d8f1885dbc976f2ceb3f7711b10b324e6546b97b" translate="yes" xml:space="preserve">
          <source>The index is computed only quantities and features inherent to the dataset.</source>
          <target state="translated">インデックスは,データセットに固有の量と特徴のみを計算します.</target>
        </trans-unit>
        <trans-unit id="eaf4dd06369196819331db8b96a42731aed3d8e0" translate="yes" xml:space="preserve">
          <source>The index is defined as the average similarity between each cluster \(C_i\) for \(i=1, ..., k\) and its most similar one \(C_j\). In the context of this index, similarity is defined as a measure \(R_{ij}\) that trades off:</source>
          <target state="translated">この指標は、各クラスタ間の平均類似度として定義される。この指標では、類似度は、トレードオフする尺度として定義される。</target>
        </trans-unit>
        <trans-unit id="f4af6c67193225ac6c1c304797e9ea3402785083" translate="yes" xml:space="preserve">
          <source>The index of the cluster.</source>
          <target state="translated">クラスタのインデックス。</target>
        </trans-unit>
        <trans-unit id="bf1ee02857437dc2bc3e741c8e3e56ef69d1ae10" translate="yes" xml:space="preserve">
          <source>The index value of a word in the vocabulary is linked to its frequency in the whole training corpus.</source>
          <target state="translated">単語のインデックス値は、学習コーパス全体の中での頻度とリンクしています。</target>
        </trans-unit>
        <trans-unit id="03c2241f87945c783a2d50da20a997d6e73ac147" translate="yes" xml:space="preserve">
          <source>The indexes of the sorted training input samples. If many tree are grown on the same dataset, this allows the ordering to be cached between trees. If None, the data will be sorted here. Don&amp;rsquo;t use this parameter unless you know what to do.</source>
          <target state="translated">ソートされたトレーニング入力サンプルのインデックス。多くのツリーが同じデータセットで成長している場合、これにより、ツリー間で順序をキャッシュできます。Noneの場合、データはここでソートされます。何をすべきか分からない場合は、このパラメーターを使用しないでください。</target>
        </trans-unit>
        <trans-unit id="0f615a1a241d635bbd7f8073a9962f88455d5840" translate="yes" xml:space="preserve">
          <source>The indicators for cluster membership of each column.</source>
          <target state="translated">各列のクラスターメンバーシップを示す指標。</target>
        </trans-unit>
        <trans-unit id="7c7c8e5dd3219610397a571246f91a523e9b0296" translate="yes" xml:space="preserve">
          <source>The indicators for cluster membership of each row.</source>
          <target state="translated">各行のクラスターメンバーシップを示す指標。</target>
        </trans-unit>
        <trans-unit id="1d09056572af0ab7d260b8a9a458cace43041c54" translate="yes" xml:space="preserve">
          <source>The inertia matrix uses a Heapq-based representation.</source>
          <target state="translated">慣性行列は、Heapq ベースの表現を使用しています。</target>
        </trans-unit>
        <trans-unit id="4f68a65921a2da6924b081b35cf28ef22fa8adaa" translate="yes" xml:space="preserve">
          <source>The inferred value of max_features.</source>
          <target state="translated">max_featuresの推定値。</target>
        </trans-unit>
        <trans-unit id="9af964a5bdc6c7f6e3e23dbd4ccea9f871ad0d0c" translate="yes" xml:space="preserve">
          <source>The initial coefficients to warm-start the optimization.</source>
          <target state="translated">最適化をウォームスタートするための初期係数。</target>
        </trans-unit>
        <trans-unit id="e326d26a40d08d9ebf56e15d175f98f61ace6983" translate="yes" xml:space="preserve">
          <source>The initial guess for the covariance.</source>
          <target state="translated">共分散の最初の推測。</target>
        </trans-unit>
        <trans-unit id="a31ad0b6b83b24ae472799017e15e5984848ac41" translate="yes" xml:space="preserve">
          <source>The initial guess of the noise variance for each feature. If None, it defaults to np.ones(n_features)</source>
          <target state="translated">各特徴量のノイズ分散の初期推定値.Noneの場合,デフォルトはnp.ones(n_features)です.</target>
        </trans-unit>
        <trans-unit id="5af1f38d3d578b440a3ef0c3e0d6f491e678143f" translate="yes" xml:space="preserve">
          <source>The initial intercept to warm-start the optimization.</source>
          <target state="translated">最適化をウォームスタートするための初期切片。</target>
        </trans-unit>
        <trans-unit id="ffb7aede167c9f36a232ef307d2df81471bb1b4f" translate="yes" xml:space="preserve">
          <source>The initial learning rate for the &amp;lsquo;constant&amp;rsquo;, &amp;lsquo;invscaling&amp;rsquo; or &amp;lsquo;adaptive&amp;rsquo; schedules. The default value is 0.0 as eta0 is not used by the default schedule &amp;lsquo;optimal&amp;rsquo;.</source>
          <target state="translated">「一定」、「invscaling」、または「適応」スケジュールの初期学習率。eta0はデフォルトのスケジュール「optimal」では使用されないため、デフォルト値は0.0です。</target>
        </trans-unit>
        <trans-unit id="f396af141edab02acfc3d8c05ea1f6e174dd0d5b" translate="yes" xml:space="preserve">
          <source>The initial learning rate used. It controls the step-size in updating the weights. Only used when solver=&amp;rsquo;sgd&amp;rsquo; or &amp;lsquo;adam&amp;rsquo;.</source>
          <target state="translated">使用された初期学習率。重みを更新する際のステップサイズを制御します。solver = 'sgd'または 'adam'の場合にのみ使用されます。</target>
        </trans-unit>
        <trans-unit id="8aa315cba5e51992abb63a7c33e92703aa91a364" translate="yes" xml:space="preserve">
          <source>The initial model \(F_{0}\) is problem specific, for least-squares regression one usually chooses the mean of the target values.</source>
          <target state="translated">初期モデル \(F_{0}\)は問題に特有のもので、最小二乗回帰では、通常、目標値の平均値を選択します。</target>
        </trans-unit>
        <trans-unit id="fcbcdc2001232ebf2d1267dbd5c1db8c05bef33b" translate="yes" xml:space="preserve">
          <source>The initial model can also be specified via the &lt;code&gt;init&lt;/code&gt; argument. The passed object has to implement &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;.</source>
          <target state="translated">初期モデルは、 &lt;code&gt;init&lt;/code&gt; 引数で指定することもできます。渡されたオブジェクトは、 &lt;code&gt;fit&lt;/code&gt; および &lt;code&gt;predict&lt;/code&gt; を実装する必要があります。</target>
        </trans-unit>
        <trans-unit id="17bd189eec62d21587058ccd88b444461d73119b" translate="yes" xml:space="preserve">
          <source>The initial values of the coefficients.</source>
          <target state="translated">係数の初期値です。</target>
        </trans-unit>
        <trans-unit id="32a92ee0ef2ffe7a34cba56e64f123bd9b2ca181" translate="yes" xml:space="preserve">
          <source>The input data consists of 28x28 pixel handwritten digits, leading to 784 features in the dataset. Therefore the first layer weight matrix have the shape (784, hidden_layer_sizes[0]). We can therefore visualize a single column of the weight matrix as a 28x28 pixel image.</source>
          <target state="translated">入力データは28x28ピクセルの手書き数字からなり,データセットの特徴量は784個である.したがって,第1層の重み行列は,(784,hidden_layer_size[0])の形をしています.したがって,重み行列の1列を28x28ピクセルの画像として可視化することができます.</target>
        </trans-unit>
        <trans-unit id="c8ff70c87eb4edb0d6df0b02362dd0f826eb86c0" translate="yes" xml:space="preserve">
          <source>The input data matrix</source>
          <target state="translated">入力データ行列</target>
        </trans-unit>
        <trans-unit id="28402823f8037a445e34f883189d18c9c8586801" translate="yes" xml:space="preserve">
          <source>The input data to complete.</source>
          <target state="translated">入力データを完成させる。</target>
        </trans-unit>
        <trans-unit id="10ea3f8f2392cebb3ea568adacc2ca87c3d24f21" translate="yes" xml:space="preserve">
          <source>The input data to project into a smaller dimensional space.</source>
          <target state="translated">より小さな次元の空間に投影するための入力データ。</target>
        </trans-unit>
        <trans-unit id="8b941f334dfce6c774093743936f754bf8137c6b" translate="yes" xml:space="preserve">
          <source>The input data.</source>
          <target state="translated">入力データです。</target>
        </trans-unit>
        <trans-unit id="a383a11075a72cc4c498a6da492b12429e0d8bd0" translate="yes" xml:space="preserve">
          <source>The input matrix \(A\) is first normalized to make the checkerboard pattern more obvious. There are three possible methods:</source>
          <target state="translated">入力行列を正規化して、市松模様をより明確にする。3つの方法が考えられます。</target>
        </trans-unit>
        <trans-unit id="5296b71b9c59f1ea88d1300f56d80b11acdd4263" translate="yes" xml:space="preserve">
          <source>The input matrix \(A\) is preprocessed as follows:</source>
          <target state="translated">入力行列は、以下のように前処理されます。</target>
        </trans-unit>
        <trans-unit id="1dfee05c320291671e1507b730cbc56e48fcc05b" translate="yes" xml:space="preserve">
          <source>The input samples with only the selected features.</source>
          <target state="translated">選択された特徴量のみの入力サンプル。</target>
        </trans-unit>
        <trans-unit id="aa9c51d1052bb07178d99dc6b9998838f3fd04f4" translate="yes" xml:space="preserve">
          <source>The input samples.</source>
          <target state="translated">入力サンプルです。</target>
        </trans-unit>
        <trans-unit id="691f5d8dd518149d763a76be65224ad6a47a3de2" translate="yes" xml:space="preserve">
          <source>The input samples. Internally, it will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt; and if a sparse matrix is provided to a sparse &lt;code&gt;csr_matrix&lt;/code&gt;.</source>
          <target state="translated">入力サンプル。内部的に、それは &lt;code&gt;dtype=np.float32&lt;/code&gt; に変換され、疎行列が疎 &lt;code&gt;csr_matrix&lt;/code&gt; に提供される場合。</target>
        </trans-unit>
        <trans-unit id="80a0f34908e0a755318c3eb528444068386165b2" translate="yes" xml:space="preserve">
          <source>The input samples. Internally, its dtype will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt;. If a sparse matrix is provided, it will be converted into a sparse &lt;code&gt;csr_matrix&lt;/code&gt;.</source>
          <target state="translated">入力サンプル。内部的には、そのdtypeは &lt;code&gt;dtype=np.float32&lt;/code&gt; に変換されます。スパース行列が指定されている場合、それはスパース &lt;code&gt;csr_matrix&lt;/code&gt; に変換されます。</target>
        </trans-unit>
        <trans-unit id="ad241eeecc4b3d71885329d69a3ecbb931dd0903" translate="yes" xml:space="preserve">
          <source>The input samples. Internally, its dtype will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt;. If a sparse matrix is provided, it will be converted to a sparse &lt;code&gt;csr_matrix&lt;/code&gt;.</source>
          <target state="translated">入力サンプル。内部的には、そのdtypeは &lt;code&gt;dtype=np.float32&lt;/code&gt; に変換されます。スパース行列が指定されている場合、それはスパース &lt;code&gt;csr_matrix&lt;/code&gt; に変換されます。</target>
        </trans-unit>
        <trans-unit id="b99e327cb34f5c84fcbd4e91cfb496e38c8c302f" translate="yes" xml:space="preserve">
          <source>The input samples. Use &lt;code&gt;dtype=np.float32&lt;/code&gt; for maximum efficiency. Sparse matrices are also supported, use sparse &lt;code&gt;csc_matrix&lt;/code&gt; for maximum efficiency.</source>
          <target state="translated">入力サンプル。効率を最大にするには、 &lt;code&gt;dtype=np.float32&lt;/code&gt; を使用します。スパース行列もサポートされています。効率を最大にするには、スパース &lt;code&gt;csc_matrix&lt;/code&gt; を使用してください。</target>
        </trans-unit>
        <trans-unit id="6e6a9dea87222450502841b10c214ed0343cd360" translate="yes" xml:space="preserve">
          <source>The input set can either be well conditioned (by default) or have a low rank-fat tail singular profile. See &lt;a href=&quot;sklearn.datasets.make_low_rank_matrix#sklearn.datasets.make_low_rank_matrix&quot;&gt;&lt;code&gt;make_low_rank_matrix&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">入力セットは、十分に条件付けされている（デフォルト）か、低ランク脂肪尾特異プロファイルを持つことができます。詳細については、&lt;a href=&quot;sklearn.datasets.make_low_rank_matrix#sklearn.datasets.make_low_rank_matrix&quot;&gt; &lt;code&gt;make_low_rank_matrix&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="289f5749e232fcbed1540450cf860647da777cb8" translate="yes" xml:space="preserve">
          <source>The input set is well conditioned, centered and gaussian with unit variance.</source>
          <target state="translated">入力集合は、良好な条件付きで、中央に位置し、単位分散を持つガウス分布です。</target>
        </trans-unit>
        <trans-unit id="9196dd161fc035e27746b1e485ce67d2bc4a2f97" translate="yes" xml:space="preserve">
          <source>The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are converted to ordinal integers. This results in a single column of integers (0 to n_categories - 1) per feature.</source>
          <target state="translated">この変換器への入力は,カテゴリ的(離散的)特徴量によって取られる値を表す整数または文字列の配列のようなものでなければなりません.特徴量は,順序の整数に変換されます.これにより,特徴量ごとに1列の整数(0からn_categories-1)が得られます.</target>
        </trans-unit>
        <trans-unit id="6fca358f78b22e3193c17b331b39a22e0cf2c917" translate="yes" xml:space="preserve">
          <source>The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka &amp;lsquo;one-of-K&amp;rsquo; or &amp;lsquo;dummy&amp;rsquo;) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array.</source>
          <target state="translated">このトランスフォーマーへの入力は、整数または文字列の配列のようなものである必要があり、カテゴリー（離散）機能がとる値を示します。機能は、ワンホット（別名「one-of-K」または「ダミー」）エンコード方式を使用してエンコードされます。これにより、各カテゴリのバイナリ列が作成され、疎行列または密配列が返されます。</target>
        </trans-unit>
        <trans-unit id="d6846270bbc73ec820141552eb32b90be4589180" translate="yes" xml:space="preserve">
          <source>The integer id or the string name metadata of the MLComp dataset to load</source>
          <target state="translated">ロードするMLCompデータセットの整数idまたは文字列名のメタデータ</target>
        </trans-unit>
        <trans-unit id="0ff5072970d94a45dfd0b3dc59cc200193becdfd" translate="yes" xml:space="preserve">
          <source>The integer labels (0 or 1) for class membership of each sample.</source>
          <target state="translated">各サンプルのクラスメンバーシップを表す整数のラベル(0または1)。</target>
        </trans-unit>
        <trans-unit id="a6afcce2561a80c34c914f95ed66620399eb0d7c" translate="yes" xml:space="preserve">
          <source>The integer labels for class membership of each sample.</source>
          <target state="translated">各サンプルのクラスメンバーシップを表す整数のラベル。</target>
        </trans-unit>
        <trans-unit id="868af1be557e4830c6f25b4ff3211b7a9aad7eb0" translate="yes" xml:space="preserve">
          <source>The integer labels for cluster membership of each sample.</source>
          <target state="translated">各サンプルのクラスタメンバーシップを表す整数のラベル。</target>
        </trans-unit>
        <trans-unit id="05d4e37f01ed47119c132248581df1196214fb7d" translate="yes" xml:space="preserve">
          <source>The integer labels for quantile membership of each sample.</source>
          <target state="translated">各標本の分位メンバーシップの整数ラベル。</target>
        </trans-unit>
        <trans-unit id="7da460eaa0239b5647e6b97a087bcede08e5a0c5" translate="yes" xml:space="preserve">
          <source>The intercept of the model. Only returned if &lt;code&gt;return_intercept&lt;/code&gt; is True and if X is a scipy sparse array.</source>
          <target state="translated">モデルの切片。 &lt;code&gt;return_intercept&lt;/code&gt; がTrueで、Xがscipyスパース配列の場合にのみ返されます。</target>
        </trans-unit>
        <trans-unit id="f0408bacce1626a183a8eadd09dc2d6f8b9e549c" translate="yes" xml:space="preserve">
          <source>The intercept term.</source>
          <target state="translated">インターセプト用語です。</target>
        </trans-unit>
        <trans-unit id="39d99a92784fd6547680c69e2a029d63ab730943" translate="yes" xml:space="preserve">
          <source>The inverse document frequency (IDF) vector; only defined if &lt;code&gt;use_idf&lt;/code&gt; is True.</source>
          <target state="translated">逆ドキュメント頻度（IDF）ベクトル。 &lt;code&gt;use_idf&lt;/code&gt; がTrueの場合にのみ定義されます。</target>
        </trans-unit>
        <trans-unit id="70e42545b4f301fb133ffa67ef01dbd1c81a14a6" translate="yes" xml:space="preserve">
          <source>The inverse of the Box-Cox transformation is given by:</source>
          <target state="translated">Box-Cox変換の逆数は次式で与えられる。</target>
        </trans-unit>
        <trans-unit id="1296b0fc0246edf109c97645c02261b1d3d028bb" translate="yes" xml:space="preserve">
          <source>The inverse of the Yeo-Johnson transformation is given by:</source>
          <target state="translated">Yeo-Johnson変換の逆数は次式で与えられる。</target>
        </trans-unit>
        <trans-unit id="8621ba40f721d68ec6f12f67b5e3cc856e6fc6d9" translate="yes" xml:space="preserve">
          <source>The iris dataset is a classic and very easy multi-class classification dataset.</source>
          <target state="translated">虹彩データセットは、古典的で非常に簡単な多クラス分類データセットです。</target>
        </trans-unit>
        <trans-unit id="82f3a3d98edcec3969a3e26fc0789c7c6c1f74ef" translate="yes" xml:space="preserve">
          <source>The iris dataset is a classification task consisting in identifying 3 different types of irises (Setosa, Versicolour, and Virginica) from their petal and sepal length and width:</source>
          <target state="translated">花菖蒲のデータセットは、花弁と萼の長さと幅から3種類の花菖蒲(Setosa,Versicolour,Virginica)を識別する分類タスクである。</target>
        </trans-unit>
        <trans-unit id="968fa226a66bed28f98c84df2ac306ac992e59c2" translate="yes" xml:space="preserve">
          <source>The isotonic regression optimization problem is defined by:</source>
          <target state="translated">等張回帰最適化問題は次のように定義される。</target>
        </trans-unit>
        <trans-unit id="710a9703f51d61fb5f41b8c77a926ca070febd6c" translate="yes" xml:space="preserve">
          <source>The iteration will stop when &lt;code&gt;max{|proj g_i | i = 1, ..., n}&lt;/code&gt; &amp;lt;= &lt;code&gt;tol&lt;/code&gt; where pg_i is the i-th component of the projected gradient.</source>
          <target state="translated">反復は、 &lt;code&gt;max{|proj g_i | i = 1, ..., n}&lt;/code&gt; ときに停止します。i = 1、...、n} &amp;lt;= &lt;code&gt;tol&lt;/code&gt; ここで、pg_iは投影された勾配のi番目の成分です。</target>
        </trans-unit>
        <trans-unit id="d1f978f02193a9d7a16e2ac4f28051f1be20fa33" translate="yes" xml:space="preserve">
          <source>The iterator consumption and dispatching is protected by the same lock so calling this function should be thread safe.</source>
          <target state="translated">イテレータの消費とディスパッチは同じロックで保護されているので、この関数の呼び出しはスレッドセーフであるべきです。</target>
        </trans-unit>
        <trans-unit id="e6bcf49b626a9dedb0663face3bd765b3dedb378" translate="yes" xml:space="preserve">
          <source>The ith element in the list represents the bias vector corresponding to layer i + 1.</source>
          <target state="translated">リストのi番目の要素は、レイヤi+1に対応するバイアスベクトルを表す。</target>
        </trans-unit>
        <trans-unit id="4b396b269a89e1a7151872f147950b58bd31d2c2" translate="yes" xml:space="preserve">
          <source>The ith element in the list represents the weight matrix corresponding to layer i.</source>
          <target state="translated">リストの第i番目の要素は、レイヤiに対応する重み行列を表す。</target>
        </trans-unit>
        <trans-unit id="8ce9197f7d2eba2389cfe44c6d9806383e84232a" translate="yes" xml:space="preserve">
          <source>The ith element represents the number of neurons in the ith hidden layer.</source>
          <target state="translated">ith要素は、ith隠れ層のニューロンの数を表す。</target>
        </trans-unit>
        <trans-unit id="110dde91ce5735296cfcdbdf4849eb2634f008ea" translate="yes" xml:space="preserve">
          <source>The justification for the formula in the loss=&amp;rdquo;modified_huber&amp;rdquo; case is in the appendix B in: &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf&quot;&gt;http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf&lt;/a&gt;</source>
          <target state="translated">loss =&amp;rdquo; modified_huber&amp;rdquo;の場合の式の正当性は、&lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf&quot;&gt;http&lt;/a&gt;：//jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdfの付録Bにあります。</target>
        </trans-unit>
        <trans-unit id="2ad3a7b0acdb773b3a5a78e7f86e7401f887f5a4" translate="yes" xml:space="preserve">
          <source>The k-means algorithm divides a set of \(N\) samples \(X\) into \(K\) disjoint clusters \(C\), each described by the mean \(\mu_j\) of the samples in the cluster. The means are commonly called the cluster &amp;ldquo;centroids&amp;rdquo;; note that they are not, in general, points from \(X\), although they live in the same space. The K-means algorithm aims to choose centroids that minimise the &lt;em&gt;inertia&lt;/em&gt;, or within-cluster sum of squared criterion:</source>
          <target state="translated">k-meansアルゴリズムは、\（N \）サンプルのセット\（X \）を\（K \）ばらばらのクラスター\（C \）に分割します。それぞれがサンプルの平均\（\ mu_j \）で記述されます。集まる。この手段は一般にクラスター「重心」と呼ばれます。それらは同じ空間に存在しますが、一般的には\（X \）からのポイントではないことに注意してください。K平均アルゴリズムは、&lt;em&gt;慣性&lt;/em&gt;を最小化する重心、またはクラスター内の二乗基準の合計を選択することを目的としています。</target>
        </trans-unit>
        <trans-unit id="3042a74be3a11ce6ced2d21d393c8c55a0b044ff" translate="yes" xml:space="preserve">
          <source>The k-means problem is solved using either Lloyd&amp;rsquo;s or Elkan&amp;rsquo;s algorithm.</source>
          <target state="translated">k-means問題は、ロイドまたはエルカンのアルゴリズムを使用して解決されます。</target>
        </trans-unit>
        <trans-unit id="698fe2144ac8be0b801483c0a1d13dc4d07c73ce" translate="yes" xml:space="preserve">
          <source>The kappa score (see docstring) is a number between -1 and 1. Scores above .8 are generally considered good agreement; zero or lower means no agreement (practically random labels).</source>
          <target state="translated">カッパスコア(docstring参照)は、-1から1の間の数値です。 0.8以上のスコアは一般的に良好な一致とみなされ、0以下は一致しないことを意味します(実質的にランダムなラベル)。</target>
        </trans-unit>
        <trans-unit id="951d69c60ea24190bd0c5a4dcae9aeb94c1c913d" translate="yes" xml:space="preserve">
          <source>The kappa statistic, which is a number between -1 and 1. The maximum value means complete agreement; zero or lower means chance agreement.</source>
          <target state="translated">カッパ統計量は、-1から1の間の数値で、最大値は完全一致を意味し、0以下は偶然の一致を意味する。</target>
        </trans-unit>
        <trans-unit id="e7d87035112e23c601a476b538c50df786049966" translate="yes" xml:space="preserve">
          <source>The kernel density estimator can be used with any of the valid distance metrics (see &lt;a href=&quot;generated/sklearn.neighbors.distancemetric#sklearn.neighbors.DistanceMetric&quot;&gt;&lt;code&gt;sklearn.neighbors.DistanceMetric&lt;/code&gt;&lt;/a&gt; for a list of available metrics), though the results are properly normalized only for the Euclidean metric. One particularly useful metric is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Haversine_formula&quot;&gt;Haversine distance&lt;/a&gt; which measures the angular distance between points on a sphere. Here is an example of using a kernel density estimate for a visualization of geospatial data, in this case the distribution of observations of two different species on the South American continent:</source>
          <target state="translated">カーネル密度推定器は、有効な距離メトリック（使用可能なメトリックのリストについては&lt;a href=&quot;generated/sklearn.neighbors.distancemetric#sklearn.neighbors.DistanceMetric&quot;&gt; &lt;code&gt;sklearn.neighbors.DistanceMetric&lt;/code&gt; &lt;/a&gt;を参照）のいずれかで使用できますが、結果はユークリッドメトリックに対してのみ正しく正規化されます。特に有用なメトリックの1つは、球上のポイント間の角距離を測定する&lt;a href=&quot;https://en.wikipedia.org/wiki/Haversine_formula&quot;&gt;ハバシン距離&lt;/a&gt;です。これは、地理空間データの視覚化にカーネル密度推定を使用する例です。この場合、南米大陸の2つの異なる種の観測の分布です。</target>
        </trans-unit>
        <trans-unit id="f5acb5ee70e93822d6214e7faa0671c1edbb5e04" translate="yes" xml:space="preserve">
          <source>The kernel is composed of several terms that are responsible for explaining different properties of the signal:</source>
          <target state="translated">カーネルは、信号のさまざまな特性を説明するためのいくつかの用語で構成されています。</target>
        </trans-unit>
        <trans-unit id="b2a264496619ccec6a15428322dff95c6847afef" translate="yes" xml:space="preserve">
          <source>The kernel specifying the covariance function of the GP. If None is passed, the kernel &amp;ldquo;1.0 * RBF(1.0)&amp;rdquo; is used as default. Note that the kernel&amp;rsquo;s hyperparameters are optimized during fitting.</source>
          <target state="translated">GPの共分散関数を指定するカーネル。Noneが渡されると、カーネル「1.0 * RBF（1.0）」がデフォルトとして使用されます。カーネルのハイパーパラメータはフィッティング中に最適化されることに注意してください。</target>
        </trans-unit>
        <trans-unit id="3f4b36434e95db38ce416f2948e370303e8d7ce4" translate="yes" xml:space="preserve">
          <source>The kernel to use. Valid kernels are [&amp;lsquo;gaussian&amp;rsquo;|&amp;rsquo;tophat&amp;rsquo;|&amp;rsquo;epanechnikov&amp;rsquo;|&amp;rsquo;exponential&amp;rsquo;|&amp;rsquo;linear&amp;rsquo;|&amp;rsquo;cosine&amp;rsquo;] Default is &amp;lsquo;gaussian&amp;rsquo;.</source>
          <target state="translated">使用するカーネル。有効なカーネルは['gaussian' | 'tophat' | 'epanechnikov' | 'exponential' | 'linear' | 'cosine']です。デフォルトは 'gaussian'です。</target>
        </trans-unit>
        <trans-unit id="7d7a3328bfc6be5d4c52eb80a02db6810f6af97a" translate="yes" xml:space="preserve">
          <source>The kernel used for prediction. In case of binary classification, the structure of the kernel is the same as the one passed as parameter but with optimized hyperparameters. In case of multi-class classification, a CompoundKernel is returned which consists of the different kernels used in the one-versus-rest classifiers.</source>
          <target state="translated">予測に利用されるカーネル.2値分類の場合,カーネルの構造はパラメータとして渡されたものと同じですが,ハイパーパラメータが最適化されています.複数クラス分類の場合は,1-versus-rest分類器で使用された異なるカーネルからなるCompoundKernelが返されます.</target>
        </trans-unit>
        <trans-unit id="5a1810e4f4370ea9d4b3eb4b69c5661264f9680f" translate="yes" xml:space="preserve">
          <source>The kernel used for prediction. The structure of the kernel is the same as the one passed as parameter but with optimized hyperparameters</source>
          <target state="translated">予測に利用されるカーネル.カーネルの構造は,パラメータとして渡されたものと同じですが,ハイパーパラメータが最適化されています.</target>
        </trans-unit>
        <trans-unit id="317ceb705f764af792a5c1b02b0c75da4f7767d2" translate="yes" xml:space="preserve">
          <source>The key &lt;code&gt;'params'&lt;/code&gt; is used to store a list of parameter settings dicts for all the parameter candidates.</source>
          <target state="translated">キー &lt;code&gt;'params'&lt;/code&gt; は、すべてのパラメーター候補のパラメーター設定辞書のリストを格納するために使用されます。</target>
        </trans-unit>
        <trans-unit id="467dfd2cbee563f1dc16d74e0763e99176db4d68" translate="yes" xml:space="preserve">
          <source>The l1-penalized estimator can recover part of this off-diagonal structure. It learns a sparse precision. It is not able to recover the exact sparsity pattern: it detects too many non-zero coefficients. However, the highest non-zero coefficients of the l1 estimated correspond to the non-zero coefficients in the ground truth. Finally, the coefficients of the l1 precision estimate are biased toward zero: because of the penalty, they are all smaller than the corresponding ground truth value, as can be seen on the figure.</source>
          <target state="translated">l1-penalized estimatorは、この対角線から外れた構造の一部を回復することができる。これは疎な精度を学習する。それは正確な疎度パターンを回復することができません:それはあまりにも多くの非ゼロ係数を検出します。しかし、推定されたl1の最も高い非ゼロ係数は、基底真理の非ゼロ係数に対応している。最後に、l1精度推定の係数はゼロに偏っています:ペナルティのため、図で見ることができるように、それらはすべて対応する基底真理値よりも小さくなっています。</target>
        </trans-unit>
        <trans-unit id="3f115dd7f6ab226f3d1efb2261d982c44eb021f0" translate="yes" xml:space="preserve">
          <source>The label of the positive class</source>
          <target state="translated">正のクラスのラベル</target>
        </trans-unit>
        <trans-unit id="1daeeb6b0700e2caf583964dcec09c381eeb8ea9" translate="yes" xml:space="preserve">
          <source>The label of the positive class. Only applied to binary &lt;code&gt;y_true&lt;/code&gt;. For multilabel-indicator &lt;code&gt;y_true&lt;/code&gt;, &lt;code&gt;pos_label&lt;/code&gt; is fixed to 1.</source>
          <target state="translated">ポジティブクラスのラベル。バイナリ &lt;code&gt;y_true&lt;/code&gt; にのみ適用されます。マルチラベルインジケーター &lt;code&gt;y_true&lt;/code&gt; の場合、 &lt;code&gt;pos_label&lt;/code&gt; は1に固定されます。</target>
        </trans-unit>
        <trans-unit id="844ad03ed5a5991044f5c2a9015a7d46ffc83176" translate="yes" xml:space="preserve">
          <source>The label sets.</source>
          <target state="translated">ラベルセットです。</target>
        </trans-unit>
        <trans-unit id="34e6595f06fcaf1ac9d996e42fb91cf1dc35ee40" translate="yes" xml:space="preserve">
          <source>The labels of the clusters.</source>
          <target state="translated">クラスターのラベル。</target>
        </trans-unit>
        <trans-unit id="f920ebdbb736d4fac6e624f1f0a24fae7f686c27" translate="yes" xml:space="preserve">
          <source>The laplacian kernel is defined as:</source>
          <target state="translated">ラプラシアンカーネルは次のように定義されています。</target>
        </trans-unit>
        <trans-unit id="321fd6d2804d3c8ea8df389dda895240e812e24c" translate="yes" xml:space="preserve">
          <source>The lasso estimate thus solves the minimization of the least-squares penalty with \(\alpha ||w||_1\) added, where \(\alpha\) is a constant and \(||w||_1\) is the \(\ell_1\)-norm of the parameter vector.</source>
          <target state="translated">lasso estimateは、このようにして、最小二乗ペナルティの最小化を、\(\alpha ||w||_1)addedで解くことができます。</target>
        </trans-unit>
        <trans-unit id="374010cdf4f6f26c62c956d36c36442b1f12b646" translate="yes" xml:space="preserve">
          <source>The last characteristic implies that the Perceptron is slightly faster to train than SGD with the hinge loss and that the resulting models are sparser.</source>
          <target state="translated">最後の特徴は、パーセプトロンの方がヒンジ損失を考慮した場合、SGDよりも学習速度がわずかに速く、結果として得られるモデルがスパースであることを示唆しています。</target>
        </trans-unit>
        <trans-unit id="6312a138a2adc5ee223c24bc72b6c9e5099fd7ea" translate="yes" xml:space="preserve">
          <source>The last dataset is an example of a &amp;lsquo;null&amp;rsquo; situation for clustering: the data is homogeneous, and there is no good clustering. For this example, the null dataset uses the same parameters as the dataset in the row above it, which represents a mismatch in the parameter values and the data structure.</source>
          <target state="translated">最後のデータセットは、クラスタリングの「null」状況の例です。データは均一であり、適切なクラスタリングはありません。この例では、nullデータセットは、その上の行のデータセットと同じパラメーターを使用しています。これは、パラメーター値とデータ構造の不一致を表しています。</target>
        </trans-unit>
        <trans-unit id="2cc9bc385685f028c8287b17d0d958b5050b56a0" translate="yes" xml:space="preserve">
          <source>The last precision and recall values are 1. and 0. respectively and do not have a corresponding threshold. This ensures that the graph starts on the y axis.</source>
          <target state="translated">最後の精度とリコール値はそれぞれ1.と0.であり、対応するしきい値を持たない。これにより、グラフがy軸上で始まることが保証される。</target>
        </trans-unit>
        <trans-unit id="9ddff80583e9f2c8a48f3b64e4a9a06a8a8cefcd" translate="yes" xml:space="preserve">
          <source>The last two panels show how we can sample from the last two models. The resulting samples distributions do not look exactly like the original data distribution. The difference primarily stems from the approximation error we made by using a model that assumes that the data was generated by a finite number of Gaussian components instead of a continuous noisy sine curve.</source>
          <target state="translated">最後の2つのパネルは、最後の2つのモデルからどのようにサンプリングできるかを示しています。結果として得られた標本分布は、元のデータ分布と全く同じようには見えません。この違いは、主に、データが連続したノイズの多い正弦曲線ではなく、有限のガウス成分によって生成されたと仮定したモデルを使用したことによる近似誤差に起因しています。</target>
        </trans-unit>
        <trans-unit id="a4514c8e853c44e1e4a78bf33662f9b32277ba44" translate="yes" xml:space="preserve">
          <source>The latent variables of X.</source>
          <target state="translated">Xの潜在変数。</target>
        </trans-unit>
        <trans-unit id="03a214378b3ef2e13a3cc6617d05b3fe221146c8" translate="yes" xml:space="preserve">
          <source>The learning rate \(\eta\) can be either constant or gradually decaying. For classification, the default learning rate schedule (&lt;code&gt;learning_rate='optimal'&lt;/code&gt;) is given by</source>
          <target state="translated">学習率\（\ eta \）は、一定にすることも、徐々に減衰させることもできます。分類の場合、デフォルトの学習率スケジュール（ &lt;code&gt;learning_rate='optimal'&lt;/code&gt; ）は次のように与えられます。</target>
        </trans-unit>
        <trans-unit id="36c5b5659a911c2db600cd9fbf9c0485b5053e95" translate="yes" xml:space="preserve">
          <source>The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If the learning rate is too high, the data may look like a &amp;lsquo;ball&amp;rsquo; with any point approximately equidistant from its nearest neighbours. If the learning rate is too low, most points may look compressed in a dense cloud with few outliers. If the cost function gets stuck in a bad local minimum increasing the learning rate may help.</source>
          <target state="translated">t-SNEの学習率は通常、[10.0、1000.0]の範囲です。学習率が高すぎる場合、データは「ボール」のように見える可能性があり、どの点も最も近い近傍から等距離にあります。学習率が低すぎる場合、ほとんどのポイントは、外れ値がほとんどない密集した雲の中で圧縮されて見えることがあります。コスト関数がローカルの最小値に留まっている場合は、学習率を上げると役立つことがあります。</target>
        </trans-unit>
        <trans-unit id="55a4138aa9f2827acbf6f24a7d7d78c5c9231271" translate="yes" xml:space="preserve">
          <source>The learning rate for weight updates. It is &lt;em&gt;highly&lt;/em&gt; recommended to tune this hyper-parameter. Reasonable values are in the 10**[0., -3.] range.</source>
          <target state="translated">体重更新の学習率。このハイパーパラメータを調整することを&lt;em&gt;強く&lt;/em&gt;お勧めします。妥当な値は10 ** [0。、-3。]の範囲です。</target>
        </trans-unit>
        <trans-unit id="aa4707838d034cc31029160760df726f17933ef5" translate="yes" xml:space="preserve">
          <source>The learning rate schedule:</source>
          <target state="translated">学習率のスケジュールです。</target>
        </trans-unit>
        <trans-unit id="866660dd294846977c41d9eb05a5f823762d12dd" translate="yes" xml:space="preserve">
          <source>The left and right examples highlight the &lt;code&gt;n_labels&lt;/code&gt; parameter: more of the samples in the right plot have 2 or 3 labels.</source>
          <target state="translated">左と右の例は、 &lt;code&gt;n_labels&lt;/code&gt; パラメーターを強調しています。右のプロットのより多くのサンプルには2つまたは3つのラベルがあります。</target>
        </trans-unit>
        <trans-unit id="3656a1b3513126216409c003f2c19b4a1bde366e" translate="yes" xml:space="preserve">
          <source>The leftmost layer, known as the input layer, consists of a set of neurons \(\{x_i | x_1, x_2, ..., x_m\}\) representing the input features. Each neuron in the hidden layer transforms the values from the previous layer with a weighted linear summation \(w_1x_1 + w_2x_2 + ... + w_mx_m\), followed by a non-linear activation function \(g(\cdot):R \rightarrow R\) - like the hyperbolic tan function. The output layer receives the values from the last hidden layer and transforms them into output values.</source>
          <target state="translated">一番左の層は、入力層と呼ばれ、入力の特徴を表すニューロンの集合である\(\{x_i | x_1,x_2,...,x_m\})からなる。隠れ層の各ニューロンは、前の層の値を加重線形和に変換し、その後、非線形活性化関数を用いて-双曲線タン関数のようなものです。出力層は最後の隠れた層から値を受け取り、出力値に変換します。</target>
        </trans-unit>
        <trans-unit id="7b2e6a226728025cdbf60737ae02746b4f5067e4" translate="yes" xml:space="preserve">
          <source>The length scale of the kernel.</source>
          <target state="translated">カーネルの長さのスケール。</target>
        </trans-unit>
        <trans-unit id="658641ffb44e40c4155905b8a662123f9fbe36a4" translate="yes" xml:space="preserve">
          <source>The length scale of the kernel. If a float, an isotropic kernel is used. If an array, an anisotropic kernel is used where each dimension of l defines the length-scale of the respective feature dimension.</source>
          <target state="translated">カーネルの長さのスケール。float の場合は,等方性カーネルが利用されます.配列の場合は,異方性カーネルが使用され,l の各次元がそれぞれの特徴次元の長さスケールを定義します.</target>
        </trans-unit>
        <trans-unit id="c8dac18109c7577d20154429eb0e5e79a703a997" translate="yes" xml:space="preserve">
          <source>The likelihood of the data set with &lt;code&gt;self.covariance_&lt;/code&gt; as an estimator of its covariance matrix.</source>
          <target state="translated">共分散行列の推定量として &lt;code&gt;self.covariance_&lt;/code&gt; を使用したデータセットの尤度。</target>
        </trans-unit>
        <trans-unit id="9d0f580fc795d85a96fff8300e9ac1a9bc3bbb0b" translate="yes" xml:space="preserve">
          <source>The linear model trained on polynomial features is able to exactly recover the input polynomial coefficients.</source>
          <target state="translated">多項式特徴量で訓練された線形モデルは、入力された多項式係数を正確に回復することができます。</target>
        </trans-unit>
        <trans-unit id="9cf1fb22576b6c52a81ed3fb3cd57a1b7c534cc0" translate="yes" xml:space="preserve">
          <source>The linear models &lt;code&gt;LinearSVC()&lt;/code&gt; and &lt;code&gt;SVC(kernel='linear')&lt;/code&gt; yield slightly different decision boundaries. This can be a consequence of the following differences:</source>
          <target state="translated">線形モデル &lt;code&gt;LinearSVC()&lt;/code&gt; および &lt;code&gt;SVC(kernel='linear')&lt;/code&gt; は、わずかに異なる決定境界を生成します。これは、次の違いの結果である可能性があります。</target>
        </trans-unit>
        <trans-unit id="02b9c4dcf92e3aac8d7274c2bb8e453c5501153c" translate="yes" xml:space="preserve">
          <source>The list of calibrated classifiers, one for each crossvalidation fold, which has been fitted on all but the validation fold and calibrated on the validation fold.</source>
          <target state="translated">較正された分類器のリストで、各クロスバリデーションフォールドに1つずつあり、バリデーションフォールド以外のすべてのフォールドに適合し、バリデーションフォールドで較正されています。</target>
        </trans-unit>
        <trans-unit id="662dd2cdb21b969ce9a44b42150a945acbfed523" translate="yes" xml:space="preserve">
          <source>The list of values of the objective function and the dual gap at each iteration. Returned only if return_costs is True.</source>
          <target state="translated">各反復時の対物関数の値とデュアルギャップのリスト。return_costsがTrueの場合にのみ返されます。</target>
        </trans-unit>
        <trans-unit id="177835998b74cd6b15a39a13b3bf0448af9a1ccf" translate="yes" xml:space="preserve">
          <source>The local outlier factor (LOF) of a sample captures its supposed &amp;lsquo;degree of abnormality&amp;rsquo;. It is the average of the ratio of the local reachability density of a sample and those of its k-nearest neighbors.</source>
          <target state="translated">サンプルの局所異常値（LOF）は、想定される「異常の程度」を示します。これは、サンプルの局所到達可能性密度とそのk最近傍の到達可能性密度の比の平均です。</target>
        </trans-unit>
        <trans-unit id="f5a19f67242aeecdaccb15b2c01bb7b8a32da9cd" translate="yes" xml:space="preserve">
          <source>The log likelihood at each iteration.</source>
          <target state="translated">各反復における対数尤度。</target>
        </trans-unit>
        <trans-unit id="d6423c4071c8619e84a84f9bcba64b070be431d3" translate="yes" xml:space="preserve">
          <source>The log-marginal-likelihood of &lt;code&gt;self.kernel_.theta&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;self.kernel_.theta&lt;/code&gt; の対数限界尤度</target>
        </trans-unit>
        <trans-unit id="b9f3852ad5acaf7d90c57ecdaa290bd2f37a1610" translate="yes" xml:space="preserve">
          <source>The log-transformed bounds on the kernel&amp;rsquo;s hyperparameters theta</source>
          <target state="translated">カーネルのハイパーパラメーターthetaの対数変換された境界</target>
        </trans-unit>
        <trans-unit id="997c53b0e19e31f9cbe762633463d7411a04ec12" translate="yes" xml:space="preserve">
          <source>The logarithm used is the natural logarithm (base-e).</source>
          <target state="translated">使用している対数は自然対数(Base-e)です。</target>
        </trans-unit>
        <trans-unit id="b4b4750e8021650b4da5de07fd1cb495068051c3" translate="yes" xml:space="preserve">
          <source>The logistic regression with One-Vs-Rest is not a multiclass classifier out of the box. As a result it has more trouble in separating class 2 and 3 than the other estimators.</source>
          <target state="translated">One-Vs-Restを用いたロジスティック回帰は、箱から出してマルチクラスの分類器ではありません。その結果、他の推定量よりもクラス2と3を分離するのに苦労する。</target>
        </trans-unit>
        <trans-unit id="658bb63624739c29e98c4d11ac78e5bf2f1fae2c" translate="yes" xml:space="preserve">
          <source>The loss function that &lt;a href=&quot;generated/sklearn.linear_model.huberregressor#sklearn.linear_model.HuberRegressor&quot;&gt;&lt;code&gt;HuberRegressor&lt;/code&gt;&lt;/a&gt; minimizes is given by</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.huberregressor#sklearn.linear_model.HuberRegressor&quot;&gt; &lt;code&gt;HuberRegressor&lt;/code&gt; が&lt;/a&gt;最小化する損失関数は、</target>
        </trans-unit>
        <trans-unit id="47dc1dfa051244f9df37eb81c616d6773441e3ed" translate="yes" xml:space="preserve">
          <source>The loss function to be used. Defaults to &amp;lsquo;hinge&amp;rsquo;, which gives a linear SVM.</source>
          <target state="translated">使用する損失関数。デフォルトは 'hinge'で、線形SVMを提供します。</target>
        </trans-unit>
        <trans-unit id="ced89f4970549dd56e97e03e89e06fddefc0d81d" translate="yes" xml:space="preserve">
          <source>The loss function to be used. The possible values are &amp;lsquo;squared_loss&amp;rsquo;, &amp;lsquo;huber&amp;rsquo;, &amp;lsquo;epsilon_insensitive&amp;rsquo;, or &amp;lsquo;squared_epsilon_insensitive&amp;rsquo;</source>
          <target state="translated">使用する損失関数。可能な値は「squared_loss」、「huber」、「epsilon_insensitive」、または「squared_epsilon_insensitive」です。</target>
        </trans-unit>
        <trans-unit id="dcd8a12c459702396e7eec22715d06aed2aee153" translate="yes" xml:space="preserve">
          <source>The loss function to be used: epsilon_insensitive: equivalent to PA-I in the reference paper. squared_epsilon_insensitive: equivalent to PA-II in the reference paper.</source>
          <target state="translated">使用する損失関数:epsilon_insensitive:参考文献のPA-Iに相当する。</target>
        </trans-unit>
        <trans-unit id="4c1d6cfb55920bc5e055d28ac3a6b6a90335ca56" translate="yes" xml:space="preserve">
          <source>The loss function to be used: hinge: equivalent to PA-I in the reference paper. squared_hinge: equivalent to PA-II in the reference paper.</source>
          <target state="translated">使用する損失関数:ヒンジ:参考文献のPA-Iに相当する.</target>
        </trans-unit>
        <trans-unit id="efb9498c0013e0aa10110f406847617e3f7359e7" translate="yes" xml:space="preserve">
          <source>The loss function to use when updating the weights after each boosting iteration.</source>
          <target state="translated">ブースティングの各イテレーション後に重みを更新する際に使用する損失関数。</target>
        </trans-unit>
        <trans-unit id="76e41cb6fdfbaf660e2380953e681777b0e6378c" translate="yes" xml:space="preserve">
          <source>The loss function used is binomial deviance. Regularization via shrinkage (&lt;code&gt;learning_rate &amp;lt; 1.0&lt;/code&gt;) improves performance considerably. In combination with shrinkage, stochastic gradient boosting (&lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt;) can produce more accurate models by reducing the variance via bagging. Subsampling without shrinkage usually does poorly. Another strategy to reduce the variance is by subsampling the features analogous to the random splits in Random Forests (via the &lt;code&gt;max_features&lt;/code&gt; parameter).</source>
          <target state="translated">使用される損失関数は二項偏差です。収縮による正則化（ &lt;code&gt;learning_rate &amp;lt; 1.0&lt;/code&gt; ）により、パフォーマンスが大幅に向上します。収縮と組み合わせて、確率的勾配ブースティング（ &lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt; ）は、バギングによって分散を減らすことにより、より正確なモデルを生成できます。通常、収縮なしのサブサンプリングは不十分です。分散を減らすもう1つの方法は、（ &lt;code&gt;max_features&lt;/code&gt; パラメーターを介して）ランダムフォレストのランダム分割に類似した機能をサブサンプリングすることです。</target>
        </trans-unit>
        <trans-unit id="b776e738f1fc829ad5cd97ce27a8c60ba5e6ea09" translate="yes" xml:space="preserve">
          <source>The low rank part of the profile can be considered the structured signal part of the data while the tail can be considered the noisy part of the data that cannot be summarized by a low number of linear components (singular vectors).</source>
          <target state="translated">プロファイルの低ランク部分は、データの構造化された信号部分と考えることができ、テール部分は、データのノイズの多い部分と考えることができ、少ない数の線形成分(特異ベクトル)ではまとめられない部分と考えることができる。</target>
        </trans-unit>
        <trans-unit id="eab337b7facddd1f9b82eece11282a55e0039c48" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on alpha</source>
          <target state="translated">アルファの下界と上界</target>
        </trans-unit>
        <trans-unit id="59acfd562a002dfced1122413eab7f832a55dd1c" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on constant_value</source>
          <target state="translated">constant_valueの下界と上界</target>
        </trans-unit>
        <trans-unit id="3a719a94ceaa10893529e6c7cb2929f1cad3fed1" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on gamma</source>
          <target state="translated">ガンマの下界と上界</target>
        </trans-unit>
        <trans-unit id="1ed15fdef2e92d07e56ad7fd40f1816e9ce5774a" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on l</source>
          <target state="translated">の下界と上界</target>
        </trans-unit>
        <trans-unit id="5a4d3b39beb7cd8132b2abe44c4adfccffd03b85" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on length_scale</source>
          <target state="translated">length_scale の下界と上界</target>
        </trans-unit>
        <trans-unit id="23389af6ff1a664526029d031ba30d8bce5de030" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on noise_level</source>
          <target state="translated">ノイズレベルの下界と上界</target>
        </trans-unit>
        <trans-unit id="5d396b2f8516f35fa726d028e889486242bc7cef" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on periodicity</source>
          <target state="translated">周期性の下限と上限</target>
        </trans-unit>
        <trans-unit id="67f0743ba9ff76a5a36032511baa5e62618c04ec" translate="yes" xml:space="preserve">
          <source>The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n &amp;lt;= n &amp;lt;= max_n will be used.</source>
          <target state="translated">抽出されるさまざまなn-gramのn値の範囲の下限と上限。min_n &amp;lt;= n &amp;lt;= max_nとなるようなnのすべての値が使用されます。</target>
        </trans-unit>
        <trans-unit id="1cf3abbfa7f1a3041db626cf750d4ae3ee4a5f71" translate="yes" xml:space="preserve">
          <source>The lower and upper percentile used create the extreme values for the &lt;code&gt;grid&lt;/code&gt;. Only if &lt;code&gt;X&lt;/code&gt; is not None.</source>
          <target state="translated">使用される下限と上限のパーセンタイルは、 &lt;code&gt;grid&lt;/code&gt; 極値を作成します。 &lt;code&gt;X&lt;/code&gt; がNoneでない場合のみ。</target>
        </trans-unit>
        <trans-unit id="ff88012ee3d2491153687a1e07b6eefe04629c89" translate="yes" xml:space="preserve">
          <source>The lower and upper percentile used to create the extreme values for the PDP axes.</source>
          <target state="translated">PDP軸の極端な値を作成するために使用される下限パーセンタイルと上限パーセンタイル。</target>
        </trans-unit>
        <trans-unit id="ca509231087e3737e2d90c6b71a37c419327789b" translate="yes" xml:space="preserve">
          <source>The lower left figure plots the pointwise decomposition of the expected mean squared error of a single decision tree. It confirms that the bias term (in blue) is low while the variance is large (in green). It also illustrates the noise part of the error which, as expected, appears to be constant and around &lt;code&gt;0.01&lt;/code&gt;.</source>
          <target state="translated">左下の図は、単一の決定木の予想平均二乗誤差の点ごとの分解をプロットしています。これにより、バイアス項（青色）が低く、分散が大きい（緑色）ことがわかります。また、エラーのノイズ部分も示しています。これは、予想どおり、一定で &lt;code&gt;0.01&lt;/code&gt; 前後であるように見えます。</target>
        </trans-unit>
        <trans-unit id="caf1b50ddc9e3b679e122c659dff7510d4ed2966" translate="yes" xml:space="preserve">
          <source>The lower the better.</source>
          <target state="translated">低ければ低いほど良い。</target>
        </trans-unit>
        <trans-unit id="bfd900c27bac872165454194e9ba0284baa1f3aa" translate="yes" xml:space="preserve">
          <source>The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems.</source>
          <target state="translated">コレスキー対角因子の計算における機械精度の正則化.非常に条件の悪い系のためにこれを増やす。</target>
        </trans-unit>
        <trans-unit id="69abd1713b39f884bea4ffeed0d502e131117e74" translate="yes" xml:space="preserve">
          <source>The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the &amp;lsquo;tol&amp;rsquo; parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization.</source>
          <target state="translated">コレスキー対角因子の計算における機械精​​度の正則化。非常に悪条件のシステムでは、これを増やします。一部の反復最適化ベースのアルゴリズムの「tol」パラメーターとは異なり、このパラメーターは最適化の許容範囲を制御しません。</target>
        </trans-unit>
        <trans-unit id="03f1e7333f5d863384674ba69d2200c51c214771" translate="yes" xml:space="preserve">
          <source>The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the &lt;code&gt;tol&lt;/code&gt; parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization.</source>
          <target state="translated">コレスキー対角因子の計算における機械精​​度の正則化。非常に悪条件のシステムでは、これを増やします。一部の反復最適化ベースのアルゴリズムの &lt;code&gt;tol&lt;/code&gt; パラメーターとは異なり、このパラメーターは最適化の許容範囲を制御しません。</target>
        </trans-unit>
        <trans-unit id="686cc4307ca584641912f9ca8288a53bb10ce6de" translate="yes" xml:space="preserve">
          <source>The main advantage for Factor Analysis over &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; is that it can model the variance in every direction of the input space independently (heteroscedastic noise):</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt;に対する因子分析の主な利点は、入力空間のあらゆる方向の分散を個別にモデル化できることです（異分散ノイズ）。</target>
        </trans-unit>
        <trans-unit id="c6108525766abe52c974030991b5e8a7ddcb6e28" translate="yes" xml:space="preserve">
          <source>The main difficulty in learning Gaussian mixture models from unlabeled data is that it is one usually doesn&amp;rsquo;t know which points came from which latent component (if one has access to this information it gets very easy to fit a separate Gaussian distribution to each set of points). &lt;a href=&quot;https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm&quot;&gt;Expectation-maximization&lt;/a&gt; is a well-founded statistical algorithm to get around this problem by an iterative process. First one assumes random components (randomly centered on data points, learned from k-means, or even just normally distributed around the origin) and computes for each point a probability of being generated by each component of the model. Then, one tweaks the parameters to maximize the likelihood of the data given those assignments. Repeating this process is guaranteed to always converge to a local optimum.</source>
          <target state="translated">ラベル付けされていないデータから混合ガウスモデルを学習する際の主な困難は、どの潜在的成分からどのポイントが生じたのか通常は分からないことです（この情報にアクセスできる場合、個別のガウス分布を各セットに当てはめることが非常に簡単になります）ポイント）。&lt;a href=&quot;https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm&quot;&gt;期待値最大化&lt;/a&gt;は、反復プロセスによってこの問題を回避するための十分に確立された統計アルゴリズムです。 1つ目は、ランダムコンポーネント（ランダムにデータポイントを中心、k平均から学習、または原点の周りに正規分布している）を想定し、各ポイントについて、モデルの各コンポーネントによって生成される確率を計算します。次に、パラメーターを調整して、これらの割り当てが与えられたデータの可能性を最大化します。このプロセスを繰り返すと、常に局所最適に収束することが保証されます。</target>
        </trans-unit>
        <trans-unit id="d2139f3f89c20ed8a9cf480b63f0750e12fef92b" translate="yes" xml:space="preserve">
          <source>The main documentation. This contains an in-depth description of all algorithms and how to apply them.</source>
          <target state="translated">主なドキュメントです。これには、すべてのアルゴリズムとその適用方法についての詳細な説明が含まれています。</target>
        </trans-unit>
        <trans-unit id="5f94da6f46e5f5e5d800fbb67f1c2ae40caf9c89" translate="yes" xml:space="preserve">
          <source>The main drawback of Affinity Propagation is its complexity. The algorithm has a time complexity of the order \(O(N^2 T)\), where \(N\) is the number of samples and \(T\) is the number of iterations until convergence. Further, the memory complexity is of the order \(O(N^2)\) if a dense similarity matrix is used, but reducible if a sparse similarity matrix is used. This makes Affinity Propagation most appropriate for small to medium sized datasets.</source>
          <target state="translated">Affinity Propagationの最大の欠点は、その複雑さです。このアルゴリズムの時間的な複雑さは、\(O(N^2 T)\(N\)はサンプル数、\(T\)は収束までの反復回数です。さらに,メモリの複雑さは,密な類似度行列を用いた場合には\(O(N^2)T)のオーダーになりますが,疎な類似度行列を用いた場合には軽減されます.このことから,Affinity Propagationは中小規模のデータセットに最も適している.</target>
        </trans-unit>
        <trans-unit id="f23633268affa3b7ac5da4b687edb4096985bac5" translate="yes" xml:space="preserve">
          <source>The main factors that influence the prediction latency are</source>
          <target state="translated">予測待ち時間に影響を与える主な要因は以下の通りです。</target>
        </trans-unit>
        <trans-unit id="26167f635fda3c2035f2a73e215b9329a59f7a43" translate="yes" xml:space="preserve">
          <source>The main observations to make are:</source>
          <target state="translated">主な観察事項は以下の通りです。</target>
        </trans-unit>
        <trans-unit id="b5b8962f0fe90f98d375c2c689eb4ed0da37d3b3" translate="yes" xml:space="preserve">
          <source>The main parameters to adjust when using these methods is &lt;code&gt;n_estimators&lt;/code&gt; and &lt;code&gt;max_features&lt;/code&gt;. The former is the number of trees in the forest. The larger the better, but also the longer it will take to compute. In addition, note that results will stop getting significantly better beyond a critical number of trees. The latter is the size of the random subsets of features to consider when splitting a node. The lower the greater the reduction of variance, but also the greater the increase in bias. Empirical good default values are &lt;code&gt;max_features=n_features&lt;/code&gt; for regression problems, and &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt; for classification tasks (where &lt;code&gt;n_features&lt;/code&gt; is the number of features in the data). Good results are often achieved when setting &lt;code&gt;max_depth=None&lt;/code&gt; in combination with &lt;code&gt;min_samples_split=2&lt;/code&gt; (i.e., when fully developing the trees). Bear in mind though that these values are usually not optimal, and might result in models that consume a lot of RAM. The best parameter values should always be cross-validated. In addition, note that in random forests, bootstrap samples are used by default (&lt;code&gt;bootstrap=True&lt;/code&gt;) while the default strategy for extra-trees is to use the whole dataset (&lt;code&gt;bootstrap=False&lt;/code&gt;). When using bootstrap sampling the generalization accuracy can be estimated on the left out or out-of-bag samples. This can be enabled by setting &lt;code&gt;oob_score=True&lt;/code&gt;.</source>
          <target state="translated">これらのメソッドを使用するときに調整する主なパラメーターは、 &lt;code&gt;n_estimators&lt;/code&gt; および &lt;code&gt;max_features&lt;/code&gt; です。前者は森の中の木の本数です。大きいほど良いですが、計算に時間がかかります。さらに、結果は、ツリーの臨界数を超えると大幅に改善されなくなることに注意してください。後者は、ノードを分割するときに考慮する機能のランダムなサブセットのサイズです。値が小さいほど、分散の減少が大きくなりますが、バイアスの増加も大きくなります。経験的に適切なデフォルト値は、回帰問題の場合は &lt;code&gt;max_features=n_features&lt;/code&gt; 、分類タスクの &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt; です（ &lt;code&gt;n_features&lt;/code&gt; はデータ内の特徴の数です）。 &lt;code&gt;max_depth=None&lt;/code&gt; を &lt;code&gt;min_samples_split=2&lt;/code&gt; と組み合わせて設定する場合（つまり、ツリーを完全に開発する場合）は、多くの場合、良い結果が得られます。ただし、これらの値は通常最適ではなく、RAMを大量に消費するモデルになる可能性があることに注意してください。最適なパラメータ値は常に相互検証する必要があります。さらに、ランダムフォレストでは、デフォルトでブートストラップサンプルが使用され（ &lt;code&gt;bootstrap=True&lt;/code&gt; ）、エクストラツリーのデフォルト戦略はデータセット全体を使用する（ &lt;code&gt;bootstrap=False&lt;/code&gt; ）ことに注意してください。ブートストラップサンプリングを使用する場合、一般化の精度は左またはアウトオブバッグサンプルで推定できます。これは &lt;code&gt;oob_score=True&lt;/code&gt; を設定することで有効にできます。</target>
        </trans-unit>
        <trans-unit id="842b51b29e297ee475304c463fec6849d760da97" translate="yes" xml:space="preserve">
          <source>The main purpose of t-SNE is visualization of high-dimensional data. Hence, it works best when the data will be embedded on two or three dimensions.</source>
          <target state="translated">t-SNEの主な目的は、高次元データの可視化です。したがって、データが2次元または3次元に埋め込まれている場合に最適です。</target>
        </trans-unit>
        <trans-unit id="85533a7e662fe3db2975f7f26edf978cd22f568d" translate="yes" xml:space="preserve">
          <source>The main theoretical result behind the efficiency of random projection is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma&quot;&gt;Johnson-Lindenstrauss lemma (quoting Wikipedia)&lt;/a&gt;:</source>
          <target state="translated">ランダム射影の効率の背後にある主な理論的結果は、&lt;a href=&quot;https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma&quot;&gt;Johnson-Lindenstrauss補題（Wikipediaを引用）&lt;/a&gt;です。</target>
        </trans-unit>
        <trans-unit id="c6fec5e2fce31198cf0394ae7c6624fc74feba7a" translate="yes" xml:space="preserve">
          <source>The main usage of a &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt;&lt;code&gt;Kernel&lt;/code&gt;&lt;/a&gt; is to compute the GP&amp;rsquo;s covariance between datapoints. For this, the method &lt;code&gt;__call__&lt;/code&gt; of the kernel can be called. This method can either be used to compute the &amp;ldquo;auto-covariance&amp;rdquo; of all pairs of datapoints in a 2d array X, or the &amp;ldquo;cross-covariance&amp;rdquo; of all combinations of datapoints of a 2d array X with datapoints in a 2d array Y. The following identity holds true for all kernels k (except for the &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.whitekernel#sklearn.gaussian_process.kernels.WhiteKernel&quot;&gt;&lt;code&gt;WhiteKernel&lt;/code&gt;&lt;/a&gt;): &lt;code&gt;k(X) == K(X, Y=X)&lt;/code&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt; &lt;code&gt;Kernel&lt;/code&gt; &lt;/a&gt;の主な用途は、データポイント間のGPの共分散を計算することです。この &lt;code&gt;__call__&lt;/code&gt; に、カーネルのメソッド__call__を呼び出すことができます。この方法は、2D配列Xのデータポイントのすべてのペアの「自動共分散」、または2D配列Xのデータポイントと2D配列Yのデータポイントのすべての組み合わせの「相互共分散」を計算するために使用できます。次の同一性は、すべてのカーネルk（&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.whitekernel#sklearn.gaussian_process.kernels.WhiteKernel&quot;&gt; &lt;code&gt;WhiteKernel&lt;/code&gt; &lt;/a&gt;を除く）に当てはまります &lt;code&gt;k(X) == K(X, Y=X)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="54123ac391766adafa62d20ad77d558f0edc6a11" translate="yes" xml:space="preserve">
          <source>The main use-case of the &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.whitekernel#sklearn.gaussian_process.kernels.WhiteKernel&quot;&gt;&lt;code&gt;WhiteKernel&lt;/code&gt;&lt;/a&gt; kernel is as part of a sum-kernel where it explains the noise-component of the signal. Tuning its parameter \(noise\_level\) corresponds to estimating the noise-level. It is defined as:e</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.whitekernel#sklearn.gaussian_process.kernels.WhiteKernel&quot;&gt; &lt;code&gt;WhiteKernel&lt;/code&gt; &lt;/a&gt;カーネルの主な使用例は、信号のノイズ成分を説明する合計カーネルの一部です。パラメータ\（noise \ _level \）の調整は、ノイズレベルの推定に相当します。それは次のように定義されます：e</target>
        </trans-unit>
        <trans-unit id="3b785a852cf47604fe35c8c001dbd6b426026b75" translate="yes" xml:space="preserve">
          <source>The main use-case of this kernel is as part of a sum-kernel where it explains the noise-component of the signal. Tuning its parameter corresponds to estimating the noise-level.</source>
          <target state="translated">このカーネルの主な使用例は、信号のノイズ成分を説明する和カーネルの一部としてです。このカーネルのパラメータを調整することは、ノイズレベルを推定することに相当します。</target>
        </trans-unit>
        <trans-unit id="b8d24bce3b0ca4f2f608d76c9973e497dd5a4966" translate="yes" xml:space="preserve">
          <source>The major advantage of SGD is its efficiency, which is basically linear in the number of training examples. If X is a matrix of size (n, p) training has a cost of \(O(k n \bar p)\), where k is the number of iterations (epochs) and \(\bar p\) is the average number of non-zero attributes per sample.</source>
          <target state="translated">SGD の最大の利点は、基本的に訓練例の数が直線的であるという効率性です。X が大きさ (n,p)の行列であるとすると、トレーニングのコストは、\(O(k n \bar p)のように、k は反復回数 (epochs)、\(\bar p)はサンプルあたりの非ゼロ属性の平均数です。</target>
        </trans-unit>
        <trans-unit id="c142aa304b117907c4ec82e7c8f2e0e1debc825c" translate="yes" xml:space="preserve">
          <source>The manifold learning implementations available in scikit-learn are summarized below</source>
          <target state="translated">scikit-learn で利用可能な多様な学習実装は以下のようにまとめられています。</target>
        </trans-unit>
        <trans-unit id="d450abcdfe5ff938dd90f9482f51850eeea5e6fe" translate="yes" xml:space="preserve">
          <source>The mapping relies on a Monte Carlo approximation to the kernel values. The &lt;code&gt;fit&lt;/code&gt; function performs the Monte Carlo sampling, whereas the &lt;code&gt;transform&lt;/code&gt; method performs the mapping of the data. Because of the inherent randomness of the process, results may vary between different calls to the &lt;code&gt;fit&lt;/code&gt; function.</source>
          <target state="translated">マッピングは、カーネル値へのモンテカルロ近似に依存しています。 &lt;code&gt;fit&lt;/code&gt; 関数は、一方、モンテカルロサンプリングを行う &lt;code&gt;transform&lt;/code&gt; 方法を行うデータのマッピング。プロセスには固有のランダム性があるため、 &lt;code&gt;fit&lt;/code&gt; 関数の呼び出しごとに結果が異なる場合があります。</target>
        </trans-unit>
        <trans-unit id="1292a72996c3834d41499b1c7abdc4ace6de6204" translate="yes" xml:space="preserve">
          <source>The mask of selected features.</source>
          <target state="translated">選択されたフィーチャーのマスク。</target>
        </trans-unit>
        <trans-unit id="c45c8ea0546b1c79f5dc6ae49b8b9eba3b94ab9e" translate="yes" xml:space="preserve">
          <source>The mathematical formulation is the following:</source>
          <target state="translated">数学的な定式化は以下の通りです。</target>
        </trans-unit>
        <trans-unit id="d7ffadc6e201c4cc579f6014a4ba6a553d3e6d97" translate="yes" xml:space="preserve">
          <source>The matrix</source>
          <target state="translated">マトリックス</target>
        </trans-unit>
        <trans-unit id="2cec1c9e3ed6a74b7dbd8e5442d20aacdeb523d2" translate="yes" xml:space="preserve">
          <source>The matrix dimension.</source>
          <target state="translated">行列の次元。</target>
        </trans-unit>
        <trans-unit id="e194b944fcebb1be4c3463284dee201bd8108680" translate="yes" xml:space="preserve">
          <source>The matrix inverse of the covariance matrix, often called the precision matrix, is proportional to the partial correlation matrix. It gives the partial independence relationship. In other words, if two features are independent conditionally on the others, the corresponding coefficient in the precision matrix will be zero. This is why it makes sense to estimate a sparse precision matrix: the estimation of the covariance matrix is better conditioned by learning independence relations from the data. This is known as &lt;em&gt;covariance selection&lt;/em&gt;.</source>
          <target state="translated">共分散行列の逆行列は、しばしば精度行列と呼ばれ、偏相関行列に比例します。それは部分的な独立関係を与えます。言い換えると、2つの特徴が他の特徴に対して条件付きで独立している場合、精度行列の対応する係数はゼロになります。これが、スパース精度行列を推定することが理にかなっている理由です。共分散行列の推定は、データから独立関係を学習することによってより適切に条件付けられます。これは、&lt;em&gt;共分散選択&lt;/em&gt;と呼ばれます。</target>
        </trans-unit>
        <trans-unit id="e578a80d83c508825e0e59f2ea21cfd3dab91a77" translate="yes" xml:space="preserve">
          <source>The matrix of features, where NP is the number of polynomial features generated from the combination of inputs.</source>
          <target state="translated">特徴量の行列であり、ここでNPは入力の組み合わせから生成される多項式特徴量の数である。</target>
        </trans-unit>
        <trans-unit id="64f7c7c3f44e0d1f1fa6c715782355e1f1d2054c" translate="yes" xml:space="preserve">
          <source>The matrix.</source>
          <target state="translated">マトリックスです。</target>
        </trans-unit>
        <trans-unit id="f3c27fa93df86412f11493765849dd6aeb05082d" translate="yes" xml:space="preserve">
          <source>The maximum depth of each tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.</source>
          <target state="translated">各木の最大深度。Noneの場合は、すべての葉が純粋になるか、すべての葉がmin_samples_splitサンプル以下になるまでノードが展開されます。</target>
        </trans-unit>
        <trans-unit id="52e0ef4a9206a897434b4d55c59a4824cb11d988" translate="yes" xml:space="preserve">
          <source>The maximum depth of the representation. If None, the tree is fully generated.</source>
          <target state="translated">表現の最大深度。Noneの場合、木は完全に生成されます。</target>
        </trans-unit>
        <trans-unit id="e0ae7481b9fb370fd4191031ea9f70e91832a43a" translate="yes" xml:space="preserve">
          <source>The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.</source>
          <target state="translated">ツリーの最大深度。Noneの場合、すべてのリーフが純粋になるまで、またはすべてのリーフがmin_samples_splitサンプル以下になるまでノードが展開されます。</target>
        </trans-unit>
        <trans-unit id="0cec94a1183be1885b8ca965cd0a310a0d6ef03e" translate="yes" xml:space="preserve">
          <source>The maximum distance between two samples for them to be considered as in the same neighborhood.</source>
          <target state="translated">2つのサンプルが同じ近傍にあるとみなされるための2つのサンプル間の最大距離。</target>
        </trans-unit>
        <trans-unit id="72469eac4bccf834885ed51dab58c805d8cdc971" translate="yes" xml:space="preserve">
          <source>The maximum number of concurrently running jobs, such as the number of Python worker processes when backend=&amp;rdquo;multiprocessing&amp;rdquo; or the size of the thread-pool when backend=&amp;rdquo;threading&amp;rdquo;. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. None is a marker for &amp;lsquo;unset&amp;rsquo; that will be interpreted as n_jobs=1 (sequential execution) unless the call is performed under a parallel_backend context manager that sets another value for n_jobs.</source>
          <target state="translated">同時に実行されるジョブの最大数。たとえば、backend =&amp;rdquo; multiprocessing&amp;rdquo;の場合のPythonワーカープロセスの数や、backend =&amp;rdquo; threading&amp;rdquo;の場合のスレッドプールのサイズ。-1の場合、すべてのCPUが使用されます。1を指定すると、並列計算コードはまったく使用されないため、デバッグに役立ちます。-1未満のn_jobsの場合、（n_cpus + 1 + n_jobs）が使用されます。したがって、n_jobs = -2の場合、1つを除くすべてのCPUが使用されます。Noneは、n_jobsに別の値を設定するparallel_backendコンテキストマネージャーの下で呼び出しが実行されない限り、n_jobs = 1（順次実行）として解釈される「設定解除」のマーカーです。</target>
        </trans-unit>
        <trans-unit id="d68dbcc29192b28b1c0e16950a4955da0229e9e9" translate="yes" xml:space="preserve">
          <source>The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early.</source>
          <target state="translated">ブースティングが終了する推定量の最大数。完全適合の場合は、学習を早期に停止する。</target>
        </trans-unit>
        <trans-unit id="0ace226674121a7119418c464f4452694b61318d" translate="yes" xml:space="preserve">
          <source>The maximum number of features selected scoring above &lt;code&gt;threshold&lt;/code&gt;. To disable &lt;code&gt;threshold&lt;/code&gt; and only select based on &lt;code&gt;max_features&lt;/code&gt;, set &lt;code&gt;threshold=-np.inf&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;threshold&lt;/code&gt; スコアを選択した特徴の最大数。 &lt;code&gt;threshold&lt;/code&gt; を無効にし、 &lt;code&gt;max_features&lt;/code&gt; に基づいてのみ選択するには、 &lt;code&gt;threshold=-np.inf&lt;/code&gt; を設定します。</target>
        </trans-unit>
        <trans-unit id="1360fbfbf92ec231e131ca0c0a387fb9e5b6a69f" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations</source>
          <target state="translated">最大反復回数</target>
        </trans-unit>
        <trans-unit id="07cf3627f06a0c97fd8b5fd02bbb00e02fc4d154" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations in Newton&amp;rsquo;s method for approximating the posterior during predict. Smaller values will reduce computation time at the cost of worse results.</source>
          <target state="translated">予測中に事後を近似するためのニュートン法の最大反復回数。値を小さくすると、計算時間が短縮されますが、結果は悪くなります。</target>
        </trans-unit>
        <trans-unit id="5ecb214f49e2d4dbf3814aaa5686d1fc9fb18e01" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations is usually high enough and does not need any tuning. The optimization consists of two phases: the early exaggeration phase and the final optimization. During early exaggeration the joint probabilities in the original space will be artificially increased by multiplication with a given factor. Larger factors result in larger gaps between natural clusters in the data. If the factor is too high, the KL divergence could increase during this phase. Usually it does not have to be tuned. A critical parameter is the learning rate. If it is too low gradient descent will get stuck in a bad local minimum. If it is too high the KL divergence will increase during optimization. More tips can be found in Laurens van der Maaten&amp;rsquo;s FAQ (see references). The last parameter, angle, is a tradeoff between performance and accuracy. Larger angles imply that we can approximate larger regions by a single point, leading to better speed but less accurate results.</source>
          <target state="translated">反復の最大数は通常十分に高く、チューニングは必要ありません。最適化は、初期の誇張フェーズと最終的な最適化の2つのフェーズで構成されます。初期の誇張の間、元の空間の結合確率は、与えられた係数との乗算によって人為的に増加します。因子が大きいほど、データ内の自然なクラスター間のギャップが大きくなります。係数が高すぎると、このフェーズでKLの発散が増加する可能性があります。通常は調整する必要はありません。重要なパラメータは学習率です。それが低すぎる場合、勾配降下は悪い局所最小値で動けなくなります。それが高すぎる場合、KLダイバージェンスは最適化中に増加します。その他のヒントは、Laurens van der MaatenのFAQ（参考文献を参照）にあります。最後のパラメータである角度は、パフォーマンスと精度の間のトレードオフです。角度が大きいほど、より大きな領域を1点で近似できるため、速度は向上しますが、結果の精度は低下します。</target>
        </trans-unit>
        <trans-unit id="3a263d39eeb80bbc61890473f9e4f7de61b380fb" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations to be run.</source>
          <target state="translated">実行する反復回数の最大値。</target>
        </trans-unit>
        <trans-unit id="42a16ed4baf475d1973848504fece7b7ddcdacc6" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations.</source>
          <target state="translated">反復回数の最大値。</target>
        </trans-unit>
        <trans-unit id="06bd569f5fc7509f2f4591004e3b0ffe2a685cf9" translate="yes" xml:space="preserve">
          <source>The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the &lt;code&gt;fit&lt;/code&gt; method, and not the &lt;code&gt;partial_fit&lt;/code&gt;. Defaults to 5. Defaults to 1000 from 0.21, or if tol is not None.</source>
          <target state="translated">トレーニングデータのパスの最大数（エポックとも呼ばれます）。これは、 &lt;code&gt;fit&lt;/code&gt; メソッドの動作にのみ影響し、 &lt;code&gt;partial_fit&lt;/code&gt; には影響しません。デフォルトは5です。デフォルトは0.21から1000です。tolがNoneでない場合。</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
